2022-09-07 14:27:31 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.2, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='multilingual_transformer_iwslt_de_en', attention_dropout=0.2, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, byte_data=None, char_or_bpe_data=None, checkpoint_suffix='', clip_norm=0.01, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path='dict/assign8.txt', decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=2, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.2, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path='dict/assign8.txt', encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=2, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=5, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_pairs='gn-es,quy-es,nah-es,shp-es', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_byte_tokens=10, max_epoch=50, max_sentences=None, max_sentences_valid=None, max_source_positions=4096, max_target_positions=4096, max_tokens=3000, max_tokens_valid=3000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_embed=True, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, nprocs_per_node=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, precision=6, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, ratio=10, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/endanger_data_gn-es_quy-es_nah-es_shp-es_0.2_3000', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_embeddings=False, share_decoder_input_output_embed=False, share_decoders=True, share_encoder_embeddings=False, share_encoders=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='multilingual_translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', update_freq=[4], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001)
2022-09-07 14:27:31 | INFO | fairseq.tasks.multilingual_translation | [es] dictionary: 176 types
2022-09-07 14:27:31 | INFO | fairseq.tasks.multilingual_translation | [gn] dictionary: 176 types
2022-09-07 14:27:31 | INFO | fairseq.tasks.multilingual_translation | [nah] dictionary: 176 types
2022-09-07 14:27:31 | INFO | fairseq.tasks.multilingual_translation | [quy] dictionary: 176 types
2022-09-07 14:27:31 | INFO | fairseq.tasks.multilingual_translation | [shp] dictionary: 176 types
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 995 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.gn-es.gn
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 995 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.gn-es.es
2022-09-07 14:27:31 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ valid gn-es 995 examples
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 996 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.quy-es.quy
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 996 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.quy-es.es
2022-09-07 14:27:31 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ valid quy-es 996 examples
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 672 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.nah-es.nah
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 672 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.nah-es.es
2022-09-07 14:27:31 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ valid nah-es 672 examples
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 996 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.shp-es.shp
2022-09-07 14:27:31 | INFO | fairseq.data.data_utils | loaded 996 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/valid.shp-es.es
2022-09-07 14:27:31 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ valid shp-es 996 examples
2022-09-07 14:27:31 | INFO | fairseq_cli.train | MultilingualTransformerModel(
  (models): ModuleDict(
    (gn-es): FairseqEncoderDecoderModel(
      (encoder): TransformerEncoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): TransformerDecoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (output_projection): Linear(in_features=512, out_features=176, bias=False)
      )
    )
    (quy-es): FairseqEncoderDecoderModel(
      (encoder): TransformerEncoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): TransformerDecoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (output_projection): Linear(in_features=512, out_features=176, bias=False)
      )
    )
    (nah-es): FairseqEncoderDecoderModel(
      (encoder): TransformerEncoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): TransformerDecoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (output_projection): Linear(in_features=512, out_features=176, bias=False)
      )
    )
    (shp-es): FairseqEncoderDecoderModel(
      (encoder): TransformerEncoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): TransformerDecoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(176, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (output_projection): Linear(in_features=512, out_features=176, bias=False)
      )
    )
  )
)
2022-09-07 14:27:31 | INFO | fairseq_cli.train | model multilingual_transformer_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion
2022-09-07 14:27:31 | INFO | fairseq_cli.train | num. model params: 10606595 (num. trained: 10516483)
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_scale <- models.quy-es.encoder.embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_scale <- models.nah-es.encoder.embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_scale <- models.shp-es.encoder.embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.gn-es.decoder.embed_tokens.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.gn-es.decoder.output_projection.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.quy-es.encoder.embed_tokens.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.quy-es.decoder.embed_tokens.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.quy-es.decoder.output_projection.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.nah-es.encoder.embed_tokens.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.nah-es.decoder.embed_tokens.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.nah-es.decoder.output_projection.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.shp-es.encoder.embed_tokens.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.shp-es.decoder.embed_tokens.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.embed_tokens.weight <- models.shp-es.decoder.output_projection.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.k_proj.weight <- models.quy-es.encoder.layers.0.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.k_proj.weight <- models.nah-es.encoder.layers.0.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.k_proj.weight <- models.shp-es.encoder.layers.0.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.k_proj.bias <- models.quy-es.encoder.layers.0.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.k_proj.bias <- models.nah-es.encoder.layers.0.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.k_proj.bias <- models.shp-es.encoder.layers.0.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.v_proj.weight <- models.quy-es.encoder.layers.0.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.v_proj.weight <- models.nah-es.encoder.layers.0.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.v_proj.weight <- models.shp-es.encoder.layers.0.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.v_proj.bias <- models.quy-es.encoder.layers.0.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.v_proj.bias <- models.nah-es.encoder.layers.0.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.v_proj.bias <- models.shp-es.encoder.layers.0.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.q_proj.weight <- models.quy-es.encoder.layers.0.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.q_proj.weight <- models.nah-es.encoder.layers.0.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.q_proj.weight <- models.shp-es.encoder.layers.0.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.q_proj.bias <- models.quy-es.encoder.layers.0.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.q_proj.bias <- models.nah-es.encoder.layers.0.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.q_proj.bias <- models.shp-es.encoder.layers.0.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.out_proj.weight <- models.quy-es.encoder.layers.0.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.out_proj.weight <- models.nah-es.encoder.layers.0.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.out_proj.weight <- models.shp-es.encoder.layers.0.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.out_proj.bias <- models.quy-es.encoder.layers.0.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.out_proj.bias <- models.nah-es.encoder.layers.0.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn.out_proj.bias <- models.shp-es.encoder.layers.0.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn_layer_norm.weight <- models.quy-es.encoder.layers.0.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn_layer_norm.weight <- models.nah-es.encoder.layers.0.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn_layer_norm.weight <- models.shp-es.encoder.layers.0.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn_layer_norm.bias <- models.quy-es.encoder.layers.0.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn_layer_norm.bias <- models.nah-es.encoder.layers.0.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.self_attn_layer_norm.bias <- models.shp-es.encoder.layers.0.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc1.weight <- models.quy-es.encoder.layers.0.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc1.weight <- models.nah-es.encoder.layers.0.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc1.weight <- models.shp-es.encoder.layers.0.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc1.bias <- models.quy-es.encoder.layers.0.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc1.bias <- models.nah-es.encoder.layers.0.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc1.bias <- models.shp-es.encoder.layers.0.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc2.weight <- models.quy-es.encoder.layers.0.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc2.weight <- models.nah-es.encoder.layers.0.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc2.weight <- models.shp-es.encoder.layers.0.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc2.bias <- models.quy-es.encoder.layers.0.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc2.bias <- models.nah-es.encoder.layers.0.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.fc2.bias <- models.shp-es.encoder.layers.0.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.final_layer_norm.weight <- models.quy-es.encoder.layers.0.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.final_layer_norm.weight <- models.nah-es.encoder.layers.0.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.final_layer_norm.weight <- models.shp-es.encoder.layers.0.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.final_layer_norm.bias <- models.quy-es.encoder.layers.0.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.final_layer_norm.bias <- models.nah-es.encoder.layers.0.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.0.final_layer_norm.bias <- models.shp-es.encoder.layers.0.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.k_proj.weight <- models.quy-es.encoder.layers.1.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.k_proj.weight <- models.nah-es.encoder.layers.1.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.k_proj.weight <- models.shp-es.encoder.layers.1.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.k_proj.bias <- models.quy-es.encoder.layers.1.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.k_proj.bias <- models.nah-es.encoder.layers.1.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.k_proj.bias <- models.shp-es.encoder.layers.1.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.v_proj.weight <- models.quy-es.encoder.layers.1.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.v_proj.weight <- models.nah-es.encoder.layers.1.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.v_proj.weight <- models.shp-es.encoder.layers.1.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.v_proj.bias <- models.quy-es.encoder.layers.1.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.v_proj.bias <- models.nah-es.encoder.layers.1.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.v_proj.bias <- models.shp-es.encoder.layers.1.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.q_proj.weight <- models.quy-es.encoder.layers.1.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.q_proj.weight <- models.nah-es.encoder.layers.1.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.q_proj.weight <- models.shp-es.encoder.layers.1.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.q_proj.bias <- models.quy-es.encoder.layers.1.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.q_proj.bias <- models.nah-es.encoder.layers.1.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.q_proj.bias <- models.shp-es.encoder.layers.1.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.out_proj.weight <- models.quy-es.encoder.layers.1.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.out_proj.weight <- models.nah-es.encoder.layers.1.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.out_proj.weight <- models.shp-es.encoder.layers.1.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.out_proj.bias <- models.quy-es.encoder.layers.1.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.out_proj.bias <- models.nah-es.encoder.layers.1.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn.out_proj.bias <- models.shp-es.encoder.layers.1.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn_layer_norm.weight <- models.quy-es.encoder.layers.1.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn_layer_norm.weight <- models.nah-es.encoder.layers.1.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn_layer_norm.weight <- models.shp-es.encoder.layers.1.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn_layer_norm.bias <- models.quy-es.encoder.layers.1.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn_layer_norm.bias <- models.nah-es.encoder.layers.1.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.self_attn_layer_norm.bias <- models.shp-es.encoder.layers.1.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc1.weight <- models.quy-es.encoder.layers.1.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc1.weight <- models.nah-es.encoder.layers.1.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc1.weight <- models.shp-es.encoder.layers.1.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc1.bias <- models.quy-es.encoder.layers.1.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc1.bias <- models.nah-es.encoder.layers.1.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc1.bias <- models.shp-es.encoder.layers.1.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc2.weight <- models.quy-es.encoder.layers.1.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc2.weight <- models.nah-es.encoder.layers.1.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc2.weight <- models.shp-es.encoder.layers.1.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc2.bias <- models.quy-es.encoder.layers.1.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc2.bias <- models.nah-es.encoder.layers.1.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.fc2.bias <- models.shp-es.encoder.layers.1.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.final_layer_norm.weight <- models.quy-es.encoder.layers.1.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.final_layer_norm.weight <- models.nah-es.encoder.layers.1.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.final_layer_norm.weight <- models.shp-es.encoder.layers.1.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.final_layer_norm.bias <- models.quy-es.encoder.layers.1.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.final_layer_norm.bias <- models.nah-es.encoder.layers.1.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layers.1.final_layer_norm.bias <- models.shp-es.encoder.layers.1.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layer_norm.weight <- models.quy-es.encoder.layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layer_norm.weight <- models.nah-es.encoder.layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layer_norm.weight <- models.shp-es.encoder.layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layer_norm.bias <- models.quy-es.encoder.layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layer_norm.bias <- models.nah-es.encoder.layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.encoder.layer_norm.bias <- models.shp-es.encoder.layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.embed_scale <- models.quy-es.decoder.embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.embed_scale <- models.nah-es.decoder.embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.embed_scale <- models.shp-es.decoder.embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.out_embed_scale <- models.quy-es.decoder.out_embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.out_embed_scale <- models.nah-es.decoder.out_embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.out_embed_scale <- models.shp-es.decoder.out_embed_scale
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.k_proj.weight <- models.quy-es.decoder.layers.0.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.k_proj.weight <- models.nah-es.decoder.layers.0.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.k_proj.weight <- models.shp-es.decoder.layers.0.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.k_proj.bias <- models.quy-es.decoder.layers.0.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.k_proj.bias <- models.nah-es.decoder.layers.0.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.k_proj.bias <- models.shp-es.decoder.layers.0.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.v_proj.weight <- models.quy-es.decoder.layers.0.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.v_proj.weight <- models.nah-es.decoder.layers.0.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.v_proj.weight <- models.shp-es.decoder.layers.0.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.v_proj.bias <- models.quy-es.decoder.layers.0.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.v_proj.bias <- models.nah-es.decoder.layers.0.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.v_proj.bias <- models.shp-es.decoder.layers.0.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.q_proj.weight <- models.quy-es.decoder.layers.0.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.q_proj.weight <- models.nah-es.decoder.layers.0.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.q_proj.weight <- models.shp-es.decoder.layers.0.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.q_proj.bias <- models.quy-es.decoder.layers.0.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.q_proj.bias <- models.nah-es.decoder.layers.0.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.q_proj.bias <- models.shp-es.decoder.layers.0.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.out_proj.weight <- models.quy-es.decoder.layers.0.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.out_proj.weight <- models.nah-es.decoder.layers.0.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.out_proj.weight <- models.shp-es.decoder.layers.0.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.out_proj.bias <- models.quy-es.decoder.layers.0.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.out_proj.bias <- models.nah-es.decoder.layers.0.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn.out_proj.bias <- models.shp-es.decoder.layers.0.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn_layer_norm.weight <- models.quy-es.decoder.layers.0.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn_layer_norm.weight <- models.nah-es.decoder.layers.0.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn_layer_norm.weight <- models.shp-es.decoder.layers.0.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn_layer_norm.bias <- models.quy-es.decoder.layers.0.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn_layer_norm.bias <- models.nah-es.decoder.layers.0.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.self_attn_layer_norm.bias <- models.shp-es.decoder.layers.0.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.k_proj.weight <- models.quy-es.decoder.layers.0.encoder_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.k_proj.weight <- models.nah-es.decoder.layers.0.encoder_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.k_proj.weight <- models.shp-es.decoder.layers.0.encoder_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.k_proj.bias <- models.quy-es.decoder.layers.0.encoder_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.k_proj.bias <- models.nah-es.decoder.layers.0.encoder_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.k_proj.bias <- models.shp-es.decoder.layers.0.encoder_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.v_proj.weight <- models.quy-es.decoder.layers.0.encoder_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.v_proj.weight <- models.nah-es.decoder.layers.0.encoder_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.v_proj.weight <- models.shp-es.decoder.layers.0.encoder_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.v_proj.bias <- models.quy-es.decoder.layers.0.encoder_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.v_proj.bias <- models.nah-es.decoder.layers.0.encoder_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.v_proj.bias <- models.shp-es.decoder.layers.0.encoder_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.q_proj.weight <- models.quy-es.decoder.layers.0.encoder_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.q_proj.weight <- models.nah-es.decoder.layers.0.encoder_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.q_proj.weight <- models.shp-es.decoder.layers.0.encoder_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.q_proj.bias <- models.quy-es.decoder.layers.0.encoder_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.q_proj.bias <- models.nah-es.decoder.layers.0.encoder_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.q_proj.bias <- models.shp-es.decoder.layers.0.encoder_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.out_proj.weight <- models.quy-es.decoder.layers.0.encoder_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.out_proj.weight <- models.nah-es.decoder.layers.0.encoder_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.out_proj.weight <- models.shp-es.decoder.layers.0.encoder_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.out_proj.bias <- models.quy-es.decoder.layers.0.encoder_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.out_proj.bias <- models.nah-es.decoder.layers.0.encoder_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn.out_proj.bias <- models.shp-es.decoder.layers.0.encoder_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn_layer_norm.weight <- models.quy-es.decoder.layers.0.encoder_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn_layer_norm.weight <- models.nah-es.decoder.layers.0.encoder_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn_layer_norm.weight <- models.shp-es.decoder.layers.0.encoder_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn_layer_norm.bias <- models.quy-es.decoder.layers.0.encoder_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn_layer_norm.bias <- models.nah-es.decoder.layers.0.encoder_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.encoder_attn_layer_norm.bias <- models.shp-es.decoder.layers.0.encoder_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc1.weight <- models.quy-es.decoder.layers.0.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc1.weight <- models.nah-es.decoder.layers.0.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc1.weight <- models.shp-es.decoder.layers.0.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc1.bias <- models.quy-es.decoder.layers.0.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc1.bias <- models.nah-es.decoder.layers.0.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc1.bias <- models.shp-es.decoder.layers.0.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc2.weight <- models.quy-es.decoder.layers.0.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc2.weight <- models.nah-es.decoder.layers.0.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc2.weight <- models.shp-es.decoder.layers.0.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc2.bias <- models.quy-es.decoder.layers.0.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc2.bias <- models.nah-es.decoder.layers.0.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.fc2.bias <- models.shp-es.decoder.layers.0.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.final_layer_norm.weight <- models.quy-es.decoder.layers.0.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.final_layer_norm.weight <- models.nah-es.decoder.layers.0.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.final_layer_norm.weight <- models.shp-es.decoder.layers.0.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.final_layer_norm.bias <- models.quy-es.decoder.layers.0.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.final_layer_norm.bias <- models.nah-es.decoder.layers.0.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.0.final_layer_norm.bias <- models.shp-es.decoder.layers.0.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.k_proj.weight <- models.quy-es.decoder.layers.1.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.k_proj.weight <- models.nah-es.decoder.layers.1.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.k_proj.weight <- models.shp-es.decoder.layers.1.self_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.k_proj.bias <- models.quy-es.decoder.layers.1.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.k_proj.bias <- models.nah-es.decoder.layers.1.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.k_proj.bias <- models.shp-es.decoder.layers.1.self_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.v_proj.weight <- models.quy-es.decoder.layers.1.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.v_proj.weight <- models.nah-es.decoder.layers.1.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.v_proj.weight <- models.shp-es.decoder.layers.1.self_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.v_proj.bias <- models.quy-es.decoder.layers.1.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.v_proj.bias <- models.nah-es.decoder.layers.1.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.v_proj.bias <- models.shp-es.decoder.layers.1.self_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.q_proj.weight <- models.quy-es.decoder.layers.1.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.q_proj.weight <- models.nah-es.decoder.layers.1.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.q_proj.weight <- models.shp-es.decoder.layers.1.self_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.q_proj.bias <- models.quy-es.decoder.layers.1.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.q_proj.bias <- models.nah-es.decoder.layers.1.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.q_proj.bias <- models.shp-es.decoder.layers.1.self_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.out_proj.weight <- models.quy-es.decoder.layers.1.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.out_proj.weight <- models.nah-es.decoder.layers.1.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.out_proj.weight <- models.shp-es.decoder.layers.1.self_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.out_proj.bias <- models.quy-es.decoder.layers.1.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.out_proj.bias <- models.nah-es.decoder.layers.1.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn.out_proj.bias <- models.shp-es.decoder.layers.1.self_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn_layer_norm.weight <- models.quy-es.decoder.layers.1.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn_layer_norm.weight <- models.nah-es.decoder.layers.1.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn_layer_norm.weight <- models.shp-es.decoder.layers.1.self_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn_layer_norm.bias <- models.quy-es.decoder.layers.1.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn_layer_norm.bias <- models.nah-es.decoder.layers.1.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.self_attn_layer_norm.bias <- models.shp-es.decoder.layers.1.self_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.k_proj.weight <- models.quy-es.decoder.layers.1.encoder_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.k_proj.weight <- models.nah-es.decoder.layers.1.encoder_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.k_proj.weight <- models.shp-es.decoder.layers.1.encoder_attn.k_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.k_proj.bias <- models.quy-es.decoder.layers.1.encoder_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.k_proj.bias <- models.nah-es.decoder.layers.1.encoder_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.k_proj.bias <- models.shp-es.decoder.layers.1.encoder_attn.k_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.v_proj.weight <- models.quy-es.decoder.layers.1.encoder_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.v_proj.weight <- models.nah-es.decoder.layers.1.encoder_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.v_proj.weight <- models.shp-es.decoder.layers.1.encoder_attn.v_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.v_proj.bias <- models.quy-es.decoder.layers.1.encoder_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.v_proj.bias <- models.nah-es.decoder.layers.1.encoder_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.v_proj.bias <- models.shp-es.decoder.layers.1.encoder_attn.v_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.q_proj.weight <- models.quy-es.decoder.layers.1.encoder_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.q_proj.weight <- models.nah-es.decoder.layers.1.encoder_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.q_proj.weight <- models.shp-es.decoder.layers.1.encoder_attn.q_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.q_proj.bias <- models.quy-es.decoder.layers.1.encoder_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.q_proj.bias <- models.nah-es.decoder.layers.1.encoder_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.q_proj.bias <- models.shp-es.decoder.layers.1.encoder_attn.q_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.out_proj.weight <- models.quy-es.decoder.layers.1.encoder_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.out_proj.weight <- models.nah-es.decoder.layers.1.encoder_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.out_proj.weight <- models.shp-es.decoder.layers.1.encoder_attn.out_proj.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.out_proj.bias <- models.quy-es.decoder.layers.1.encoder_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.out_proj.bias <- models.nah-es.decoder.layers.1.encoder_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn.out_proj.bias <- models.shp-es.decoder.layers.1.encoder_attn.out_proj.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn_layer_norm.weight <- models.quy-es.decoder.layers.1.encoder_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn_layer_norm.weight <- models.nah-es.decoder.layers.1.encoder_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn_layer_norm.weight <- models.shp-es.decoder.layers.1.encoder_attn_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn_layer_norm.bias <- models.quy-es.decoder.layers.1.encoder_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn_layer_norm.bias <- models.nah-es.decoder.layers.1.encoder_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.encoder_attn_layer_norm.bias <- models.shp-es.decoder.layers.1.encoder_attn_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc1.weight <- models.quy-es.decoder.layers.1.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc1.weight <- models.nah-es.decoder.layers.1.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc1.weight <- models.shp-es.decoder.layers.1.fc1.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc1.bias <- models.quy-es.decoder.layers.1.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc1.bias <- models.nah-es.decoder.layers.1.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc1.bias <- models.shp-es.decoder.layers.1.fc1.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc2.weight <- models.quy-es.decoder.layers.1.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc2.weight <- models.nah-es.decoder.layers.1.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc2.weight <- models.shp-es.decoder.layers.1.fc2.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc2.bias <- models.quy-es.decoder.layers.1.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc2.bias <- models.nah-es.decoder.layers.1.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.fc2.bias <- models.shp-es.decoder.layers.1.fc2.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.final_layer_norm.weight <- models.quy-es.decoder.layers.1.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.final_layer_norm.weight <- models.nah-es.decoder.layers.1.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.final_layer_norm.weight <- models.shp-es.decoder.layers.1.final_layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.final_layer_norm.bias <- models.quy-es.decoder.layers.1.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.final_layer_norm.bias <- models.nah-es.decoder.layers.1.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layers.1.final_layer_norm.bias <- models.shp-es.decoder.layers.1.final_layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layer_norm.weight <- models.quy-es.decoder.layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layer_norm.weight <- models.nah-es.decoder.layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layer_norm.weight <- models.shp-es.decoder.layer_norm.weight
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layer_norm.bias <- models.quy-es.decoder.layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layer_norm.bias <- models.nah-es.decoder.layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.layer_norm.bias <- models.shp-es.decoder.layer_norm.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.output_projection.bias <- models.quy-es.decoder.output_projection.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.output_projection.bias <- models.nah-es.decoder.output_projection.bias
2022-09-07 14:27:34 | INFO | fairseq.trainer | detected shared parameter: models.gn-es.decoder.output_projection.bias <- models.shp-es.decoder.output_projection.bias
2022-09-07 14:27:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-09-07 14:27:34 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 11.738 GB ; name = NVIDIA GeForce RTX 3080 Ti              
2022-09-07 14:27:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-09-07 14:27:34 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-09-07 14:27:34 | INFO | fairseq_cli.train | max tokens per GPU = 3000 and max sentences per GPU = None
2022-09-07 14:27:34 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/endanger_data_gn-es_quy-es_nah-es_shp-es_0.2_3000/checkpoint_last.pt
2022-09-07 14:27:34 | INFO | fairseq.trainer | loading train data for epoch 1
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 26032 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.gn-es.gn
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 26032 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.gn-es.es
2022-09-07 14:27:34 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ train gn-es 26032 examples
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 125008 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.quy-es.quy
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 125008 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.quy-es.es
2022-09-07 14:27:34 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ train quy-es 125008 examples
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 16145 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.nah-es.nah
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 16145 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.nah-es.es
2022-09-07 14:27:34 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ train nah-es 16145 examples
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 14592 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.shp-es.shp
2022-09-07 14:27:34 | INFO | fairseq.data.data_utils | loaded 14592 examples from: endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/train.shp-es.es
2022-09-07 14:27:34 | INFO | fairseq.tasks.translation | endanger_data.gn-es_quy-es_nah-es_shp-es//data_bin/gn-es_quy-es_nah-es_shp-es/ train shp-es 14592 examples
2022-09-07 14:27:36 | WARNING | fairseq.data.data_utils | 232 samples have invalid sizes and will be skipped, max_positions=OrderedDict([('gn-es', (3000, 3000)), ('quy-es', (3000, 3000)), ('nah-es', (3000, 3000)), ('shp-es', (3000, 3000))]), first few sample ids=[16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121]
2022-09-07 14:27:36 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2022-09-07 14:27:36 | INFO | fairseq_cli.train | begin training epoch 1
2022-09-07 14:27:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 924.84 MiB already allocated; 154.88 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:37 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     924 MB |     929 MB |   14749 MB |   13824 MB |
|       from large pool |     827 MB |     832 MB |   12707 MB |   11879 MB |
|       from small pool |      97 MB |     146 MB |    2041 MB |    1944 MB |
|---------------------------------------------------------------------------|
| Active memory         |     924 MB |     929 MB |   14749 MB |   13824 MB |
|       from large pool |     827 MB |     832 MB |   12707 MB |   11879 MB |
|       from small pool |      97 MB |     146 MB |    2041 MB |    1944 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1048 MB |    1048 MB |    2630 MB |    1582 MB |
|       from large pool |     942 MB |     942 MB |    2392 MB |    1450 MB |
|       from small pool |     106 MB |     154 MB |     238 MB |     132 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  126117 KB |  136618 KB |   15348 MB |   15225 MB |
|       from large pool |  117061 KB |  127541 KB |   13214 MB |   13100 MB |
|       from small pool |    9055 KB |   16139 KB |    2133 MB |    2124 MB |
|---------------------------------------------------------------------------|
| Allocations           |     533    |     545    |   13115    |   12582    |
|       from large pool |     126    |     127    |    3891    |    3765    |
|       from small pool |     407    |     507    |    9224    |    8817    |
|---------------------------------------------------------------------------|
| Active allocs         |     533    |     545    |   13115    |   12582    |
|       from large pool |     126    |     127    |    3891    |    3765    |
|       from small pool |     407    |     507    |    9224    |    8817    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     120    |     242    |     143    |
|       from large pool |      46    |      46    |     123    |      77    |
|       from small pool |      53    |      77    |     119    |      66    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      57    |      57    |    5857    |    5800    |
|       from large pool |      41    |      41    |    2471    |    2430    |
|       from small pool |      16    |      43    |    3386    |    3370    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 1005.35 MiB already allocated; 170.88 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:40 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1005 MB |    1005 MB |  210150 MB |  209145 MB |
|       from large pool |     908 MB |     908 MB |  186463 MB |  185554 MB |
|       from small pool |      97 MB |     166 MB |   23687 MB |   23590 MB |
|---------------------------------------------------------------------------|
| Active memory         |    1005 MB |    1005 MB |  210150 MB |  209145 MB |
|       from large pool |     908 MB |     908 MB |  186463 MB |  185554 MB |
|       from small pool |      97 MB |     166 MB |   23687 MB |   23590 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1032 MB |    1050 MB |    9832 MB |    8800 MB |
|       from large pool |     926 MB |     942 MB |    9064 MB |    8138 MB |
|       from small pool |     106 MB |     172 MB |     768 MB |     662 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27287 KB |  169780 KB |  213566 MB |  213540 MB |
|       from large pool |   18223 KB |  154438 KB |  188231 MB |  188213 MB |
|       from small pool |    9063 KB |   21739 KB |   25335 MB |   25326 MB |
|---------------------------------------------------------------------------|
| Allocations           |     537    |     569    |  148631    |  148094    |
|       from large pool |     130    |     133    |   48537    |   48407    |
|       from small pool |     407    |     534    |  100094    |   99687    |
|---------------------------------------------------------------------------|
| Active allocs         |     537    |     569    |  148631    |  148094    |
|       from large pool |     130    |     133    |   48537    |   48407    |
|       from small pool |     407    |     534    |  100094    |   99687    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |     845    |     748    |
|       from large pool |      44    |      52    |     461    |     417    |
|       from small pool |      53    |      86    |     384    |     331    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      71    |   69948    |   69926    |
|       from large pool |       6    |      47    |   30922    |   30916    |
|       from small pool |      16    |      55    |   39026    |   39010    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.74 GiB total capacity; 981.43 MiB already allocated; 161.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:41 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     981 MB |    1005 MB |  249322 MB |  248341 MB |
|       from large pool |     878 MB |     908 MB |  221638 MB |  220759 MB |
|       from small pool |     103 MB |     166 MB |   27684 MB |   27581 MB |
|---------------------------------------------------------------------------|
| Active memory         |     981 MB |    1005 MB |  249322 MB |  248341 MB |
|       from large pool |     878 MB |     908 MB |  221638 MB |  220759 MB |
|       from small pool |     103 MB |     166 MB |   27684 MB |   27581 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1050 MB |   11690 MB |   10648 MB |
|       from large pool |     932 MB |     942 MB |   10794 MB |    9862 MB |
|       from small pool |     110 MB |     172 MB |     896 MB |     786 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   62022 KB |  169780 KB |  255279 MB |  255218 MB |
|       from large pool |   54936 KB |  154438 KB |  225682 MB |  225628 MB |
|       from small pool |    7086 KB |   21739 KB |   29597 MB |   29590 MB |
|---------------------------------------------------------------------------|
| Allocations           |     543    |     569    |  175688    |  175145    |
|       from large pool |     111    |     133    |   58241    |   58130    |
|       from small pool |     432    |     534    |  117447    |  117015    |
|---------------------------------------------------------------------------|
| Active allocs         |     543    |     569    |  175688    |  175145    |
|       from large pool |     111    |     133    |   58241    |   58130    |
|       from small pool |     432    |     534    |  117447    |  117015    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |     134    |     983    |     894    |
|       from large pool |      34    |      52    |     535    |     501    |
|       from small pool |      55    |      86    |     448    |     393    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      71    |   82944    |   82904    |
|       from large pool |      22    |      47    |   37142    |   37120    |
|       from small pool |      18    |      55    |   45802    |   45784    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 989.89 MiB already allocated; 167.00 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:41 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     984 MB |    1005 MB |  256838 MB |  255853 MB |
|       from large pool |     887 MB |     908 MB |  228036 MB |  227149 MB |
|       from small pool |      97 MB |     166 MB |   28801 MB |   28704 MB |
|---------------------------------------------------------------------------|
| Active memory         |     984 MB |    1005 MB |  256838 MB |  255853 MB |
|       from large pool |     887 MB |     908 MB |  228036 MB |  227149 MB |
|       from small pool |      97 MB |     166 MB |   28801 MB |   28704 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1036 MB |    1050 MB |   13246 MB |   12210 MB |
|       from large pool |     924 MB |     942 MB |   12282 MB |   11358 MB |
|       from small pool |     112 MB |     172 MB |     964 MB |     852 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   52514 KB |  169780 KB |  261543 MB |  261492 MB |
|       from large pool |   37221 KB |  154438 KB |  230749 MB |  230712 MB |
|       from small pool |   15292 KB |   21739 KB |   30794 MB |   30779 MB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     569    |  181420    |  180904    |
|       from large pool |     105    |     133    |   59612    |   59507    |
|       from small pool |     411    |     534    |  121808    |  121397    |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     569    |  181420    |  180904    |
|       from large pool |     105    |     133    |   59612    |   59507    |
|       from small pool |     411    |     534    |  121808    |  121397    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     134    |    1080    |     989    |
|       from large pool |      35    |      52    |     598    |     563    |
|       from small pool |      56    |      86    |     482    |     426    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      71    |   85720    |   85684    |
|       from large pool |      10    |      47    |   38016    |   38006    |
|       from small pool |      26    |      55    |   47704    |   47678    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 967.68 MiB already allocated; 171.00 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:42 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     967 MB |    1005 MB |  321814 MB |  320846 MB |
|       from large pool |     870 MB |     908 MB |  287090 MB |  286219 MB |
|       from small pool |      97 MB |     166 MB |   34724 MB |   34626 MB |
|---------------------------------------------------------------------------|
| Active memory         |     967 MB |    1005 MB |  321814 MB |  320846 MB |
|       from large pool |     870 MB |     908 MB |  287090 MB |  286219 MB |
|       from small pool |      97 MB |     166 MB |   34724 MB |   34626 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1032 MB |    1050 MB |   15980 MB |   14948 MB |
|       from large pool |     926 MB |     942 MB |   14856 MB |   13930 MB |
|       from small pool |     106 MB |     172 MB |    1124 MB |    1018 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   65865 KB |  169780 KB |  330351 MB |  330286 MB |
|       from large pool |   56807 KB |  154438 KB |  293176 MB |  293120 MB |
|       from small pool |    9058 KB |   26654 KB |   37175 MB |   37166 MB |
|---------------------------------------------------------------------------|
| Allocations           |     531    |     569    |  225137    |  224606    |
|       from large pool |     116    |     133    |   75099    |   74983    |
|       from small pool |     415    |     534    |  150038    |  149623    |
|---------------------------------------------------------------------------|
| Active allocs         |     531    |     569    |  225137    |  224606    |
|       from large pool |     116    |     133    |   75099    |   74983    |
|       from small pool |     415    |     534    |  150038    |  149623    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     134    |    1285    |    1186    |
|       from large pool |      46    |      52    |     723    |     677    |
|       from small pool |      53    |      86    |     562    |     509    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      71    |  106831    |  106790    |
|       from large pool |      24    |      47    |   48041    |   48017    |
|       from small pool |      17    |      55    |   58790    |   58773    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.74 GiB total capacity; 966.37 MiB already allocated; 179.00 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:43 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     966 MB |    1005 MB |  375292 MB |  374326 MB |
|       from large pool |     861 MB |     908 MB |  333783 MB |  332922 MB |
|       from small pool |     104 MB |     166 MB |   41508 MB |   41404 MB |
|---------------------------------------------------------------------------|
| Active memory         |     966 MB |    1005 MB |  375292 MB |  374326 MB |
|       from large pool |     861 MB |     908 MB |  333783 MB |  332922 MB |
|       from small pool |     104 MB |     166 MB |   41508 MB |   41404 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1024 MB |    1050 MB |   18322 MB |   17298 MB |
|       from large pool |     908 MB |     942 MB |   17026 MB |   16118 MB |
|       from small pool |     116 MB |     172 MB |    1296 MB |    1180 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   59014 KB |  169780 KB |  384027 MB |  383969 MB |
|       from large pool |   47600 KB |  154438 KB |  339632 MB |  339585 MB |
|       from small pool |   11414 KB |   26654 KB |   44395 MB |   44383 MB |
|---------------------------------------------------------------------------|
| Allocations           |     525    |     569    |  266218    |  265693    |
|       from large pool |     112    |     133    |   87672    |   87560    |
|       from small pool |     413    |     534    |  178546    |  178133    |
|---------------------------------------------------------------------------|
| Active allocs         |     525    |     569    |  266218    |  265693    |
|       from large pool |     112    |     133    |   87672    |   87560    |
|       from small pool |     413    |     534    |  178546    |  178133    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |    1478    |    1380    |
|       from large pool |      40    |      52    |     830    |     790    |
|       from small pool |      58    |      86    |     648    |     590    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      45    |      71    |  126764    |  126719    |
|       from large pool |      19    |      47    |   56384    |   56365    |
|       from small pool |      26    |      60    |   70380    |   70354    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 11.74 GiB total capacity; 1008.87 MiB already allocated; 155.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:43 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     895 MB |    1008 MB |  378934 MB |  378038 MB |
|       from large pool |     798 MB |     911 MB |  337005 MB |  336206 MB |
|       from small pool |      97 MB |     166 MB |   41929 MB |   41832 MB |
|---------------------------------------------------------------------------|
| Active memory         |     895 MB |    1008 MB |  378934 MB |  378038 MB |
|       from large pool |     798 MB |     911 MB |  337005 MB |  336206 MB |
|       from small pool |      97 MB |     166 MB |   41929 MB |   41832 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1048 MB |    1050 MB |   19128 MB |   18080 MB |
|       from large pool |     944 MB |     944 MB |   17800 MB |   16856 MB |
|       from small pool |     104 MB |     172 MB |    1328 MB |    1224 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   39017 KB |  169780 KB |  387214 MB |  387176 MB |
|       from large pool |   31871 KB |  154438 KB |  342354 MB |  342323 MB |
|       from small pool |    7146 KB |   26654 KB |   44859 MB |   44852 MB |
|---------------------------------------------------------------------------|
| Allocations           |     454    |     569    |  269550    |  269096    |
|       from large pool |      62    |     133    |   88391    |   88329    |
|       from small pool |     392    |     534    |  181159    |  180767    |
|---------------------------------------------------------------------------|
| Active allocs         |     454    |     569    |  269550    |  269096    |
|       from large pool |      62    |     133    |   88391    |   88329    |
|       from small pool |     392    |     534    |  181159    |  180767    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      71    |     134    |    1503    |    1432    |
|       from large pool |      19    |      52    |     839    |     820    |
|       from small pool |      52    |      86    |     664    |     612    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      71    |  128474    |  128442    |
|       from large pool |      13    |      47    |   56862    |   56849    |
|       from small pool |      19    |      60    |   71612    |   71593    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 996.17 MiB already allocated; 163.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:43 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     996 MB |    1008 MB |  412500 MB |  411504 MB |
|       from large pool |     895 MB |     911 MB |  367400 MB |  366505 MB |
|       from small pool |     100 MB |     166 MB |   45099 MB |   44998 MB |
|---------------------------------------------------------------------------|
| Active memory         |     996 MB |    1008 MB |  412500 MB |  411504 MB |
|       from large pool |     895 MB |     911 MB |  367400 MB |  366505 MB |
|       from small pool |     100 MB |     166 MB |   45099 MB |   44998 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1050 MB |   19304 MB |   18264 MB |
|       from large pool |     930 MB |     944 MB |   17900 MB |   16970 MB |
|       from small pool |     110 MB |     172 MB |    1404 MB |    1294 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   44886 KB |  250073 KB |  428194 MB |  428150 MB |
|       from large pool |   35536 KB |  240938 KB |  379941 MB |  379906 MB |
|       from small pool |    9350 KB |   26654 KB |   48253 MB |   48244 MB |
|---------------------------------------------------------------------------|
| Allocations           |     526    |     569    |  292006    |  291480    |
|       from large pool |     117    |     133    |   96270    |   96153    |
|       from small pool |     409    |     534    |  195736    |  195327    |
|---------------------------------------------------------------------------|
| Active allocs         |     526    |     569    |  292006    |  291480    |
|       from large pool |     117    |     133    |   96270    |   96153    |
|       from small pool |     409    |     534    |  195736    |  195327    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |     134    |    1545    |    1468    |
|       from large pool |      22    |      52    |     843    |     821    |
|       from small pool |      55    |      86    |     702    |     647    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      71    |  138825    |  138793    |
|       from large pool |      13    |      47    |   61752    |   61739    |
|       from small pool |      19    |      60    |   77073    |   77054    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 1006.30 MiB already allocated; 159.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:43 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     959 MB |    1008 MB |  413996 MB |  413036 MB |
|       from large pool |     862 MB |     911 MB |  368577 MB |  367714 MB |
|       from small pool |      97 MB |     166 MB |   45419 MB |   45322 MB |
|---------------------------------------------------------------------------|
| Active memory         |     959 MB |    1008 MB |  413996 MB |  413036 MB |
|       from large pool |     862 MB |     911 MB |  368577 MB |  367714 MB |
|       from small pool |      97 MB |     166 MB |   45419 MB |   45322 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1044 MB |    1050 MB |   20156 MB |   19112 MB |
|       from large pool |     932 MB |     944 MB |   18720 MB |   17788 MB |
|       from small pool |     112 MB |     172 MB |    1436 MB |    1324 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   37155 KB |  345437 KB |  429499 MB |  429463 MB |
|       from large pool |   21845 KB |  327682 KB |  380900 MB |  380878 MB |
|       from small pool |   15309 KB |   26654 KB |   48599 MB |   48584 MB |
|---------------------------------------------------------------------------|
| Allocations           |     483    |     569    |  293235    |  292752    |
|       from large pool |      94    |     133    |   96405    |   96311    |
|       from small pool |     389    |     534    |  196830    |  196441    |
|---------------------------------------------------------------------------|
| Active allocs         |     483    |     569    |  293235    |  292752    |
|       from large pool |      94    |     133    |   96405    |   96311    |
|       from small pool |     389    |     534    |  196830    |  196441    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |     134    |    1588    |    1499    |
|       from large pool |      33    |      52    |     870    |     837    |
|       from small pool |      56    |      86    |     718    |     662    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      71    |  139376    |  139339    |
|       from large pool |      10    |      47    |   61835    |   61825    |
|       from small pool |      27    |      60    |   77541    |   77514    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.74 GiB total capacity; 871.46 MiB already allocated; 175.00 MiB free; 1.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:44 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     871 MB |    1008 MB |  457501 MB |  456630 MB |
|       from large pool |     774 MB |     911 MB |  406947 MB |  406173 MB |
|       from small pool |      97 MB |     166 MB |   50554 MB |   50457 MB |
|---------------------------------------------------------------------------|
| Active memory         |     871 MB |    1008 MB |  457501 MB |  456630 MB |
|       from large pool |     774 MB |     911 MB |  406947 MB |  406173 MB |
|       from small pool |      97 MB |     166 MB |   50554 MB |   50457 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1028 MB |    1050 MB |   20262 MB |   19234 MB |
|       from large pool |     922 MB |     944 MB |   18758 MB |   17836 MB |
|       from small pool |     106 MB |     172 MB |    1504 MB |    1398 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  160302 KB |  345437 KB |  478998 MB |  478842 MB |
|       from large pool |  151165 KB |  327682 KB |  424842 MB |  424694 MB |
|       from small pool |    9137 KB |   26654 KB |   54156 MB |   54147 MB |
|---------------------------------------------------------------------------|
| Allocations           |     503    |     569    |  327094    |  326591    |
|       from large pool |      94    |     133    |  107282    |  107188    |
|       from small pool |     409    |     534    |  219812    |  219403    |
|---------------------------------------------------------------------------|
| Active allocs         |     503    |     569    |  327094    |  326591    |
|       from large pool |      94    |     133    |  107282    |  107188    |
|       from small pool |     409    |     534    |  219812    |  219403    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     134    |    1623    |    1537    |
|       from large pool |      33    |      52    |     871    |     838    |
|       from small pool |      53    |      86    |     752    |     699    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      71    |  155215    |  155172    |
|       from large pool |      28    |      47    |   69002    |   68974    |
|       from small pool |      15    |      60    |   86213    |   86198    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 11.74 GiB total capacity; 978.40 MiB already allocated; 155.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:45 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     978 MB |    1008 MB |  515704 MB |  514726 MB |
|       from large pool |     881 MB |     911 MB |  459201 MB |  458319 MB |
|       from small pool |      97 MB |     166 MB |   56503 MB |   56406 MB |
|---------------------------------------------------------------------------|
| Active memory         |     978 MB |    1008 MB |  515704 MB |  514726 MB |
|       from large pool |     881 MB |     911 MB |  459201 MB |  458319 MB |
|       from small pool |      97 MB |     166 MB |   56503 MB |   56406 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1048 MB |    1050 MB |   21828 MB |   20780 MB |
|       from large pool |     944 MB |     944 MB |   20252 MB |   19308 MB |
|       from small pool |     104 MB |     172 MB |    1576 MB |    1472 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   71272 KB |  345437 KB |  539988 MB |  539919 MB |
|       from large pool |   64196 KB |  327682 KB |  479483 MB |  479420 MB |
|       from small pool |    7076 KB |   26654 KB |   60505 MB |   60498 MB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     569    |  368334    |  367818    |
|       from large pool |     111    |     133    |  121468    |  121357    |
|       from small pool |     405    |     534    |  246866    |  246461    |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     569    |  368334    |  367818    |
|       from large pool |     111    |     133    |  121468    |  121357    |
|       from small pool |     405    |     534    |  246866    |  246461    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |     134    |    1726    |    1637    |
|       from large pool |      37    |      52    |     938    |     901    |
|       from small pool |      52    |      86    |     788    |     736    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      71    |  174972    |  174941    |
|       from large pool |      16    |      47    |   78396    |   78380    |
|       from small pool |      15    |      60    |   96576    |   96561    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 11.74 GiB total capacity; 1001.61 MiB already allocated; 163.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:45 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1001 MB |    1008 MB |  516895 MB |  515893 MB |
|       from large pool |     904 MB |     911 MB |  460240 MB |  459336 MB |
|       from small pool |      97 MB |     166 MB |   56654 MB |   56557 MB |
|---------------------------------------------------------------------------|
| Active memory         |    1001 MB |    1008 MB |  516895 MB |  515893 MB |
|       from large pool |     904 MB |     911 MB |  460240 MB |  459336 MB |
|       from small pool |      97 MB |     166 MB |   56654 MB |   56557 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1050 MB |   22676 MB |   21636 MB |
|       from large pool |     934 MB |     944 MB |   21074 MB |   20140 MB |
|       from small pool |     106 MB |     172 MB |    1602 MB |    1496 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   39313 KB |  345437 KB |  540626 MB |  540587 MB |
|       from large pool |   30121 KB |  327682 KB |  479958 MB |  479928 MB |
|       from small pool |    9192 KB |   26654 KB |   60668 MB |   60659 MB |
|---------------------------------------------------------------------------|
| Allocations           |     469    |     569    |  369468    |  368999    |
|       from large pool |      82    |     133    |  121552    |  121470    |
|       from small pool |     387    |     534    |  247916    |  247529    |
|---------------------------------------------------------------------------|
| Active allocs         |     469    |     569    |  369468    |  368999    |
|       from large pool |      82    |     133    |  121552    |  121470    |
|       from small pool |     387    |     534    |  247916    |  247529    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     134    |    1756    |    1680    |
|       from large pool |      23    |      52    |     955    |     932    |
|       from small pool |      53    |      86    |     801    |     748    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      71    |  175578    |  175548    |
|       from large pool |      11    |      47    |   78455    |   78444    |
|       from small pool |      19    |      60    |   97123    |   97104    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 997.92 MiB already allocated; 169.00 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:45 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     997 MB |    1008 MB |  518349 MB |  517351 MB |
|       from large pool |     900 MB |     911 MB |  461392 MB |  460491 MB |
|       from small pool |      97 MB |     166 MB |   56956 MB |   56859 MB |
|---------------------------------------------------------------------------|
| Active memory         |     997 MB |    1008 MB |  518349 MB |  517351 MB |
|       from large pool |     900 MB |     911 MB |  461392 MB |  460491 MB |
|       from small pool |      97 MB |     166 MB |   56956 MB |   56859 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1034 MB |    1050 MB |   23550 MB |   22516 MB |
|       from large pool |     926 MB |     944 MB |   21886 MB |   20960 MB |
|       from small pool |     108 MB |     172 MB |    1664 MB |    1556 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   36944 KB |  345437 KB |  541728 MB |  541692 MB |
|       from large pool |   25808 KB |  327682 KB |  480731 MB |  480706 MB |
|       from small pool |   11136 KB |   26654 KB |   60996 MB |   60985 MB |
|---------------------------------------------------------------------------|
| Allocations           |     510    |     569    |  370738    |  370228    |
|       from large pool |     115    |     133    |  121713    |  121598    |
|       from small pool |     395    |     534    |  249025    |  248630    |
|---------------------------------------------------------------------------|
| Active allocs         |     510    |     569    |  370738    |  370228    |
|       from large pool |     115    |     133    |  121713    |  121598    |
|       from small pool |     395    |     534    |  249025    |  248630    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      93    |     134    |    1820    |    1727    |
|       from large pool |      39    |      52    |     988    |     949    |
|       from small pool |      54    |      86    |     832    |     778    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      71    |  176136    |  176099    |
|       from large pool |      12    |      47    |   78552    |   78540    |
|       from small pool |      25    |      60    |   97584    |   97559    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 975.24 MiB already allocated; 171.00 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:46 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     969 MB |    1008 MB |  564908 MB |  563938 MB |
|       from large pool |     868 MB |     911 MB |  502454 MB |  501585 MB |
|       from small pool |     101 MB |     166 MB |   62453 MB |   62352 MB |
|---------------------------------------------------------------------------|
| Active memory         |     969 MB |    1008 MB |  564908 MB |  563938 MB |
|       from large pool |     868 MB |     911 MB |  502454 MB |  501585 MB |
|       from small pool |     101 MB |     166 MB |   62453 MB |   62352 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1032 MB |    1050 MB |   25176 MB |   24144 MB |
|       from large pool |     926 MB |     944 MB |   23406 MB |   22480 MB |
|       from small pool |     106 MB |     172 MB |    1770 MB |    1664 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   63620 KB |  345437 KB |  589758 MB |  589696 MB |
|       from large pool |   58524 KB |  327682 KB |  522850 MB |  522793 MB |
|       from small pool |    5096 KB |   26654 KB |   66908 MB |   66903 MB |
|---------------------------------------------------------------------------|
| Allocations           |     540    |     569    |  404733    |  404193    |
|       from large pool |     121    |     133    |  132895    |  132774    |
|       from small pool |     419    |     534    |  271838    |  271419    |
|---------------------------------------------------------------------------|
| Active allocs         |     540    |     569    |  404733    |  404193    |
|       from large pool |     121    |     133    |  132895    |  132774    |
|       from small pool |     419    |     534    |  271838    |  271419    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |    1947    |    1849    |
|       from large pool |      45    |      52    |    1062    |    1017    |
|       from small pool |      53    |      86    |     885    |     832    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      71    |  191888    |  191847    |
|       from large pool |      24    |      47    |   85844    |   85820    |
|       from small pool |      17    |      60    |  106044    |  106027    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.74 GiB total capacity; 948.63 MiB already allocated; 157.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:46 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     948 MB |    1008 MB |  589579 MB |  588630 MB |
|       from large pool |     851 MB |     911 MB |  524052 MB |  523200 MB |
|       from small pool |      97 MB |     166 MB |   65526 MB |   65429 MB |
|---------------------------------------------------------------------------|
| Active memory         |     948 MB |    1008 MB |  589579 MB |  588630 MB |
|       from large pool |     851 MB |     911 MB |  524052 MB |  523200 MB |
|       from small pool |      97 MB |     166 MB |   65526 MB |   65429 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1050 MB |   26254 MB |   25208 MB |
|       from large pool |     942 MB |     944 MB |   24410 MB |   23468 MB |
|       from small pool |     104 MB |     172 MB |    1844 MB |    1740 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   99705 KB |  345437 KB |  615949 MB |  615852 MB |
|       from large pool |   92679 KB |  327682 KB |  545741 MB |  545651 MB |
|       from small pool |    7026 KB |   26654 KB |   70207 MB |   70200 MB |
|---------------------------------------------------------------------------|
| Allocations           |     546    |     569    |  422315    |  421769    |
|       from large pool |     121    |     133    |  139218    |  139097    |
|       from small pool |     425    |     534    |  283097    |  282672    |
|---------------------------------------------------------------------------|
| Active allocs         |     546    |     569    |  422315    |  421769    |
|       from large pool |     121    |     133    |  139218    |  139097    |
|       from small pool |     425    |     534    |  283097    |  282672    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     134    |    2032    |    1933    |
|       from large pool |      47    |      52    |    1110    |    1063    |
|       from small pool |      52    |      86    |     922    |     870    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      39    |      71    |  200493    |  200454    |
|       from large pool |      22    |      47    |   89817    |   89795    |
|       from small pool |      17    |      60    |  110676    |  110659    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.74 GiB total capacity; 998.11 MiB already allocated; 163.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:47 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 83        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     998 MB |    1008 MB |  640343 MB |  639345 MB |
|       from large pool |     895 MB |     911 MB |  569659 MB |  568764 MB |
|       from small pool |     103 MB |     166 MB |   70684 MB |   70581 MB |
|---------------------------------------------------------------------------|
| Active memory         |     998 MB |    1008 MB |  640343 MB |  639345 MB |
|       from large pool |     895 MB |     911 MB |  569659 MB |  568764 MB |
|       from small pool |     103 MB |     166 MB |   70684 MB |   70581 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1050 MB |   29328 MB |   28288 MB |
|       from large pool |     932 MB |     944 MB |   27312 MB |   26380 MB |
|       from small pool |     108 MB |     172 MB |    2016 MB |    1908 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   42890 KB |  345437 KB |  664513 MB |  664471 MB |
|       from large pool |   37863 KB |  327682 KB |  588751 MB |  588714 MB |
|       from small pool |    5027 KB |   26654 KB |   75761 MB |   75756 MB |
|---------------------------------------------------------------------------|
| Allocations           |     543    |     569    |  458388    |  457845    |
|       from large pool |     111    |     133    |  151153    |  151042    |
|       from small pool |     432    |     534    |  307235    |  306803    |
|---------------------------------------------------------------------------|
| Active allocs         |     543    |     569    |  458388    |  457845    |
|       from large pool |     111    |     133    |  151153    |  151042    |
|       from small pool |     432    |     534    |  307235    |  306803    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     134    |    2256    |    2165    |
|       from large pool |      37    |      53    |    1248    |    1211    |
|       from small pool |      54    |      86    |    1008    |     954    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      46    |      71    |  217843    |  217797    |
|       from large pool |      24    |      47    |   97625    |   97601    |
|       from small pool |      22    |      60    |  120218    |  120196    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 957.91 MiB already allocated; 161.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:48 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 89        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     957 MB |    1008 MB |  682867 MB |  681909 MB |
|       from large pool |     860 MB |     911 MB |  608465 MB |  607604 MB |
|       from small pool |      97 MB |     166 MB |   74401 MB |   74304 MB |
|---------------------------------------------------------------------------|
| Active memory         |     957 MB |    1008 MB |  682867 MB |  681909 MB |
|       from large pool |     860 MB |     911 MB |  608465 MB |  607604 MB |
|       from small pool |      97 MB |     166 MB |   74401 MB |   74304 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1050 MB |   31588 MB |   30546 MB |
|       from large pool |     938 MB |     944 MB |   29472 MB |   28534 MB |
|       from small pool |     104 MB |     172 MB |    2116 MB |    2012 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   86103 KB |  345437 KB |  706826 MB |  706742 MB |
|       from large pool |   79117 KB |  327682 KB |  627069 MB |  626992 MB |
|       from small pool |    6986 KB |   26654 KB |   79757 MB |   79750 MB |
|---------------------------------------------------------------------------|
| Allocations           |     538    |     569    |  485453    |  484915    |
|       from large pool |     113    |     133    |  161340    |  161227    |
|       from small pool |     425    |     534    |  324113    |  323688    |
|---------------------------------------------------------------------------|
| Active allocs         |     538    |     569    |  485453    |  484915    |
|       from large pool |     113    |     133    |  161340    |  161227    |
|       from small pool |     425    |     534    |  324113    |  323688    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |    2414    |    2318    |
|       from large pool |      44    |      53    |    1356    |    1312    |
|       from small pool |      52    |      86    |    1058    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      46    |      71    |  230893    |  230847    |
|       from large pool |      31    |      47    |  104103    |  104072    |
|       from small pool |      15    |      60    |  126790    |  126775    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.74 GiB total capacity; 929.90 MiB already allocated; 183.00 MiB free; 1020.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:48 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 97        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     929 MB |    1008 MB |  733214 MB |  732284 MB |
|       from large pool |     832 MB |     911 MB |  653243 MB |  652410 MB |
|       from small pool |      97 MB |     166 MB |   79970 MB |   79873 MB |
|---------------------------------------------------------------------------|
| Active memory         |     929 MB |    1008 MB |  733214 MB |  732284 MB |
|       from large pool |     832 MB |     911 MB |  653243 MB |  652410 MB |
|       from small pool |      97 MB |     166 MB |   79970 MB |   79873 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1020 MB |    1050 MB |   34602 MB |   33582 MB |
|       from large pool |     912 MB |     944 MB |   32282 MB |   31370 MB |
|       from small pool |     108 MB |     172 MB |    2320 MB |    2212 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   92267 KB |  345437 KB |  756919 MB |  756828 MB |
|       from large pool |   81167 KB |  327682 KB |  671237 MB |  671158 MB |
|       from small pool |   11100 KB |   26654 KB |   85681 MB |   85670 MB |
|---------------------------------------------------------------------------|
| Allocations           |     537    |     569    |  521539    |  521002    |
|       from large pool |     112    |     133    |  173526    |  173414    |
|       from small pool |     425    |     534    |  348013    |  347588    |
|---------------------------------------------------------------------------|
| Active allocs         |     537    |     569    |  521539    |  521002    |
|       from large pool |     112    |     133    |  173526    |  173414    |
|       from small pool |     425    |     534    |  348013    |  347588    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |    2658    |    2561    |
|       from large pool |      43    |      53    |    1498    |    1455    |
|       from small pool |      54    |      86    |    1160    |    1106    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |      71    |  248598    |  248554    |
|       from large pool |      28    |      47    |  112371    |  112343    |
|       from small pool |      16    |      60    |  136227    |  136211    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 968.75 MiB already allocated; 165.00 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:48 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 99        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     964 MB |    1008 MB |  734744 MB |  733780 MB |
|       from large pool |     863 MB |     911 MB |  654450 MB |  653587 MB |
|       from small pool |     101 MB |     166 MB |   80293 MB |   80192 MB |
|---------------------------------------------------------------------------|
| Active memory         |     964 MB |    1008 MB |  734744 MB |  733780 MB |
|       from large pool |     863 MB |     911 MB |  654450 MB |  653587 MB |
|       from small pool |     101 MB |     166 MB |   80293 MB |   80192 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1038 MB |    1050 MB |   34768 MB |   33730 MB |
|       from large pool |     922 MB |     944 MB |   32404 MB |   31482 MB |
|       from small pool |     116 MB |     172 MB |    2364 MB |    2248 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   75626 KB |  345437 KB |  758102 MB |  758028 MB |
|       from large pool |   60331 KB |  327682 KB |  672090 MB |  672031 MB |
|       from small pool |   15295 KB |   26654 KB |   86011 MB |   85997 MB |
|---------------------------------------------------------------------------|
| Allocations           |     520    |     569    |  522825    |  522305    |
|       from large pool |     121    |     133    |  173734    |  173613    |
|       from small pool |     399    |     534    |  349091    |  348692    |
|---------------------------------------------------------------------------|
| Active allocs         |     520    |     569    |  522825    |  522305    |
|       from large pool |     121    |     133    |  173734    |  173613    |
|       from small pool |     399    |     534    |  349091    |  348692    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     134    |    2685    |    2585    |
|       from large pool |      42    |      53    |    1503    |    1461    |
|       from small pool |      58    |      86    |    1182    |    1124    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      50    |      71    |  249234    |  249184    |
|       from large pool |      23    |      47    |  112516    |  112493    |
|       from small pool |      27    |      60    |  136718    |  136691    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 971.34 MiB already allocated; 157.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:49 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 102       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     971 MB |    1008 MB |  744829 MB |  743858 MB |
|       from large pool |     874 MB |     911 MB |  663127 MB |  662253 MB |
|       from small pool |      97 MB |     166 MB |   81701 MB |   81604 MB |
|---------------------------------------------------------------------------|
| Active memory         |     971 MB |    1008 MB |  744829 MB |  743858 MB |
|       from large pool |     874 MB |     911 MB |  663127 MB |  662253 MB |
|       from small pool |      97 MB |     166 MB |   81701 MB |   81604 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1050 MB |   35634 MB |   34588 MB |
|       from large pool |     934 MB |     944 MB |   33224 MB |   32290 MB |
|       from small pool |     112 MB |     172 MB |    2410 MB |    2298 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   76452 KB |  345437 KB |  769586 MB |  769511 MB |
|       from large pool |   61268 KB |  327682 KB |  682087 MB |  682027 MB |
|       from small pool |   15184 KB |   26654 KB |   87498 MB |   87483 MB |
|---------------------------------------------------------------------------|
| Allocations           |     557    |     569    |  530951    |  530394    |
|       from large pool |     130    |     133    |  176284    |  176154    |
|       from small pool |     427    |     534    |  354667    |  354240    |
|---------------------------------------------------------------------------|
| Active allocs         |     557    |     569    |  530951    |  530394    |
|       from large pool |     130    |     133    |  176284    |  176154    |
|       from small pool |     427    |     534    |  354667    |  354240    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     101    |     134    |    2747    |    2646    |
|       from large pool |      45    |      53    |    1542    |    1497    |
|       from small pool |      56    |      86    |    1205    |    1149    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      56    |      71    |  253010    |  252954    |
|       from large pool |      24    |      47    |  114180    |  114156    |
|       from small pool |      32    |      60    |  138830    |  138798    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 980.99 MiB already allocated; 163.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:49 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 107       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     980 MB |    1008 MB |  767990 MB |  767009 MB |
|       from large pool |     883 MB |     911 MB |  683047 MB |  682163 MB |
|       from small pool |      97 MB |     166 MB |   84943 MB |   84846 MB |
|---------------------------------------------------------------------------|
| Active memory         |     980 MB |    1008 MB |  767990 MB |  767009 MB |
|       from large pool |     883 MB |     911 MB |  683047 MB |  682163 MB |
|       from small pool |      97 MB |     166 MB |   84943 MB |   84846 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1050 MB |   37882 MB |   36842 MB |
|       from large pool |     930 MB |     944 MB |   35338 MB |   34408 MB |
|       from small pool |     110 MB |     172 MB |    2544 MB |    2434 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   60426 KB |  345437 KB |     773 GB |     773 GB |
|       from large pool |   47261 KB |  327682 KB |     685 GB |     685 GB |
|       from small pool |   13165 KB |   26654 KB |      88 GB |      88 GB |
|---------------------------------------------------------------------------|
| Allocations           |     538    |     569    |  548370    |  547832    |
|       from large pool |     113    |     133    |  181258    |  181145    |
|       from small pool |     425    |     534    |  367112    |  366687    |
|---------------------------------------------------------------------------|
| Active allocs         |     538    |     569    |  548370    |  547832    |
|       from large pool |     113    |     133    |  181258    |  181145    |
|       from small pool |     425    |     534    |  367112    |  366687    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |    2917    |    2820    |
|       from large pool |      42    |      53    |    1645    |    1603    |
|       from small pool |      55    |      86    |    1272    |    1217    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      71    |  260622    |  260590    |
|       from large pool |      12    |      47    |  117234    |  117222    |
|       from small pool |      20    |      60    |  143388    |  143368    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 988.76 MiB already allocated; 157.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:49 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 111       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     988 MB |    1008 MB |     769 GB |     768 GB |
|       from large pool |     891 MB |     911 MB |     684 GB |     683 GB |
|       from small pool |      97 MB |     166 MB |      85 GB |      85 GB |
|---------------------------------------------------------------------------|
| Active memory         |     988 MB |    1008 MB |     769 GB |     768 GB |
|       from large pool |     891 MB |     911 MB |     684 GB |     683 GB |
|       from small pool |      97 MB |     166 MB |      85 GB |      85 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1050 MB |   39712 MB |   38666 MB |
|       from large pool |     938 MB |     944 MB |   37064 MB |   36126 MB |
|       from small pool |     108 MB |     172 MB |    2648 MB |    2540 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   58616 KB |  345437 KB |     793 GB |     793 GB |
|       from large pool |   47469 KB |  327682 KB |     701 GB |     701 GB |
|       from small pool |   11147 KB |   26654 KB |      91 GB |      91 GB |
|---------------------------------------------------------------------------|
| Allocations           |     530    |     569    |  563548    |  563018    |
|       from large pool |     115    |     133    |  185732    |  185617    |
|       from small pool |     415    |     534    |  377816    |  377401    |
|---------------------------------------------------------------------------|
| Active allocs         |     530    |     569    |  563548    |  563018    |
|       from large pool |     115    |     133    |  185732    |  185617    |
|       from small pool |     415    |     534    |  377816    |  377401    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      93    |     134    |    3049    |    2956    |
|       from large pool |      39    |      53    |    1725    |    1686    |
|       from small pool |      54    |      86    |    1324    |    1270    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      71    |  268029    |  267993    |
|       from large pool |      20    |      47    |  120139    |  120119    |
|       from small pool |      16    |      60    |  147890    |  147874    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 954.96 MiB already allocated; 159.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:50 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 115       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     954 MB |    1008 MB |     790 GB |     789 GB |
|       from large pool |     857 MB |     911 MB |     703 GB |     702 GB |
|       from small pool |      97 MB |     166 MB |      87 GB |      87 GB |
|---------------------------------------------------------------------------|
| Active memory         |     954 MB |    1008 MB |     790 GB |     789 GB |
|       from large pool |     857 MB |     911 MB |     703 GB |     702 GB |
|       from small pool |      97 MB |     166 MB |      87 GB |      87 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1044 MB |    1050 MB |   40956 MB |   39912 MB |
|       from large pool |     938 MB |     944 MB |   38174 MB |   37236 MB |
|       from small pool |     106 MB |     172 MB |    2782 MB |    2676 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   91173 KB |  345437 KB |     813 GB |     813 GB |
|       from large pool |   82008 KB |  327682 KB |     719 GB |     719 GB |
|       from small pool |    9165 KB |   26654 KB |      93 GB |      93 GB |
|---------------------------------------------------------------------------|
| Allocations           |     506    |     569    |  578801    |  578295    |
|       from large pool |      97    |     133    |  190876    |  190779    |
|       from small pool |     409    |     534    |  387925    |  387516    |
|---------------------------------------------------------------------------|
| Active allocs         |     506    |     569    |  578801    |  578295    |
|       from large pool |      97    |     133    |  190876    |  190779    |
|       from small pool |     409    |     534    |  387925    |  387516    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     134    |    3165    |    3071    |
|       from large pool |      41    |      53    |    1774    |    1733    |
|       from small pool |      53    |      86    |    1391    |    1338    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      46    |      71    |  275501    |  275455    |
|       from large pool |      29    |      47    |  123627    |  123598    |
|       from small pool |      17    |      60    |  151874    |  151857    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 11.74 GiB total capacity; 958.82 MiB already allocated; 167.00 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:50 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 118       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     958 MB |    1008 MB |     812 GB |     811 GB |
|       from large pool |     861 MB |     911 MB |     722 GB |     721 GB |
|       from small pool |      97 MB |     166 MB |      89 GB |      89 GB |
|---------------------------------------------------------------------------|
| Active memory         |     958 MB |    1008 MB |     812 GB |     811 GB |
|       from large pool |     861 MB |     911 MB |     722 GB |     721 GB |
|       from small pool |      97 MB |     166 MB |      89 GB |      89 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1036 MB |    1050 MB |   42504 MB |   41468 MB |
|       from large pool |     930 MB |     944 MB |   39652 MB |   38722 MB |
|       from small pool |     106 MB |     172 MB |    2852 MB |    2746 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   79033 KB |  345437 KB |     835 GB |     835 GB |
|       from large pool |   69852 KB |  327682 KB |     739 GB |     739 GB |
|       from small pool |    9181 KB |   26654 KB |      96 GB |      96 GB |
|---------------------------------------------------------------------------|
| Allocations           |     464    |     569    |  593925    |  593461    |
|       from large pool |      62    |     133    |  196096    |  196034    |
|       from small pool |     402    |     534    |  397829    |  397427    |
|---------------------------------------------------------------------------|
| Active allocs         |     464    |     569    |  593925    |  593461    |
|       from large pool |      62    |     133    |  196096    |  196034    |
|       from small pool |     402    |     534    |  397829    |  397427    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     134    |    3246    |    3172    |
|       from large pool |      21    |      53    |    1820    |    1799    |
|       from small pool |      53    |      86    |    1426    |    1373    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      71    |  283115    |  283081    |
|       from large pool |      19    |      47    |  127007    |  126988    |
|       from small pool |      15    |      60    |  156108    |  156093    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 955.97 MiB already allocated; 159.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:50 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 121       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     955 MB |    1008 MB |     814 GB |     813 GB |
|       from large pool |     858 MB |     911 MB |     724 GB |     723 GB |
|       from small pool |      97 MB |     166 MB |      90 GB |      89 GB |
|---------------------------------------------------------------------------|
| Active memory         |     955 MB |    1008 MB |     814 GB |     813 GB |
|       from large pool |     858 MB |     911 MB |     724 GB |     723 GB |
|       from small pool |      97 MB |     166 MB |      90 GB |      89 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1044 MB |    1050 MB |   43310 MB |   42266 MB |
|       from large pool |     938 MB |     944 MB |   40412 MB |   39474 MB |
|       from small pool |     106 MB |     172 MB |    2898 MB |    2792 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   90143 KB |  345437 KB |     837 GB |     837 GB |
|       from large pool |   81061 KB |  327682 KB |     740 GB |     740 GB |
|       from small pool |    9082 KB |   26654 KB |      96 GB |      96 GB |
|---------------------------------------------------------------------------|
| Allocations           |     520    |     569    |  595214    |  594694    |
|       from large pool |     125    |     133    |  196543    |  196418    |
|       from small pool |     395    |     534    |  398671    |  398276    |
|---------------------------------------------------------------------------|
| Active allocs         |     520    |     569    |  595214    |  594694    |
|       from large pool |     125    |     133    |  196543    |  196418    |
|       from small pool |     395    |     534    |  398671    |  398276    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     101    |     134    |    3307    |    3206    |
|       from large pool |      48    |      53    |    1858    |    1810    |
|       from small pool |      53    |      86    |    1449    |    1396    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      71    |  283686    |  283643    |
|       from large pool |      26    |      47    |  127258    |  127232    |
|       from small pool |      17    |      60    |  156428    |  156411    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 968.88 MiB already allocated; 171.00 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:51 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 127       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     928 MB |    1008 MB |     894 GB |     893 GB |
|       from large pool |     828 MB |     911 MB |     796 GB |     795 GB |
|       from small pool |     100 MB |     166 MB |      97 GB |      97 GB |
|---------------------------------------------------------------------------|
| Active memory         |     928 MB |    1008 MB |     894 GB |     893 GB |
|       from large pool |     828 MB |     911 MB |     796 GB |     795 GB |
|       from small pool |     100 MB |     166 MB |      97 GB |      97 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1032 MB |    1050 MB |   45136 MB |   44104 MB |
|       from large pool |     926 MB |     944 MB |   42120 MB |   41194 MB |
|       from small pool |     106 MB |     172 MB |    3016 MB |    2910 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   64633 KB |  345437 KB |     920 GB |     920 GB |
|       from large pool |   59387 KB |  327682 KB |     816 GB |     816 GB |
|       from small pool |    5245 KB |   26654 KB |     104 GB |     104 GB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     569    |  648406    |  647878    |
|       from large pool |     109    |     133    |  215980    |  215871    |
|       from small pool |     419    |     534    |  432426    |  432007    |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     569    |  648406    |  647878    |
|       from large pool |     109    |     133    |  215980    |  215871    |
|       from small pool |     419    |     534    |  432426    |  432007    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |    3441    |    3345    |
|       from large pool |      43    |      53    |    1933    |    1890    |
|       from small pool |      53    |      86    |    1508    |    1455    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      71    |  309618    |  309581    |
|       from large pool |      21    |      47    |  139714    |  139693    |
|       from small pool |      16    |      60    |  169904    |  169888    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 920.70 MiB already allocated; 161.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 132       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     900 MB |    1008 MB |     926 GB |     925 GB |
|       from large pool |     802 MB |     911 MB |     825 GB |     825 GB |
|       from small pool |      97 MB |     166 MB |     100 GB |     100 GB |
|---------------------------------------------------------------------------|
| Active memory         |     900 MB |    1008 MB |     926 GB |     925 GB |
|       from large pool |     802 MB |     911 MB |     825 GB |     825 GB |
|       from small pool |      97 MB |     166 MB |     100 GB |     100 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1050 MB |   46416 MB |   45374 MB |
|       from large pool |     922 MB |     944 MB |   43322 MB |   42400 MB |
|       from small pool |     120 MB |     172 MB |    3094 MB |    2974 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  122728 KB |  345437 KB |     953 GB |     953 GB |
|       from large pool |   99334 KB |  327682 KB |     846 GB |     845 GB |
|       from small pool |   23394 KB |   26654 KB |     107 GB |     107 GB |
|---------------------------------------------------------------------------|
| Allocations           |     526    |     569    |  670877    |  670351    |
|       from large pool |     121    |     133    |  224474    |  224353    |
|       from small pool |     405    |     534    |  446403    |  445998    |
|---------------------------------------------------------------------------|
| Active allocs         |     526    |     569    |  670877    |  670351    |
|       from large pool |     121    |     133    |  224474    |  224353    |
|       from small pool |     405    |     534    |  446403    |  445998    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     134    |    3537    |    3432    |
|       from large pool |      45    |      53    |    1990    |    1945    |
|       from small pool |      60    |      86    |    1547    |    1487    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      61    |      71    |  320636    |  320575    |
|       from large pool |      32    |      47    |  145049    |  145017    |
|       from small pool |      29    |      60    |  175587    |  175558    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 11.74 GiB total capacity; 961.08 MiB already allocated; 183.00 MiB free; 1020.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 136       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     961 MB |    1008 MB |     947 GB |     946 GB |
|       from large pool |     864 MB |     911 MB |     845 GB |     844 GB |
|       from small pool |      97 MB |     166 MB |     102 GB |     102 GB |
|---------------------------------------------------------------------------|
| Active memory         |     961 MB |    1008 MB |     947 GB |     946 GB |
|       from large pool |     864 MB |     911 MB |     845 GB |     844 GB |
|       from small pool |      97 MB |     166 MB |     102 GB |     102 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1020 MB |    1050 MB |   47988 MB |   46968 MB |
|       from large pool |     916 MB |     944 MB |   44792 MB |   43876 MB |
|       from small pool |     104 MB |     172 MB |    3196 MB |    3092 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   60330 KB |  345437 KB |     975 GB |     975 GB |
|       from large pool |   53248 KB |  327682 KB |     865 GB |     865 GB |
|       from small pool |    7082 KB |   31651 KB |     109 GB |     109 GB |
|---------------------------------------------------------------------------|
| Allocations           |     524    |     569    |  686104    |  685580    |
|       from large pool |     109    |     133    |  229457    |  229348    |
|       from small pool |     415    |     534    |  456647    |  456232    |
|---------------------------------------------------------------------------|
| Active allocs         |     524    |     569    |  686104    |  685580    |
|       from large pool |     109    |     133    |  229457    |  229348    |
|       from small pool |     415    |     534    |  456647    |  456232    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     134    |    3653    |    3565    |
|       from large pool |      36    |      53    |    2055    |    2019    |
|       from small pool |      52    |      86    |    1598    |    1546    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      71    |  327745    |  327714    |
|       from large pool |      16    |      47    |  148252    |  148236    |
|       from small pool |      15    |      60    |  179493    |  179478    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 964.81 MiB already allocated; 161.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 142       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     964 MB |    1009 MB |     964 GB |     963 GB |
|       from large pool |     867 MB |     911 MB |     860 GB |     859 GB |
|       from small pool |      97 MB |     166 MB |     104 GB |     103 GB |
|---------------------------------------------------------------------------|
| Active memory         |     964 MB |    1009 MB |     964 GB |     963 GB |
|       from large pool |     867 MB |     911 MB |     860 GB |     859 GB |
|       from small pool |      97 MB |     166 MB |     104 GB |     103 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1050 MB |   49346 MB |   48304 MB |
|       from large pool |     936 MB |     944 MB |   46070 MB |   45134 MB |
|       from small pool |     106 MB |     172 MB |    3276 MB |    3170 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   79042 KB |  345437 KB |     992 GB |     992 GB |
|       from large pool |   69973 KB |  327682 KB |     880 GB |     880 GB |
|       from small pool |    9069 KB |   31651 KB |     111 GB |     111 GB |
|---------------------------------------------------------------------------|
| Allocations           |     514    |     569    |  696929    |  696415    |
|       from large pool |     119    |     133    |  233381    |  233262    |
|       from small pool |     395    |     534    |  463548    |  463153    |
|---------------------------------------------------------------------------|
| Active allocs         |     514    |     569    |  696929    |  696415    |
|       from large pool |     119    |     133    |  233381    |  233262    |
|       from small pool |     395    |     534    |  463548    |  463153    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     134    |    3746    |    3654    |
|       from large pool |      39    |      53    |    2108    |    2069    |
|       from small pool |      53    |      86    |    1638    |    1585    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      71    |  333222    |  333175    |
|       from large pool |      18    |      47    |  150812    |  150794    |
|       from small pool |      29    |      60    |  182410    |  182381    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 11.74 GiB total capacity; 924.49 MiB already allocated; 187.00 MiB free; 1016.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:53 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 145       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     924 MB |    1009 MB |     987 GB |     986 GB |
|       from large pool |     827 MB |     911 MB |     880 GB |     879 GB |
|       from small pool |      97 MB |     166 MB |     107 GB |     107 GB |
|---------------------------------------------------------------------------|
| Active memory         |     924 MB |    1009 MB |     987 GB |     986 GB |
|       from large pool |     827 MB |     911 MB |     880 GB |     879 GB |
|       from small pool |      97 MB |     166 MB |     107 GB |     107 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1016 MB |    1050 MB |   50796 MB |   49780 MB |
|       from large pool |     902 MB |     944 MB |   47444 MB |   46542 MB |
|       from small pool |     114 MB |     172 MB |    3352 MB |    3238 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   93711 KB |  345437 KB |    1015 GB |    1015 GB |
|       from large pool |   76370 KB |  327682 KB |     900 GB |     900 GB |
|       from small pool |   17340 KB |   31651 KB |     114 GB |     114 GB |
|---------------------------------------------------------------------------|
| Allocations           |     512    |     569    |  714313    |  713801    |
|       from large pool |      93    |     133    |  238594    |  238501    |
|       from small pool |     419    |     534    |  475719    |  475300    |
|---------------------------------------------------------------------------|
| Active allocs         |     512    |     569    |  714313    |  713801    |
|       from large pool |      93    |     133    |  238594    |  238501    |
|       from small pool |     419    |     534    |  475719    |  475300    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     134    |    3843    |    3752    |
|       from large pool |      34    |      53    |    2167    |    2133    |
|       from small pool |      57    |      86    |    1676    |    1619    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      71    |  341218    |  341171    |
|       from large pool |      16    |      47    |  154267    |  154251    |
|       from small pool |      31    |      60    |  186951    |  186920    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.74 GiB total capacity; 972.54 MiB already allocated; 161.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:53 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 147       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     972 MB |    1009 MB |     994 GB |     993 GB |
|       from large pool |     875 MB |     911 MB |     886 GB |     885 GB |
|       from small pool |      97 MB |     166 MB |     108 GB |     108 GB |
|---------------------------------------------------------------------------|
| Active memory         |     972 MB |    1009 MB |     994 GB |     993 GB |
|       from large pool |     875 MB |     911 MB |     886 GB |     885 GB |
|       from small pool |      97 MB |     166 MB |     108 GB |     108 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1050 MB |   50964 MB |   49922 MB |
|       from large pool |     938 MB |     944 MB |   47560 MB |   46622 MB |
|       from small pool |     104 MB |     172 MB |    3404 MB |    3300 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   71127 KB |  345437 KB |    1023 GB |    1023 GB |
|       from large pool |   64124 KB |  327682 KB |     907 GB |     907 GB |
|       from small pool |    7002 KB |   31651 KB |     115 GB |     115 GB |
|---------------------------------------------------------------------------|
| Allocations           |     526    |     569    |  720112    |  719586    |
|       from large pool |     111    |     133    |  240900    |  240789    |
|       from small pool |     415    |     534    |  479212    |  478797    |
|---------------------------------------------------------------------------|
| Active allocs         |     526    |     569    |  720112    |  719586    |
|       from large pool |     111    |     133    |  240900    |  240789    |
|       from small pool |     415    |     534    |  479212    |  478797    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     134    |    3873    |    3787    |
|       from large pool |      34    |      53    |    2171    |    2137    |
|       from small pool |      52    |      86    |    1702    |    1650    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      71    |  344072    |  344040    |
|       from large pool |      17    |      47    |  155822    |  155805    |
|       from small pool |      15    |      60    |  188250    |  188235    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 941.08 MiB already allocated; 163.00 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:53 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 150       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     941 MB |    1009 MB |    1002 GB |    1001 GB |
|       from large pool |     843 MB |     911 MB |     893 GB |     892 GB |
|       from small pool |      97 MB |     166 MB |     108 GB |     108 GB |
|---------------------------------------------------------------------------|
| Active memory         |     941 MB |    1009 MB |    1002 GB |    1001 GB |
|       from large pool |     843 MB |     911 MB |     893 GB |     892 GB |
|       from small pool |      97 MB |     166 MB |     108 GB |     108 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1050 MB |   51680 MB |   50640 MB |
|       from large pool |     934 MB |     944 MB |   48200 MB |   47266 MB |
|       from small pool |     106 MB |     172 MB |    3480 MB |    3374 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  101290 KB |  345437 KB |    1030 GB |    1030 GB |
|       from large pool |   92235 KB |  327682 KB |     914 GB |     914 GB |
|       from small pool |    9055 KB |   31651 KB |     116 GB |     116 GB |
|---------------------------------------------------------------------------|
| Allocations           |     536    |     569    |  724892    |  724356    |
|       from large pool |     127    |     133    |  242684    |  242557    |
|       from small pool |     409    |     534    |  482208    |  481799    |
|---------------------------------------------------------------------------|
| Active allocs         |     536    |     569    |  724892    |  724356    |
|       from large pool |     127    |     133    |  242684    |  242557    |
|       from small pool |     409    |     534    |  482208    |  481799    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     134    |    3946    |    3841    |
|       from large pool |      52    |      53    |    2206    |    2154    |
|       from small pool |      53    |      86    |    1740    |    1687    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      46    |      71    |  346454    |  346408    |
|       from large pool |      25    |      47    |  156952    |  156927    |
|       from small pool |      21    |      60    |  189502    |  189481    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 957.91 MiB already allocated; 168.75 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 156       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     957 MB |    1009 MB |    1038 GB |    1038 GB |
|       from large pool |     860 MB |     911 MB |     925 GB |     924 GB |
|       from small pool |      97 MB |     166 MB |     113 GB |     113 GB |
|---------------------------------------------------------------------------|
| Active memory         |     957 MB |    1009 MB |    1038 GB |    1038 GB |
|       from large pool |     860 MB |     911 MB |     925 GB |     924 GB |
|       from small pool |      97 MB |     166 MB |     113 GB |     113 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1034 MB |    1050 MB |   54058 MB |   53024 MB |
|       from large pool |     924 MB |     944 MB |   50466 MB |   49542 MB |
|       from small pool |     110 MB |     172 MB |    3592 MB |    3482 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   77916 KB |  345437 KB |    1067 GB |    1067 GB |
|       from large pool |   64714 KB |  327682 KB |     946 GB |     946 GB |
|       from small pool |   13202 KB |   31651 KB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| Allocations           |     546    |     569    |     751 K  |     751 K  |
|       from large pool |     121    |     133    |     251 K  |     251 K  |
|       from small pool |     425    |     534    |     500 K  |     500 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     546    |     569    |     751 K  |     751 K  |
|       from large pool |     121    |     133    |     251 K  |     251 K  |
|       from small pool |     425    |     534    |     500 K  |     500 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |    4110    |    4014    |
|       from large pool |      41    |      53    |    2314    |    2273    |
|       from small pool |      55    |      86    |    1796    |    1741    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      71    |  359061    |  359028    |
|       from large pool |      16    |      47    |  162435    |  162419    |
|       from small pool |      17    |      60    |  196626    |  196609    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 11.74 GiB total capacity; 922.42 MiB already allocated; 174.75 MiB free; 1.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 158       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     922 MB |    1009 MB |    1047 GB |    1047 GB |
|       from large pool |     825 MB |     911 MB |     934 GB |     933 GB |
|       from small pool |      97 MB |     166 MB |     113 GB |     113 GB |
|---------------------------------------------------------------------------|
| Active memory         |     922 MB |    1009 MB |    1047 GB |    1047 GB |
|       from large pool |     825 MB |     911 MB |     934 GB |     933 GB |
|       from small pool |      97 MB |     166 MB |     113 GB |     113 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1028 MB |    1050 MB |   54852 MB |   53824 MB |
|       from large pool |     920 MB |     944 MB |   51222 MB |   50302 MB |
|       from small pool |     108 MB |     172 MB |    3630 MB |    3522 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  108110 KB |  345437 KB |    1077 GB |    1077 GB |
|       from large pool |   96955 KB |  327682 KB |     956 GB |     955 GB |
|       from small pool |   11155 KB |   31651 KB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     569    |     757 K  |     757 K  |
|       from large pool |      93    |     133    |     253 K  |     253 K  |
|       from small pool |     409    |     534    |     504 K  |     503 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     569    |     757 K  |     757 K  |
|       from large pool |      93    |     133    |     253 K  |     253 K  |
|       from small pool |     409    |     534    |     504 K  |     503 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      90    |     134    |    4156    |    4066    |
|       from large pool |      36    |      53    |    2341    |    2305    |
|       from small pool |      54    |      86    |    1815    |    1761    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      71    |  361879    |  361843    |
|       from large pool |      19    |      47    |  163922    |  163903    |
|       from small pool |      17    |      60    |  197957    |  197940    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.74 GiB total capacity; 949.27 MiB already allocated; 162.75 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 161       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     949 MB |    1009 MB |    1056 GB |    1055 GB |
|       from large pool |     852 MB |     911 MB |     942 GB |     941 GB |
|       from small pool |      97 MB |     166 MB |     114 GB |     114 GB |
|---------------------------------------------------------------------------|
| Active memory         |     949 MB |    1009 MB |    1056 GB |    1055 GB |
|       from large pool |     852 MB |     911 MB |     942 GB |     941 GB |
|       from small pool |      97 MB |     166 MB |     114 GB |     114 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1050 MB |   55624 MB |   54584 MB |
|       from large pool |     932 MB |     944 MB |   51942 MB |   51010 MB |
|       from small pool |     108 MB |     172 MB |    3682 MB |    3574 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   92908 KB |  345437 KB |    1087 GB |    1086 GB |
|       from large pool |   81776 KB |  327682 KB |     964 GB |     963 GB |
|       from small pool |   11132 KB |   31651 KB |     122 GB |     122 GB |
|---------------------------------------------------------------------------|
| Allocations           |     536    |     569    |     763 K  |     762 K  |
|       from large pool |     121    |     133    |     255 K  |     255 K  |
|       from small pool |     415    |     534    |     507 K  |     507 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     536    |     569    |     763 K  |     762 K  |
|       from large pool |     121    |     133    |     255 K  |     255 K  |
|       from small pool |     415    |     534    |     507 K  |     507 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |    4214    |    4118    |
|       from large pool |      42    |      53    |    2373    |    2331    |
|       from small pool |      54    |      86    |    1841    |    1787    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      38    |      71    |  364647    |  364609    |
|       from large pool |      25    |      47    |  165242    |  165217    |
|       from small pool |      13    |      60    |  199405    |  199392    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 966.69 MiB already allocated; 156.75 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:55 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 165       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     966 MB |    1009 MB |    1107 GB |    1106 GB |
|       from large pool |     869 MB |     911 MB |     987 GB |     986 GB |
|       from small pool |      97 MB |     166 MB |     120 GB |     120 GB |
|---------------------------------------------------------------------------|
| Active memory         |     966 MB |    1009 MB |    1107 GB |    1106 GB |
|       from large pool |     869 MB |     911 MB |     987 GB |     986 GB |
|       from small pool |      97 MB |     166 MB |     120 GB |     120 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1050 MB |   57502 MB |   56456 MB |
|       from large pool |     942 MB |     944 MB |   53704 MB |   52762 MB |
|       from small pool |     104 MB |     172 MB |    3798 MB |    3694 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81217 KB |  345437 KB |    1139 GB |    1139 GB |
|       from large pool |   74189 KB |  327682 KB |    1010 GB |    1009 GB |
|       from small pool |    7028 KB |   31651 KB |     129 GB |     129 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     569    |     802 K  |     801 K  |
|       from large pool |     121    |     133    |     269 K  |     269 K  |
|       from small pool |     395    |     534    |     533 K  |     532 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     569    |     802 K  |     801 K  |
|       from large pool |     121    |     133    |     269 K  |     269 K  |
|       from small pool |     395    |     534    |     533 K  |     532 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |    4363    |    4265    |
|       from large pool |      46    |      53    |    2464    |    2418    |
|       from small pool |      52    |      86    |    1899    |    1847    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |      71    |  383512    |  383468    |
|       from large pool |      21    |      47    |  174408    |  174387    |
|       from small pool |      23    |      60    |  209104    |  209081    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 11.74 GiB total capacity; 920.85 MiB already allocated; 172.75 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:55 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 169       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     920 MB |    1009 MB |    1131 GB |    1130 GB |
|       from large pool |     823 MB |     911 MB |    1009 GB |    1008 GB |
|       from small pool |      97 MB |     166 MB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| Active memory         |     920 MB |    1009 MB |    1131 GB |    1130 GB |
|       from large pool |     823 MB |     911 MB |    1009 GB |    1008 GB |
|       from small pool |      97 MB |     166 MB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1030 MB |    1050 MB |   58586 MB |   57556 MB |
|       from large pool |     926 MB |     944 MB |   54726 MB |   53800 MB |
|       from small pool |     104 MB |     172 MB |    3860 MB |    3756 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  111765 KB |  345437 KB |    1162 GB |    1161 GB |
|       from large pool |  104754 KB |  327682 KB |    1031 GB |    1031 GB |
|       from small pool |    7011 KB |   31651 KB |     130 GB |     130 GB |
|---------------------------------------------------------------------------|
| Allocations           |     526    |     569    |     815 K  |     815 K  |
|       from large pool |     121    |     133    |     275 K  |     275 K  |
|       from small pool |     405    |     534    |     540 K  |     539 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     526    |     569    |     815 K  |     815 K  |
|       from large pool |     121    |     133    |     275 K  |     275 K  |
|       from small pool |     405    |     534    |     540 K  |     539 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     104    |     134    |    4448    |    4344    |
|       from large pool |      52    |      53    |    2518    |    2466    |
|       from small pool |      52    |      86    |    1930    |    1878    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      71    |  390338    |  390291    |
|       from large pool |      33    |      47    |  178336    |  178303    |
|       from small pool |      14    |      60    |  212002    |  211988    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.74 GiB total capacity; 967.89 MiB already allocated; 164.75 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:55 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 172       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     967 MB |    1009 MB |    1132 GB |    1131 GB |
|       from large pool |     870 MB |     911 MB |    1010 GB |    1009 GB |
|       from small pool |      97 MB |     166 MB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| Active memory         |     967 MB |    1009 MB |    1132 GB |    1131 GB |
|       from large pool |     870 MB |     911 MB |    1010 GB |    1009 GB |
|       from small pool |      97 MB |     166 MB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1038 MB |    1050 MB |   59442 MB |   58404 MB |
|       from large pool |     926 MB |     944 MB |   55538 MB |   54612 MB |
|       from small pool |     112 MB |     172 MB |    3904 MB |    3792 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   71791 KB |  345437 KB |    1163 GB |    1162 GB |
|       from large pool |   56510 KB |  327682 KB |    1032 GB |    1032 GB |
|       from small pool |   15280 KB |   31651 KB |     130 GB |     130 GB |
|---------------------------------------------------------------------------|
| Allocations           |     506    |     569    |     816 K  |     816 K  |
|       from large pool |     111    |     133    |     275 K  |     275 K  |
|       from small pool |     395    |     534    |     541 K  |     541 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     506    |     569    |     816 K  |     816 K  |
|       from large pool |     111    |     133    |     275 K  |     275 K  |
|       from small pool |     395    |     534    |     541 K  |     541 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      90    |     134    |    4498    |    4408    |
|       from large pool |      34    |      53    |    2546    |    2512    |
|       from small pool |      56    |      86    |    1952    |    1896    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      52    |      71    |  390999    |  390947    |
|       from large pool |      17    |      47    |  178430    |  178413    |
|       from small pool |      35    |      60    |  212569    |  212534    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.74 GiB total capacity; 946.36 MiB already allocated; 180.75 MiB free; 1022.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 177       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     946 MB |    1009 MB |    1198 GB |    1197 GB |
|       from large pool |     849 MB |     911 MB |    1069 GB |    1068 GB |
|       from small pool |      97 MB |     167 MB |     129 GB |     129 GB |
|---------------------------------------------------------------------------|
| Active memory         |     946 MB |    1009 MB |    1198 GB |    1197 GB |
|       from large pool |     849 MB |     911 MB |    1069 GB |    1068 GB |
|       from small pool |      97 MB |     167 MB |     129 GB |     129 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1022 MB |    1050 MB |   61368 MB |   60346 MB |
|       from large pool |     914 MB |     944 MB |   57338 MB |   56424 MB |
|       from small pool |     108 MB |     172 MB |    4030 MB |    3922 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   77451 KB |  345437 KB |    1231 GB |    1231 GB |
|       from large pool |   66325 KB |  327682 KB |    1092 GB |    1092 GB |
|       from small pool |   11125 KB |   31651 KB |     138 GB |     138 GB |
|---------------------------------------------------------------------------|
| Allocations           |     518    |     569    |     867 K  |     866 K  |
|       from large pool |     113    |     133    |     292 K  |     292 K  |
|       from small pool |     405    |     534    |     574 K  |     574 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     518    |     569    |     867 K  |     866 K  |
|       from large pool |     113    |     133    |     292 K  |     292 K  |
|       from small pool |     405    |     534    |     574 K  |     574 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     134    |    4650    |    4555    |
|       from large pool |      41    |      53    |    2635    |    2594    |
|       from small pool |      54    |      86    |    2015    |    1961    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      71    |  415160    |  415123    |
|       from large pool |      21    |      47    |  189725    |  189704    |
|       from small pool |      16    |      60    |  225435    |  225419    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 11.74 GiB total capacity; 979.10 MiB already allocated; 160.75 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 186       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     979 MB |    1009 MB |    1252 GB |    1251 GB |
|       from large pool |     881 MB |     911 MB |    1115 GB |    1114 GB |
|       from small pool |      97 MB |     167 MB |     136 GB |     136 GB |
|---------------------------------------------------------------------------|
| Active memory         |     979 MB |    1009 MB |    1252 GB |    1251 GB |
|       from large pool |     881 MB |     911 MB |    1115 GB |    1114 GB |
|       from small pool |      97 MB |     167 MB |     136 GB |     136 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1050 MB |   64172 MB |   63130 MB |
|       from large pool |     938 MB |     944 MB |   59944 MB |   59006 MB |
|       from small pool |     104 MB |     172 MB |    4228 MB |    4124 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   64412 KB |  345437 KB |    1287 GB |    1287 GB |
|       from large pool |   57362 KB |  327682 KB |    1140 GB |    1140 GB |
|       from small pool |    7050 KB |   31651 KB |     146 GB |     146 GB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     569    |     910 K  |     910 K  |
|       from large pool |     113    |     133    |     305 K  |     305 K  |
|       from small pool |     415    |     534    |     604 K  |     604 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     569    |     910 K  |     910 K  |
|       from large pool |     113    |     133    |     305 K  |     305 K  |
|       from small pool |     415    |     534    |     604 K  |     604 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      93    |     134    |    4867    |    4774    |
|       from large pool |      41    |      53    |    2753    |    2712    |
|       from small pool |      52    |      86    |    2114    |    2062    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      71    |  435267    |  435233    |
|       from large pool |      20    |      47    |  198075    |  198055    |
|       from small pool |      14    |      60    |  237192    |  237178    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.74 GiB total capacity; 937.54 MiB already allocated; 170.75 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:58 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 194       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     937 MB |    1009 MB |    1319 GB |    1319 GB |
|       from large pool |     840 MB |     911 MB |    1175 GB |    1174 GB |
|       from small pool |      97 MB |     167 MB |     144 GB |     144 GB |
|---------------------------------------------------------------------------|
| Active memory         |     937 MB |    1009 MB |    1319 GB |    1319 GB |
|       from large pool |     840 MB |     911 MB |    1175 GB |    1174 GB |
|       from small pool |      97 MB |     167 MB |     144 GB |     144 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1032 MB |    1050 MB |   67684 MB |   66652 MB |
|       from large pool |     924 MB |     944 MB |   63294 MB |   62370 MB |
|       from small pool |     108 MB |     172 MB |    4390 MB |    4282 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   96728 KB |  345437 KB |    1358 GB |    1358 GB |
|       from large pool |   85617 KB |  327682 KB |    1204 GB |    1203 GB |
|       from small pool |   11111 KB |   31651 KB |     154 GB |     154 GB |
|---------------------------------------------------------------------------|
| Allocations           |     524    |     569    |     961 K  |     960 K  |
|       from large pool |     119    |     133    |     322 K  |     322 K  |
|       from small pool |     405    |     534    |     638 K  |     638 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     524    |     569    |     961 K  |     960 K  |
|       from large pool |     119    |     133    |     322 K  |     322 K  |
|       from small pool |     405    |     534    |     638 K  |     638 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     134    |    5107    |    5007    |
|       from large pool |      46    |      53    |    2912    |    2866    |
|       from small pool |      54    |      86    |    2195    |    2141    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      71    |  459743    |  459696    |
|       from large pool |      25    |      47    |  208960    |  208935    |
|       from small pool |      22    |      60    |  250783    |  250761    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:27:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 976.25 MiB already allocated; 156.75 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:27:59 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 197       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     976 MB |    1009 MB |    1336 GB |    1335 GB |
|       from large pool |     879 MB |     911 MB |    1190 GB |    1189 GB |
|       from small pool |      97 MB |     167 MB |     146 GB |     146 GB |
|---------------------------------------------------------------------------|
| Active memory         |     976 MB |    1009 MB |    1336 GB |    1335 GB |
|       from large pool |     879 MB |     911 MB |    1190 GB |    1189 GB |
|       from small pool |      97 MB |     167 MB |     146 GB |     146 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1050 MB |   68934 MB |   67888 MB |
|       from large pool |     930 MB |     944 MB |   64478 MB |   63548 MB |
|       from small pool |     116 MB |     172 MB |    4456 MB |    4340 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   71421 KB |  345437 KB |    1375 GB |    1375 GB |
|       from large pool |   52069 KB |  327682 KB |    1219 GB |    1219 GB |
|       from small pool |   19352 KB |   31651 KB |     156 GB |     156 GB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     569    |     974 K  |     973 K  |
|       from large pool |     101    |     133    |     326 K  |     326 K  |
|       from small pool |     401    |     534    |     647 K  |     647 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     569    |     974 K  |     973 K  |
|       from large pool |     101    |     133    |     326 K  |     326 K  |
|       from small pool |     401    |     534    |     647 K  |     647 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     134    |    5193    |    5094    |
|       from large pool |      41    |      53    |    2965    |    2924    |
|       from small pool |      58    |      86    |    2228    |    2170    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      55    |      71    |  465876    |  465821    |
|       from large pool |      22    |      47    |  211604    |  211582    |
|       from small pool |      33    |      60    |  254272    |  254239    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:27:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 954.37 MiB already allocated; 168.75 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:00 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 202       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     954 MB |    1009 MB |    1413 GB |    1412 GB |
|       from large pool |     857 MB |     911 MB |    1259 GB |    1259 GB |
|       from small pool |      97 MB |     167 MB |     153 GB |     153 GB |
|---------------------------------------------------------------------------|
| Active memory         |     954 MB |    1009 MB |    1413 GB |    1412 GB |
|       from large pool |     857 MB |     911 MB |    1259 GB |    1259 GB |
|       from small pool |      97 MB |     167 MB |     153 GB |     153 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1038 MB |    1054 MB |   70902 MB |   69864 MB |
|       from large pool |     932 MB |     944 MB |   66324 MB |   65392 MB |
|       from small pool |     106 MB |     172 MB |    4578 MB |    4472 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   85640 KB |  345437 KB |    1454 GB |    1454 GB |
|       from large pool |   76599 KB |  327682 KB |    1290 GB |    1290 GB |
|       from small pool |    9041 KB |   31651 KB |     164 GB |     164 GB |
|---------------------------------------------------------------------------|
| Allocations           |     540    |     569    |    1027 K  |    1026 K  |
|       from large pool |     125    |     133    |     346 K  |     346 K  |
|       from small pool |     415    |     534    |     680 K  |     680 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     540    |     569    |    1027 K  |    1026 K  |
|       from large pool |     125    |     133    |     346 K  |     346 K  |
|       from small pool |     415    |     534    |     680 K  |     680 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     104    |     134    |    5352    |    5248    |
|       from large pool |      51    |      53    |    3063    |    3012    |
|       from small pool |      53    |      86    |    2289    |    2236    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      57    |      71    |  491334    |  491277    |
|       from large pool |      38    |      47    |  224303    |  224265    |
|       from small pool |      19    |      60    |  267031    |  267012    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 11.74 GiB total capacity; 972.33 MiB already allocated; 164.75 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:00 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 204       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     972 MB |    1009 MB |    1414 GB |    1413 GB |
|       from large pool |     875 MB |     911 MB |    1261 GB |    1260 GB |
|       from small pool |      97 MB |     167 MB |     153 GB |     153 GB |
|---------------------------------------------------------------------------|
| Active memory         |     972 MB |    1009 MB |    1414 GB |    1413 GB |
|       from large pool |     875 MB |     911 MB |    1261 GB |    1260 GB |
|       from small pool |      97 MB |     167 MB |     153 GB |     153 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1054 MB |   71704 MB |   70662 MB |
|       from large pool |     930 MB |     944 MB |   67092 MB |   66162 MB |
|       from small pool |     112 MB |     172 MB |    4612 MB |    4500 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   71344 KB |  345437 KB |    1456 GB |    1456 GB |
|       from large pool |   56092 KB |  327682 KB |    1291 GB |    1291 GB |
|       from small pool |   15252 KB |   31651 KB |     164 GB |     164 GB |
|---------------------------------------------------------------------------|
| Allocations           |     508    |     569    |    1028 K  |    1028 K  |
|       from large pool |     113    |     133    |     346 K  |     346 K  |
|       from small pool |     395    |     534    |     681 K  |     681 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     508    |     569    |    1028 K  |    1028 K  |
|       from large pool |     113    |     133    |     346 K  |     346 K  |
|       from small pool |     395    |     534    |     681 K  |     681 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |    5399    |    5303    |
|       from large pool |      40    |      53    |    3093    |    3053    |
|       from small pool |      56    |      86    |    2306    |    2250    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      71    |  491955    |  491908    |
|       from large pool |      18    |      47    |  224509    |  224491    |
|       from small pool |      29    |      60    |  267446    |  267417    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 11.74 GiB total capacity; 937.87 MiB already allocated; 228.75 MiB free; 978.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:00 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 209       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     937 MB |    1009 MB |    1454 GB |    1453 GB |
|       from large pool |     840 MB |     911 MB |    1296 GB |    1295 GB |
|       from small pool |      97 MB |     167 MB |     158 GB |     157 GB |
|---------------------------------------------------------------------------|
| Active memory         |     937 MB |    1009 MB |    1454 GB |    1453 GB |
|       from large pool |     840 MB |     911 MB |    1296 GB |    1295 GB |
|       from small pool |      97 MB |     167 MB |     158 GB |     157 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |     978 MB |    1054 MB |   74340 MB |   73362 MB |
|       from large pool |     872 MB |     944 MB |   69604 MB |   68732 MB |
|       from small pool |     106 MB |     172 MB |    4736 MB |    4630 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   41097 KB |  345437 KB |    1494 GB |    1494 GB |
|       from large pool |   31902 KB |  327682 KB |    1325 GB |    1325 GB |
|       from small pool |    9195 KB |   31651 KB |     169 GB |     169 GB |
|---------------------------------------------------------------------------|
| Allocations           |     444    |     569    |    1057 K  |    1057 K  |
|       from large pool |      62    |     133    |     355 K  |     355 K  |
|       from small pool |     382    |     534    |     702 K  |     701 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     444    |     569    |    1057 K  |    1057 K  |
|       from large pool |      62    |     133    |     355 K  |     355 K  |
|       from small pool |     382    |     534    |     702 K  |     701 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      72    |     134    |    5571    |    5499    |
|       from large pool |      19    |      53    |    3203    |    3184    |
|       from small pool |      53    |      86    |    2368    |    2315    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      71    |  505926    |  505893    |
|       from large pool |      13    |      47    |  230228    |  230215    |
|       from small pool |      20    |      60    |  275698    |  275678    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 11.74 GiB total capacity; 992.00 MiB already allocated; 139.94 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:02 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 211       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     991 MB |    1009 MB |    1531 GB |    1530 GB |
|       from large pool |     894 MB |     911 MB |    1365 GB |    1364 GB |
|       from small pool |      97 MB |     167 MB |     165 GB |     165 GB |
|---------------------------------------------------------------------------|
| Active memory         |     991 MB |    1009 MB |    1531 GB |    1530 GB |
|       from large pool |     894 MB |     911 MB |    1365 GB |    1364 GB |
|       from small pool |      97 MB |     167 MB |     165 GB |     165 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1038 MB |    1054 MB |   74466 MB |   73428 MB |
|       from large pool |     924 MB |     944 MB |   69656 MB |   68732 MB |
|       from small pool |     114 MB |     172 MB |    4810 MB |    4696 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   47108 KB |  345437 KB |    1592 GB |    1592 GB |
|       from large pool |   29807 KB |  327682 KB |    1414 GB |    1414 GB |
|       from small pool |   17301 KB |   31651 KB |     177 GB |     177 GB |
|---------------------------------------------------------------------------|
| Allocations           |     514    |     569    |    1115 K  |    1115 K  |
|       from large pool |     119    |     133    |     375 K  |     375 K  |
|       from small pool |     395    |     534    |     740 K  |     739 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     514    |     569    |    1115 K  |    1115 K  |
|       from large pool |     119    |     133    |     375 K  |     375 K  |
|       from small pool |     395    |     534    |     740 K  |     739 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      78    |     134    |    5610    |    5532    |
|       from large pool |      21    |      53    |    3205    |    3184    |
|       from small pool |      57    |      86    |    2405    |    2348    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      51    |      71    |  532885    |  532834    |
|       from large pool |      10    |      47    |  242119    |  242109    |
|       from small pool |      41    |      60    |  290766    |  290725    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 913.68 MiB already allocated; 147.44 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:02 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 217       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     913 MB |    1009 MB |    1577 GB |    1576 GB |
|       from large pool |     816 MB |     911 MB |    1407 GB |    1406 GB |
|       from small pool |      97 MB |     167 MB |     170 GB |     170 GB |
|---------------------------------------------------------------------------|
| Active memory         |     913 MB |    1009 MB |    1577 GB |    1576 GB |
|       from large pool |     816 MB |     911 MB |    1407 GB |    1406 GB |
|       from small pool |      97 MB |     167 MB |     170 GB |     170 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1030 MB |    1054 MB |   76784 MB |   75754 MB |
|       from large pool |     924 MB |     944 MB |   71828 MB |   70904 MB |
|       from small pool |     106 MB |     172 MB |    4956 MB |    4850 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  119108 KB |  345437 KB |    1640 GB |    1640 GB |
|       from large pool |  110048 KB |  327682 KB |    1458 GB |    1458 GB |
|       from small pool |    9060 KB |   31651 KB |     182 GB |     182 GB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     569    |    1149 K  |    1149 K  |
|       from large pool |     113    |     133    |     387 K  |     387 K  |
|       from small pool |     415    |     534    |     762 K  |     761 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     569    |    1149 K  |    1149 K  |
|       from large pool |     113    |     133    |     387 K  |     387 K  |
|       from small pool |     415    |     534    |     762 K  |     761 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |    5792    |    5694    |
|       from large pool |      45    |      53    |    3314    |    3269    |
|       from small pool |      53    |      86    |    2478    |    2425    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      54    |      71    |  549520    |  549466    |
|       from large pool |      35    |      47    |  250046    |  250011    |
|       from small pool |      19    |      60    |  299474    |  299455    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.74 GiB total capacity; 897.46 MiB already allocated; 134.88 MiB free; 992.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:04 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 222       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     897 MB |    1009 MB |    1700 GB |    1699 GB |
|       from large pool |     800 MB |     911 MB |    1516 GB |    1515 GB |
|       from small pool |      97 MB |     167 MB |     184 GB |     184 GB |
|---------------------------------------------------------------------------|
| Active memory         |     897 MB |    1009 MB |    1700 GB |    1699 GB |
|       from large pool |     800 MB |     911 MB |    1516 GB |    1515 GB |
|       from small pool |      97 MB |     167 MB |     184 GB |     184 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |     992 MB |    1054 MB |   78684 MB |   77692 MB |
|       from large pool |     888 MB |     944 MB |   73578 MB |   72690 MB |
|       from small pool |     104 MB |     172 MB |    5106 MB |    5002 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   96810 KB |  345437 KB |    1770 GB |    1770 GB |
|       from large pool |   89780 KB |  327682 KB |    1572 GB |    1572 GB |
|       from small pool |    7030 KB |   31651 KB |     197 GB |     197 GB |
|---------------------------------------------------------------------------|
| Allocations           |     506    |     569    |    1245 K  |    1244 K  |
|       from large pool |     111    |     133    |     420 K  |     420 K  |
|       from small pool |     395    |     534    |     824 K  |     824 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     506    |     569    |    1245 K  |    1244 K  |
|       from large pool |     111    |     133    |     420 K  |     420 K  |
|       from small pool |     395    |     534    |     824 K  |     824 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     134    |    5956    |    5861    |
|       from large pool |      43    |      53    |    3403    |    3360    |
|       from small pool |      52    |      86    |    2553    |    2501    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      53    |      71    |  595188    |  595135    |
|       from large pool |      26    |      47    |  271126    |  271100    |
|       from small pool |      27    |      60    |  324062    |  324035    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 11.74 GiB total capacity; 885.28 MiB already allocated; 110.88 MiB free; 1016.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:05 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 227       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     885 MB |    1009 MB |    1735 GB |    1735 GB |
|       from large pool |     788 MB |     911 MB |    1548 GB |    1547 GB |
|       from small pool |      97 MB |     167 MB |     187 GB |     187 GB |
|---------------------------------------------------------------------------|
| Active memory         |     885 MB |    1009 MB |    1735 GB |    1735 GB |
|       from large pool |     788 MB |     911 MB |    1548 GB |    1547 GB |
|       from small pool |      97 MB |     167 MB |     187 GB |     187 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1016 MB |    1054 MB |   79684 MB |   78668 MB |
|       from large pool |     900 MB |     944 MB |   74470 MB |   73570 MB |
|       from small pool |     116 MB |     172 MB |    5214 MB |    5098 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  133860 KB |  345437 KB |    1808 GB |    1808 GB |
|       from large pool |  114577 KB |  327682 KB |    1607 GB |    1607 GB |
|       from small pool |   19282 KB |   31651 KB |     201 GB |     201 GB |
|---------------------------------------------------------------------------|
| Allocations           |     536    |     569    |    1269 K  |    1269 K  |
|       from large pool |     121    |     133    |     429 K  |     428 K  |
|       from small pool |     415    |     534    |     840 K  |     840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     536    |     569    |    1269 K  |    1269 K  |
|       from large pool |     121    |     133    |     429 K  |     428 K  |
|       from small pool |     415    |     534    |     840 K  |     840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     134    |    6053    |    5951    |
|       from large pool |      44    |      53    |    3446    |    3402    |
|       from small pool |      58    |      86    |    2607    |    2549    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      60    |      71    |  607167    |  607107    |
|       from large pool |      27    |      47    |  276635    |  276608    |
|       from small pool |      33    |      60    |  330532    |  330499    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 963.24 MiB already allocated; 114.88 MiB free; 1020.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:05 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 230       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     963 MB |    1009 MB |    1781 GB |    1780 GB |
|       from large pool |     866 MB |     911 MB |    1588 GB |    1587 GB |
|       from small pool |      97 MB |     167 MB |     192 GB |     192 GB |
|---------------------------------------------------------------------------|
| Active memory         |     963 MB |    1009 MB |    1781 GB |    1780 GB |
|       from large pool |     866 MB |     911 MB |    1588 GB |    1587 GB |
|       from small pool |      97 MB |     167 MB |     192 GB |     192 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1020 MB |    1054 MB |   81218 MB |   80198 MB |
|       from large pool |     914 MB |     944 MB |   75944 MB |   75030 MB |
|       from small pool |     106 MB |     172 MB |    5274 MB |    5168 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   58126 KB |  345437 KB |    1857 GB |    1857 GB |
|       from large pool |   49003 KB |  327682 KB |    1650 GB |    1650 GB |
|       from small pool |    9123 KB |   31651 KB |     206 GB |     206 GB |
|---------------------------------------------------------------------------|
| Allocations           |     525    |     569    |    1304 K  |    1303 K  |
|       from large pool |     110    |     133    |     440 K  |     440 K  |
|       from small pool |     415    |     534    |     863 K  |     863 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     525    |     569    |    1304 K  |    1303 K  |
|       from large pool |     110    |     133    |     440 K  |     440 K  |
|       from small pool |     415    |     534    |     863 K  |     863 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     134    |    6145    |    6057    |
|       from large pool |      35    |      53    |    3508    |    3473    |
|       from small pool |      53    |      86    |    2637    |    2584    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      71    |  623557    |  623522    |
|       from large pool |      19    |      47    |  284036    |  284017    |
|       from small pool |      16    |      60    |  339521    |  339505    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 898.43 MiB already allocated; 122.88 MiB free; 1012.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:05 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 233       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     898 MB |    1009 MB |    1792 GB |    1791 GB |
|       from large pool |     801 MB |     911 MB |    1597 GB |    1597 GB |
|       from small pool |      97 MB |     167 MB |     194 GB |     194 GB |
|---------------------------------------------------------------------------|
| Active memory         |     898 MB |    1009 MB |    1792 GB |    1791 GB |
|       from large pool |     801 MB |     911 MB |    1597 GB |    1597 GB |
|       from small pool |      97 MB |     167 MB |     194 GB |     194 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1012 MB |    1054 MB |   82140 MB |   81128 MB |
|       from large pool |     896 MB |     944 MB |   76790 MB |   75894 MB |
|       from small pool |     116 MB |     172 MB |    5350 MB |    5234 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  116293 KB |  345437 KB |    1869 GB |    1869 GB |
|       from large pool |   96982 KB |  327682 KB |    1660 GB |    1660 GB |
|       from small pool |   19311 KB |   31651 KB |     208 GB |     208 GB |
|---------------------------------------------------------------------------|
| Allocations           |     553    |     569    |    1312 K  |    1311 K  |
|       from large pool |     126    |     133    |     442 K  |     442 K  |
|       from small pool |     427    |     534    |     869 K  |     868 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     553    |     569    |    1312 K  |    1311 K  |
|       from large pool |     126    |     133    |     442 K  |     442 K  |
|       from small pool |     427    |     534    |     869 K  |     868 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     134    |    6224    |    6121    |
|       from large pool |      45    |      53    |    3549    |    3504    |
|       from small pool |      58    |      86    |    2675    |    2617    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      64    |      71    |  627405    |  627341    |
|       from large pool |      31    |      47    |  285686    |  285655    |
|       from small pool |      33    |      60    |  341719    |  341686    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 948.48 MiB already allocated; 119.75 MiB free; 1014.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:06 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 236       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     948 MB |    1009 MB |    1823 GB |    1822 GB |
|       from large pool |     851 MB |     911 MB |    1625 GB |    1624 GB |
|       from small pool |      97 MB |     167 MB |     197 GB |     197 GB |
|---------------------------------------------------------------------------|
| Active memory         |     948 MB |    1009 MB |    1823 GB |    1822 GB |
|       from large pool |     851 MB |     911 MB |    1625 GB |    1624 GB |
|       from small pool |      97 MB |     167 MB |     197 GB |     197 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1014 MB |    1054 MB |   83578 MB |   82564 MB |
|       from large pool |     902 MB |     944 MB |   78172 MB |   77270 MB |
|       from small pool |     112 MB |     172 MB |    5406 MB |    5294 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   67091 KB |  345437 KB |    1900 GB |    1900 GB |
|       from large pool |   51864 KB |  327682 KB |    1688 GB |    1688 GB |
|       from small pool |   15227 KB |   31651 KB |     211 GB |     211 GB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     569    |    1332 K  |    1331 K  |
|       from large pool |     130    |     133    |     449 K  |     449 K  |
|       from small pool |     397    |     534    |     882 K  |     882 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     569    |    1332 K  |    1331 K  |
|       from large pool |     130    |     133    |     449 K  |     449 K  |
|       from small pool |     397    |     534    |     882 K  |     882 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     134    |    6324    |    6221    |
|       from large pool |      47    |      53    |    3621    |    3574    |
|       from small pool |      56    |      86    |    2703    |    2647    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      61    |      75    |  636975    |  636914    |
|       from large pool |      27    |      47    |  290100    |  290073    |
|       from small pool |      34    |      60    |  346875    |  346841    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 963.94 MiB already allocated; 122.88 MiB free; 1012.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:06 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 239       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     963 MB |    1009 MB |    1855 GB |    1854 GB |
|       from large pool |     863 MB |     911 MB |    1653 GB |    1653 GB |
|       from small pool |     100 MB |     167 MB |     201 GB |     201 GB |
|---------------------------------------------------------------------------|
| Active memory         |     963 MB |    1009 MB |    1855 GB |    1854 GB |
|       from large pool |     863 MB |     911 MB |    1653 GB |    1653 GB |
|       from small pool |     100 MB |     167 MB |     201 GB |     201 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1012 MB |    1054 MB |   84558 MB |   83546 MB |
|       from large pool |     906 MB |     944 MB |   79082 MB |   78176 MB |
|       from small pool |     106 MB |     172 MB |    5476 MB |    5370 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   49213 KB |  345437 KB |    1936 GB |    1936 GB |
|       from large pool |   43885 KB |  327682 KB |    1720 GB |    1720 GB |
|       from small pool |    5328 KB |   31651 KB |     216 GB |     216 GB |
|---------------------------------------------------------------------------|
| Allocations           |     543    |     569    |    1357 K  |    1356 K  |
|       from large pool |     122    |     133    |     457 K  |     457 K  |
|       from small pool |     421    |     534    |     899 K  |     898 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     543    |     569    |    1357 K  |    1356 K  |
|       from large pool |     122    |     133    |     457 K  |     457 K  |
|       from small pool |     421    |     534    |     899 K  |     898 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     134    |    6400    |    6305    |
|       from large pool |      42    |      53    |    3662    |    3620    |
|       from small pool |      53    |      86    |    2738    |    2685    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      75    |  648947    |  648910    |
|       from large pool |      20    |      47    |  295322    |  295302    |
|       from small pool |      17    |      60    |  353625    |  353608    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 11.74 GiB total capacity; 922.31 MiB already allocated; 142.88 MiB free; 992.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 242       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     922 MB |    1009 MB |    1873 GB |    1872 GB |
|       from large pool |     821 MB |     911 MB |    1668 GB |    1667 GB |
|       from small pool |     100 MB |     167 MB |     204 GB |     204 GB |
|---------------------------------------------------------------------------|
| Active memory         |     922 MB |    1009 MB |    1873 GB |    1872 GB |
|       from large pool |     821 MB |     911 MB |    1668 GB |    1667 GB |
|       from small pool |     100 MB |     167 MB |     204 GB |     204 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |     992 MB |    1054 MB |   85616 MB |   84624 MB |
|       from large pool |     886 MB |     944 MB |   80062 MB |   79176 MB |
|       from small pool |     106 MB |     172 MB |    5554 MB |    5448 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   71361 KB |  345437 KB |    1955 GB |    1955 GB |
|       from large pool |   65936 KB |  327682 KB |    1736 GB |    1736 GB |
|       from small pool |    5425 KB |   31651 KB |     219 GB |     219 GB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     569    |    1372 K  |    1371 K  |
|       from large pool |     109    |     133    |     462 K  |     462 K  |
|       from small pool |     419    |     534    |     910 K  |     909 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     569    |    1372 K  |    1371 K  |
|       from large pool |     109    |     133    |     462 K  |     462 K  |
|       from small pool |     419    |     534    |     910 K  |     909 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |     134    |    6479    |    6390    |
|       from large pool |      36    |      53    |    3702    |    3666    |
|       from small pool |      53    |      86    |    2777    |    2724    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      75    |  656289    |  656247    |
|       from large pool |      23    |      47    |  298059    |  298036    |
|       from small pool |      19    |      60    |  358230    |  358211    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 933.45 MiB already allocated; 110.88 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 244       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     933 MB |    1009 MB |    1875 GB |    1874 GB |
|       from large pool |     836 MB |     911 MB |    1670 GB |    1669 GB |
|       from small pool |      97 MB |     167 MB |     204 GB |     204 GB |
|---------------------------------------------------------------------------|
| Active memory         |     933 MB |    1009 MB |    1875 GB |    1874 GB |
|       from large pool |     836 MB |     911 MB |    1670 GB |    1669 GB |
|       from small pool |      97 MB |     167 MB |     204 GB |     204 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1024 MB |    1054 MB |   85690 MB |   84666 MB |
|       from large pool |     918 MB |     944 MB |   80094 MB |   79176 MB |
|       from small pool |     106 MB |     172 MB |    5596 MB |    5490 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   92720 KB |  345437 KB |    1957 GB |    1957 GB |
|       from large pool |   83680 KB |  327682 KB |    1737 GB |    1737 GB |
|       from small pool |    9040 KB |   31651 KB |     219 GB |     219 GB |
|---------------------------------------------------------------------------|
| Allocations           |     524    |     569    |    1373 K  |    1373 K  |
|       from large pool |     127    |     133    |     462 K  |     462 K  |
|       from small pool |     397    |     534    |     910 K  |     910 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     524    |     569    |    1373 K  |    1373 K  |
|       from large pool |     127    |     133    |     462 K  |     462 K  |
|       from small pool |     397    |     534    |     910 K  |     910 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     134    |    6502    |    6411    |
|       from large pool |      38    |      53    |    3704    |    3666    |
|       from small pool |      53    |      86    |    2798    |    2745    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      51    |      75    |  656931    |  656880    |
|       from large pool |      26    |      47    |  298340    |  298314    |
|       from small pool |      25    |      60    |  358591    |  358566    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.74 GiB total capacity; 961.98 MiB already allocated; 135.00 MiB free; 1010.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 56           |        cudaMalloc retries: 247       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     937 MB |    1009 MB |    1908 GB |    1907 GB |
|       from large pool |     840 MB |     911 MB |    1701 GB |    1700 GB |
|       from small pool |      97 MB |     167 MB |     207 GB |     207 GB |
|---------------------------------------------------------------------------|
| Active memory         |     937 MB |    1009 MB |    1908 GB |    1907 GB |
|       from large pool |     840 MB |     911 MB |    1701 GB |    1700 GB |
|       from small pool |      97 MB |     167 MB |     207 GB |     207 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1010 MB |    1054 MB |   87102 MB |   86092 MB |
|       from large pool |     906 MB |     944 MB |   81448 MB |   80542 MB |
|       from small pool |     104 MB |     172 MB |    5654 MB |    5550 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   49175 KB |  345437 KB |    1991 GB |    1991 GB |
|       from large pool |   42012 KB |  327682 KB |    1769 GB |    1769 GB |
|       from small pool |    7163 KB |   31651 KB |     222 GB |     222 GB |
|---------------------------------------------------------------------------|
| Allocations           |     458    |     569    |    1396 K  |    1395 K  |
|       from large pool |      66    |     133    |     471 K  |     471 K  |
|       from small pool |     392    |     534    |     924 K  |     924 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     458    |     569    |    1396 K  |    1395 K  |
|       from large pool |      66    |     133    |     471 K  |     471 K  |
|       from small pool |     392    |     534    |     924 K  |     924 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      75    |     134    |    6578    |    6503    |
|       from large pool |      23    |      53    |    3751    |    3728    |
|       from small pool |      52    |      86    |    2827    |    2775    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      75    |  668284    |  668252    |
|       from large pool |      10    |      47    |  303941    |  303931    |
|       from small pool |      22    |      60    |  364343    |  364321    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 983.75 MiB already allocated; 129.00 MiB free; 1016.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 57           |        cudaMalloc retries: 250       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     983 MB |    1009 MB |    1931 GB |    1930 GB |
|       from large pool |     880 MB |     911 MB |    1722 GB |    1721 GB |
|       from small pool |     103 MB |     167 MB |     209 GB |     209 GB |
|---------------------------------------------------------------------------|
| Active memory         |     983 MB |    1009 MB |    1931 GB |    1930 GB |
|       from large pool |     880 MB |     911 MB |    1722 GB |    1721 GB |
|       from small pool |     103 MB |     167 MB |     209 GB |     209 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1016 MB |    1054 MB |   88132 MB |   87116 MB |
|       from large pool |     910 MB |     944 MB |   82410 MB |   81500 MB |
|       from small pool |     106 MB |     172 MB |    5722 MB |    5616 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   33026 KB |  345437 KB |    2014 GB |    2014 GB |
|       from large pool |   30038 KB |  327682 KB |    1790 GB |    1790 GB |
|       from small pool |    2987 KB |   31651 KB |     224 GB |     224 GB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     569    |    1411 K  |    1410 K  |
|       from large pool |     106    |     133    |     476 K  |     476 K  |
|       from small pool |     421    |     534    |     934 K  |     934 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     569    |    1411 K  |    1410 K  |
|       from large pool |     106    |     133    |     476 K  |     476 K  |
|       from small pool |     421    |     534    |     934 K  |     934 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |     134    |    6654    |    6569    |
|       from large pool |      32    |      53    |    3793    |    3761    |
|       from small pool |      53    |      86    |    2861    |    2808    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      75    |  675928    |  675895    |
|       from large pool |      18    |      47    |  307324    |  307306    |
|       from small pool |      15    |      60    |  368604    |  368589    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.74 GiB total capacity; 901.62 MiB already allocated; 149.00 MiB free; 992.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:08 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 58           |        cudaMalloc retries: 253       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     901 MB |    1009 MB |    1948 GB |    1947 GB |
|       from large pool |     804 MB |     911 MB |    1736 GB |    1736 GB |
|       from small pool |      97 MB |     167 MB |     211 GB |     211 GB |
|---------------------------------------------------------------------------|
| Active memory         |     901 MB |    1009 MB |    1948 GB |    1947 GB |
|       from large pool |     804 MB |     911 MB |    1736 GB |    1736 GB |
|       from small pool |      97 MB |     167 MB |     211 GB |     211 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |     992 MB |    1054 MB |   89472 MB |   88480 MB |
|       from large pool |     888 MB |     944 MB |   83666 MB |   82778 MB |
|       from small pool |     104 MB |     172 MB |    5806 MB |    5702 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   92553 KB |  345437 KB |    2034 GB |    2033 GB |
|       from large pool |   85530 KB |  327682 KB |    1807 GB |    1807 GB |
|       from small pool |    7022 KB |   31651 KB |     226 GB |     226 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     569    |    1424 K  |    1423 K  |
|       from large pool |     111    |     133    |     480 K  |     480 K  |
|       from small pool |     405    |     534    |     943 K  |     943 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     569    |    1424 K  |    1423 K  |
|       from large pool |     111    |     133    |     480 K  |     480 K  |
|       from small pool |     405    |     534    |     943 K  |     943 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     134    |    6756    |    6662    |
|       from large pool |      42    |      53    |    3853    |    3811    |
|       from small pool |      52    |      86    |    2903    |    2851    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      75    |  681788    |  681747    |
|       from large pool |      23    |      47    |  310042    |  310019    |
|       from small pool |      18    |      60    |  371746    |  371728    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 11.74 GiB total capacity; 917.73 MiB already allocated; 127.00 MiB free; 1014.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:08 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 59           |        cudaMalloc retries: 256       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     917 MB |    1009 MB |    1951 GB |    1950 GB |
|       from large pool |     820 MB |     911 MB |    1739 GB |    1738 GB |
|       from small pool |      97 MB |     167 MB |     212 GB |     212 GB |
|---------------------------------------------------------------------------|
| Active memory         |     917 MB |    1009 MB |    1951 GB |    1950 GB |
|       from large pool |     820 MB |     911 MB |    1739 GB |    1738 GB |
|       from small pool |      97 MB |     167 MB |     212 GB |     212 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1014 MB |    1054 MB |   90304 MB |   89290 MB |
|       from large pool |     908 MB |     944 MB |   84434 MB |   83526 MB |
|       from small pool |     106 MB |     172 MB |    5870 MB |    5764 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   98585 KB |  345437 KB |    2037 GB |    2037 GB |
|       from large pool |   89485 KB |  327682 KB |    1809 GB |    1809 GB |
|       from small pool |    9100 KB |   31651 KB |     227 GB |     227 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     569    |    1427 K  |    1427 K  |
|       from large pool |     111    |     133    |     481 K  |     481 K  |
|       from small pool |     405    |     534    |     946 K  |     945 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     569    |    1427 K  |    1427 K  |
|       from large pool |     111    |     133    |     481 K  |     481 K  |
|       from small pool |     405    |     534    |     946 K  |     945 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     134    |    6819    |    6727    |
|       from large pool |      39    |      53    |    3884    |    3845    |
|       from small pool |      53    |      86    |    2935    |    2882    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      45    |      75    |  683332    |  683287    |
|       from large pool |      28    |      47    |  310404    |  310376    |
|       from small pool |      17    |      60    |  372928    |  372911    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 912.80 MiB already allocated; 133.00 MiB free; 1008.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:08 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 60           |        cudaMalloc retries: 259       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     912 MB |    1009 MB |    1963 GB |    1962 GB |
|       from large pool |     815 MB |     911 MB |    1749 GB |    1748 GB |
|       from small pool |      97 MB |     167 MB |     213 GB |     213 GB |
|---------------------------------------------------------------------------|
| Active memory         |     912 MB |    1009 MB |    1963 GB |    1962 GB |
|       from large pool |     815 MB |     911 MB |    1749 GB |    1748 GB |
|       from small pool |      97 MB |     167 MB |     213 GB |     213 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1008 MB |    1054 MB |   91382 MB |   90374 MB |
|       from large pool |     902 MB |     944 MB |   85456 MB |   84554 MB |
|       from small pool |     106 MB |     172 MB |    5926 MB |    5820 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   97484 KB |  345437 KB |    2050 GB |    2050 GB |
|       from large pool |   88442 KB |  327682 KB |    1821 GB |    1821 GB |
|       from small pool |    9042 KB |   31651 KB |     228 GB |     228 GB |
|---------------------------------------------------------------------------|
| Allocations           |     538    |     569    |    1435 K  |    1435 K  |
|       from large pool |     113    |     133    |     484 K  |     484 K  |
|       from small pool |     425    |     534    |     951 K  |     951 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     538    |     569    |    1435 K  |    1435 K  |
|       from large pool |     113    |     133    |     484 K  |     484 K  |
|       from small pool |     425    |     534    |     951 K  |     951 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     134    |    6894    |    6799    |
|       from large pool |      42    |      53    |    3931    |    3889    |
|       from small pool |      53    |      86    |    2963    |    2910    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      45    |      75    |  687436    |  687391    |
|       from large pool |      30    |      47    |  312366    |  312336    |
|       from small pool |      15    |      60    |  375070    |  375055    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 890.24 MiB already allocated; 123.00 MiB free; 1022.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:08 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 61           |        cudaMalloc retries: 264       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     884 MB |    1009 MB |    1998 GB |    1997 GB |
|       from large pool |     787 MB |     911 MB |    1779 GB |    1778 GB |
|       from small pool |      97 MB |     167 MB |     218 GB |     218 GB |
|---------------------------------------------------------------------------|
| Active memory         |     884 MB |    1009 MB |    1998 GB |    1997 GB |
|       from large pool |     787 MB |     911 MB |    1779 GB |    1778 GB |
|       from small pool |      97 MB |     167 MB |     218 GB |     218 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1022 MB |    1054 MB |   93076 MB |   92054 MB |
|       from large pool |     914 MB |     944 MB |   87024 MB |   86110 MB |
|       from small pool |     108 MB |     172 MB |    6052 MB |    5944 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  119820 KB |  345437 KB |    2084 GB |    2084 GB |
|       from large pool |  108700 KB |  327682 KB |    1850 GB |    1850 GB |
|       from small pool |   11119 KB |   31651 KB |     234 GB |     234 GB |
|---------------------------------------------------------------------------|
| Allocations           |     524    |     569    |    1465 K  |    1464 K  |
|       from large pool |     127    |     133    |     492 K  |     492 K  |
|       from small pool |     397    |     534    |     972 K  |     972 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     524    |     569    |    1465 K  |    1464 K  |
|       from large pool |     127    |     133    |     492 K  |     492 K  |
|       from small pool |     397    |     534    |     972 K  |     972 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     134    |    7040    |    6938    |
|       from large pool |      48    |      53    |    4014    |    3966    |
|       from small pool |      54    |      86    |    3026    |    2972    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      59    |      75    |  701137    |  701078    |
|       from large pool |      34    |      47    |  317741    |  317707    |
|       from small pool |      25    |      60    |  383396    |  383371    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 945.18 MiB already allocated; 131.00 MiB free; 1014.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 62           |        cudaMalloc retries: 267       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     945 MB |    1009 MB |    2007 GB |    2006 GB |
|       from large pool |     847 MB |     911 MB |    1787 GB |    1786 GB |
|       from small pool |      97 MB |     167 MB |     219 GB |     219 GB |
|---------------------------------------------------------------------------|
| Active memory         |     945 MB |    1009 MB |    2007 GB |    2006 GB |
|       from large pool |     847 MB |     911 MB |    1787 GB |    1786 GB |
|       from small pool |      97 MB |     167 MB |     219 GB |     219 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1014 MB |    1054 MB |   93870 MB |   92856 MB |
|       from large pool |     910 MB |     944 MB |   87764 MB |   86854 MB |
|       from small pool |     104 MB |     172 MB |    6106 MB |    6002 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   70467 KB |  345437 KB |    2094 GB |    2094 GB |
|       from large pool |   63514 KB |  327682 KB |    1858 GB |    1858 GB |
|       from small pool |    6953 KB |   31651 KB |     235 GB |     235 GB |
|---------------------------------------------------------------------------|
| Allocations           |     557    |     569    |    1473 K  |    1472 K  |
|       from large pool |     130    |     133    |     495 K  |     495 K  |
|       from small pool |     427    |     534    |     977 K  |     977 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     557    |     569    |    1473 K  |    1472 K  |
|       from large pool |     130    |     133    |     495 K  |     495 K  |
|       from small pool |     427    |     534    |     977 K  |     977 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     101    |     134    |    7106    |    7005    |
|       from large pool |      49    |      53    |    4053    |    4004    |
|       from small pool |      52    |      86    |    3053    |    3001    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      75    |  705148    |  705112    |
|       from large pool |      21    |      47    |  319552    |  319531    |
|       from small pool |      15    |      60    |  385596    |  385581    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 961.14 MiB already allocated; 129.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 63           |        cudaMalloc retries: 272       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     961 MB |    1009 MB |    2054 GB |    2053 GB |
|       from large pool |     864 MB |     911 MB |    1829 GB |    1829 GB |
|       from small pool |      97 MB |     167 MB |     224 GB |     224 GB |
|---------------------------------------------------------------------------|
| Active memory         |     961 MB |    1009 MB |    2054 GB |    2053 GB |
|       from large pool |     864 MB |     911 MB |    1829 GB |    1829 GB |
|       from small pool |      97 MB |     167 MB |     224 GB |     224 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1044 MB |    1054 MB |   95688 MB |   94644 MB |
|       from large pool |     928 MB |     944 MB |   89454 MB |   88526 MB |
|       from small pool |     116 MB |     172 MB |    6234 MB |    6118 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   84851 KB |  345437 KB |    2145 GB |    2145 GB |
|       from large pool |   65502 KB |  327682 KB |    1904 GB |    1904 GB |
|       from small pool |   19348 KB |   31651 KB |     241 GB |     241 GB |
|---------------------------------------------------------------------------|
| Allocations           |     512    |     569    |    1507 K  |    1507 K  |
|       from large pool |     101    |     133    |     508 K  |     508 K  |
|       from small pool |     411    |     534    |     999 K  |     998 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     512    |     569    |    1507 K  |    1507 K  |
|       from large pool |     101    |     133    |     508 K  |     508 K  |
|       from small pool |     411    |     534    |     999 K  |     998 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |    7248    |    7152    |
|       from large pool |      38    |      53    |    4131    |    4093    |
|       from small pool |      58    |      86    |    3117    |    3059    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      54    |      75    |  721943    |  721889    |
|       from large pool |      21    |      47    |  327916    |  327895    |
|       from small pool |      33    |      60    |  394027    |  393994    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 938.67 MiB already allocated; 157.69 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 64           |        cudaMalloc retries: 276       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     938 MB |    1009 MB |    2107 GB |    2106 GB |
|       from large pool |     841 MB |     911 MB |    1877 GB |    1876 GB |
|       from small pool |      97 MB |     167 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| Active memory         |     938 MB |    1009 MB |    2107 GB |    2106 GB |
|       from large pool |     841 MB |     911 MB |    1877 GB |    1876 GB |
|       from small pool |      97 MB |     167 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1032 MB |    1054 MB |   97026 MB |   95994 MB |
|       from large pool |     924 MB |     944 MB |   90676 MB |   89752 MB |
|       from small pool |     108 MB |     172 MB |    6350 MB |    6242 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   95573 KB |  345437 KB |    2198 GB |    2198 GB |
|       from large pool |   84512 KB |  327682 KB |    1951 GB |    1951 GB |
|       from small pool |   11061 KB |   31651 KB |     246 GB |     246 GB |
|---------------------------------------------------------------------------|
| Allocations           |     557    |     569    |    1543 K  |    1543 K  |
|       from large pool |     130    |     133    |     520 K  |     520 K  |
|       from small pool |     427    |     534    |    1023 K  |    1023 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     557    |     569    |    1543 K  |    1543 K  |
|       from large pool |     130    |     133    |     520 K  |     520 K  |
|       from small pool |     427    |     534    |    1023 K  |    1023 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     134    |    7372    |    7265    |
|       from large pool |      53    |      53    |    4197    |    4144    |
|       from small pool |      54    |      86    |    3175    |    3121    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      52    |      75    |  739753    |  739701    |
|       from large pool |      32    |      47    |  336001    |  335969    |
|       from small pool |      20    |      60    |  403752    |  403732    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 945.08 MiB already allocated; 170.44 MiB free; 1020.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 65           |        cudaMalloc retries: 278       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     945 MB |    1009 MB |    2109 GB |    2108 GB |
|       from large pool |     847 MB |     911 MB |    1879 GB |    1878 GB |
|       from small pool |      97 MB |     167 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| Active memory         |     945 MB |    1009 MB |    2109 GB |    2108 GB |
|       from large pool |     847 MB |     911 MB |    1879 GB |    1878 GB |
|       from small pool |      97 MB |     167 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1020 MB |    1054 MB |   97778 MB |   96758 MB |
|       from large pool |     916 MB |     944 MB |   91406 MB |   90490 MB |
|       from small pool |     104 MB |     172 MB |    6372 MB |    6268 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   76722 KB |  345437 KB |    2200 GB |    2200 GB |
|       from large pool |   69739 KB |  327682 KB |    1953 GB |    1953 GB |
|       from small pool |    6983 KB |   31651 KB |     246 GB |     246 GB |
|---------------------------------------------------------------------------|
| Allocations           |     508    |     569    |    1545 K  |    1544 K  |
|       from large pool |     113    |     133    |     521 K  |     521 K  |
|       from small pool |     395    |     534    |    1024 K  |    1023 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     508    |     569    |    1545 K  |    1544 K  |
|       from large pool |     113    |     133    |     521 K  |     521 K  |
|       from small pool |     395    |     534    |    1024 K  |    1023 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |    7416    |    7319    |
|       from large pool |      45    |      53    |    4230    |    4185    |
|       from small pool |      52    |      86    |    3186    |    3134    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      45    |      75    |  740491    |  740446    |
|       from large pool |      25    |      47    |  336509    |  336484    |
|       from small pool |      20    |      60    |  403982    |  403962    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 937.94 MiB already allocated; 157.69 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 66           |        cudaMalloc retries: 280       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     937 MB |    1009 MB |    2110 GB |    2110 GB |
|       from large pool |     840 MB |     911 MB |    1880 GB |    1879 GB |
|       from small pool |      97 MB |     167 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| Active memory         |     937 MB |    1009 MB |    2110 GB |    2110 GB |
|       from large pool |     840 MB |     911 MB |    1880 GB |    1879 GB |
|       from small pool |      97 MB |     167 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1032 MB |    1054 MB |   97848 MB |   96816 MB |
|       from large pool |     924 MB |     944 MB |   91426 MB |   90502 MB |
|       from small pool |     108 MB |     172 MB |    6422 MB |    6314 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   96319 KB |  345437 KB |    2201 GB |    2201 GB |
|       from large pool |   85168 KB |  327682 KB |    1954 GB |    1954 GB |
|       from small pool |   11151 KB |   31651 KB |     247 GB |     247 GB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     569    |    1546 K  |    1546 K  |
|       from large pool |     120    |     133    |     521 K  |     521 K  |
|       from small pool |     395    |     534    |    1025 K  |    1024 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     569    |    1546 K  |    1546 K  |
|       from large pool |     120    |     133    |     521 K  |     521 K  |
|       from small pool |     395    |     534    |    1025 K  |    1024 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     134    |    7442    |    7343    |
|       from large pool |      45    |      53    |    4231    |    4186    |
|       from small pool |      54    |      86    |    3211    |    3157    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      46    |      75    |  741042    |  740996    |
|       from large pool |      24    |      47    |  336594    |  336570    |
|       from small pool |      22    |      60    |  404448    |  404426    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 11.74 GiB total capacity; 943.34 MiB already allocated; 171.69 MiB free; 1018.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:11 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 67           |        cudaMalloc retries: 286       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     943 MB |    1009 MB |    2142 GB |    2141 GB |
|       from large pool |     846 MB |     911 MB |    1908 GB |    1907 GB |
|       from small pool |      97 MB |     167 MB |     233 GB |     233 GB |
|---------------------------------------------------------------------------|
| Active memory         |     943 MB |    1009 MB |    2142 GB |    2141 GB |
|       from large pool |     846 MB |     911 MB |    1908 GB |    1907 GB |
|       from small pool |      97 MB |     167 MB |     233 GB |     233 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1018 MB |    1054 MB |  100380 MB |   99362 MB |
|       from large pool |     910 MB |     944 MB |   93812 MB |   92902 MB |
|       from small pool |     108 MB |     172 MB |    6568 MB |    6460 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   76448 KB |  345437 KB |    2232 GB |    2232 GB |
|       from large pool |   65288 KB |  327682 KB |    1982 GB |    1982 GB |
|       from small pool |   11160 KB |   31651 KB |     250 GB |     250 GB |
|---------------------------------------------------------------------------|
| Allocations           |     517    |     569    |    1569 K  |    1568 K  |
|       from large pool |     112    |     133    |     529 K  |     529 K  |
|       from small pool |     405    |     534    |    1039 K  |    1039 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     517    |     569    |    1569 K  |    1568 K  |
|       from large pool |     112    |     133    |     529 K  |     529 K  |
|       from small pool |     405    |     534    |    1039 K  |    1039 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      93    |     134    |    7628    |    7535    |
|       from large pool |      39    |      53    |    4344    |    4305    |
|       from small pool |      54    |      86    |    3284    |    3230    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      75    |     751 K  |     751 K  |
|       from large pool |      22    |      47    |     341 K  |     341 K  |
|       from small pool |      18    |      60    |     410 K  |     410 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 983.87 MiB already allocated; 149.94 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:11 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 68           |        cudaMalloc retries: 291       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     983 MB |    1009 MB |    2199 GB |    2198 GB |
|       from large pool |     886 MB |     911 MB |    1959 GB |    1958 GB |
|       from small pool |      97 MB |     167 MB |     239 GB |     239 GB |
|---------------------------------------------------------------------------|
| Active memory         |     983 MB |    1009 MB |    2199 GB |    2198 GB |
|       from large pool |     886 MB |     911 MB |    1959 GB |    1958 GB |
|       from small pool |      97 MB |     167 MB |     239 GB |     239 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1054 MB |  101802 MB |  100756 MB |
|       from large pool |     926 MB |     944 MB |   95156 MB |   94230 MB |
|       from small pool |     120 MB |     172 MB |    6646 MB |    6526 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   63624 KB |  345437 KB |    2296 GB |    2296 GB |
|       from large pool |   40224 KB |  327682 KB |    2039 GB |    2039 GB |
|       from small pool |   23399 KB |   31651 KB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| Allocations           |     530    |     569    |    1612 K  |    1611 K  |
|       from large pool |     115    |     133    |     544 K  |     544 K  |
|       from small pool |     415    |     534    |    1067 K  |    1067 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     530    |     569    |    1612 K  |    1611 K  |
|       from large pool |     115    |     133    |     544 K  |     544 K  |
|       from small pool |     415    |     534    |    1067 K  |    1067 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     101    |     134    |    7727    |    7626    |
|       from large pool |      41    |      53    |    4404    |    4363    |
|       from small pool |      60    |      86    |    3323    |    3263    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      51    |      75    |     772 K  |     772 K  |
|       from large pool |      18    |      47    |     351 K  |     351 K  |
|       from small pool |      33    |      60    |     421 K  |     421 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 994.21 MiB already allocated; 137.94 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:12 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 69           |        cudaMalloc retries: 293       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     994 MB |    1009 MB |    2218 GB |    2217 GB |
|       from large pool |     893 MB |     911 MB |    1977 GB |    1976 GB |
|       from small pool |     100 MB |     167 MB |     241 GB |     240 GB |
|---------------------------------------------------------------------------|
| Active memory         |     994 MB |    1009 MB |    2218 GB |    2217 GB |
|       from large pool |     893 MB |     911 MB |    1977 GB |    1976 GB |
|       from small pool |     100 MB |     167 MB |     241 GB |     240 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1058 MB |    1058 MB |  101886 MB |  100828 MB |
|       from large pool |     948 MB |     948 MB |   95202 MB |   94254 MB |
|       from small pool |     110 MB |     172 MB |    6684 MB |    6574 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   65320 KB |  345437 KB |    2317 GB |    2317 GB |
|       from large pool |   55929 KB |  327682 KB |    2058 GB |    2058 GB |
|       from small pool |    9391 KB |   33723 KB |     258 GB |     258 GB |
|---------------------------------------------------------------------------|
| Allocations           |     534    |     569    |    1625 K  |    1625 K  |
|       from large pool |     123    |     133    |     549 K  |     549 K  |
|       from small pool |     411    |     534    |    1075 K  |    1075 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     534    |     569    |    1625 K  |    1625 K  |
|       from large pool |     123    |     133    |     549 K  |     549 K  |
|       from small pool |     411    |     534    |    1075 K  |    1075 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |    7748    |    7651    |
|       from large pool |      42    |      53    |    4406    |    4364    |
|       from small pool |      55    |      86    |    3342    |    3287    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      75    |     779 K  |     779 K  |
|       from large pool |      23    |      47    |     354 K  |     354 K  |
|       from small pool |      26    |      60    |     424 K  |     424 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 1003.86 MiB already allocated; 139.94 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:12 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 70           |        cudaMalloc retries: 297       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1003 MB |    1009 MB |    2232 GB |    2231 GB |
|       from large pool |     902 MB |     911 MB |    1988 GB |    1988 GB |
|       from small pool |     101 MB |     167 MB |     243 GB |     243 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1003 MB |    1009 MB |    2232 GB |    2231 GB |
|       from large pool |     902 MB |     911 MB |    1988 GB |    1988 GB |
|       from small pool |     101 MB |     167 MB |     243 GB |     243 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1056 MB |    1058 MB |  103430 MB |  102374 MB |
|       from large pool |     946 MB |     948 MB |   96646 MB |   95700 MB |
|       from small pool |     110 MB |     172 MB |    6784 MB |    6674 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   53394 KB |  345437 KB |    2330 GB |    2330 GB |
|       from large pool |   44252 KB |  327682 KB |    2069 GB |    2069 GB |
|       from small pool |    9141 KB |   33723 KB |     260 GB |     260 GB |
|---------------------------------------------------------------------------|
| Allocations           |     537    |     569    |    1638 K  |    1637 K  |
|       from large pool |     126    |     133    |     553 K  |     553 K  |
|       from small pool |     411    |     534    |    1085 K  |    1084 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     537    |     569    |    1638 K  |    1637 K  |
|       from large pool |     126    |     133    |     553 K  |     553 K  |
|       from small pool |     411    |     534    |    1085 K  |    1084 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     134    |    7868    |    7768    |
|       from large pool |      45    |      53    |    4476    |    4431    |
|       from small pool |      55    |      86    |    3392    |    3337    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      75    |     785 K  |     785 K  |
|       from large pool |      22    |      47    |     357 K  |     357 K  |
|       from small pool |      21    |      60    |     428 K  |     428 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 948.29 MiB already allocated; 153.94 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:12 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 71           |        cudaMalloc retries: 300       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     948 MB |    1009 MB |    2238 GB |    2237 GB |
|       from large pool |     851 MB |     911 MB |    1993 GB |    1992 GB |
|       from small pool |      97 MB |     167 MB |     244 GB |     244 GB |
|---------------------------------------------------------------------------|
| Active memory         |     948 MB |    1009 MB |    2238 GB |    2237 GB |
|       from large pool |     851 MB |     911 MB |    1993 GB |    1992 GB |
|       from small pool |      97 MB |     167 MB |     244 GB |     244 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1058 MB |  104988 MB |  103946 MB |
|       from large pool |     930 MB |     948 MB |   98136 MB |   97206 MB |
|       from small pool |     112 MB |     172 MB |    6852 MB |    6740 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   95961 KB |  345437 KB |    2335 GB |    2335 GB |
|       from large pool |   80758 KB |  327682 KB |    2073 GB |    2073 GB |
|       from small pool |   15203 KB |   33723 KB |     262 GB |     262 GB |
|---------------------------------------------------------------------------|
| Allocations           |     534    |     569    |    1644 K  |    1643 K  |
|       from large pool |     119    |     133    |     554 K  |     554 K  |
|       from small pool |     415    |     534    |    1090 K  |    1089 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     534    |     569    |    1644 K  |    1643 K  |
|       from large pool |     119    |     133    |     554 K  |     554 K  |
|       from small pool |     415    |     534    |    1090 K  |    1089 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     134    |    7974    |    7874    |
|       from large pool |      44    |      53    |    4548    |    4504    |
|       from small pool |      56    |      86    |    3426    |    3370    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      62    |      75    |     788 K  |     788 K  |
|       from large pool |      32    |      47    |     357 K  |     357 K  |
|       from small pool |      30    |      60    |     430 K  |     430 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 981.02 MiB already allocated; 149.94 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 72           |        cudaMalloc retries: 303       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     981 MB |    1009 MB |    2273 GB |    2272 GB |
|       from large pool |     883 MB |     911 MB |    2024 GB |    2023 GB |
|       from small pool |      97 MB |     167 MB |     249 GB |     249 GB |
|---------------------------------------------------------------------------|
| Active memory         |     981 MB |    1009 MB |    2273 GB |    2272 GB |
|       from large pool |     883 MB |     911 MB |    2024 GB |    2023 GB |
|       from small pool |      97 MB |     167 MB |     249 GB |     249 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1058 MB |  106250 MB |  105204 MB |
|       from large pool |     934 MB |     948 MB |   99330 MB |   98396 MB |
|       from small pool |     112 MB |     172 MB |    6920 MB |    6808 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   66534 KB |  345437 KB |    2373 GB |    2373 GB |
|       from large pool |   51294 KB |  327682 KB |    2106 GB |    2106 GB |
|       from small pool |   15240 KB |   33723 KB |     267 GB |     267 GB |
|---------------------------------------------------------------------------|
| Allocations           |     546    |     569    |    1670 K  |    1670 K  |
|       from large pool |     121    |     133    |     562 K  |     562 K  |
|       from small pool |     425    |     534    |    1108 K  |    1107 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     546    |     569    |    1670 K  |    1670 K  |
|       from large pool |     121    |     133    |     562 K  |     562 K  |
|       from small pool |     425    |     534    |    1108 K  |    1107 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |    8060    |    7963    |
|       from large pool |      41    |      53    |    4600    |    4559    |
|       from small pool |      56    |      86    |    3460    |    3404    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      75    |     800 K  |     800 K  |
|       from large pool |      22    |      47    |     363 K  |     363 K  |
|       from small pool |      21    |      60    |     436 K  |     436 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.74 GiB total capacity; 975.14 MiB already allocated; 155.94 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 73           |        cudaMalloc retries: 306       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     975 MB |    1009 MB |    2294 GB |    2293 GB |
|       from large pool |     878 MB |     911 MB |    2042 GB |    2041 GB |
|       from small pool |      97 MB |     167 MB |     252 GB |     252 GB |
|---------------------------------------------------------------------------|
| Active memory         |     975 MB |    1009 MB |    2294 GB |    2293 GB |
|       from large pool |     878 MB |     911 MB |    2042 GB |    2041 GB |
|       from small pool |      97 MB |     167 MB |     252 GB |     252 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1058 MB |  107420 MB |  106380 MB |
|       from large pool |     936 MB |     948 MB |  100434 MB |   99498 MB |
|       from small pool |     104 MB |     172 MB |    6986 MB |    6882 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   66416 KB |  345437 KB |    2396 GB |    2396 GB |
|       from large pool |   59381 KB |  327682 KB |    2126 GB |    2126 GB |
|       from small pool |    7035 KB |   33723 KB |     270 GB |     270 GB |
|---------------------------------------------------------------------------|
| Allocations           |     536    |     569    |    1688 K  |    1687 K  |
|       from large pool |     111    |     133    |     567 K  |     567 K  |
|       from small pool |     425    |     534    |    1120 K  |    1120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     536    |     569    |    1688 K  |    1687 K  |
|       from large pool |     111    |     133    |     567 K  |     567 K  |
|       from small pool |     425    |     534    |    1120 K  |    1120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     134    |    8139    |    8048    |
|       from large pool |      39    |      53    |    4646    |    4607    |
|       from small pool |      52    |      86    |    3493    |    3441    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      75    |     808 K  |     807 K  |
|       from large pool |      18    |      47    |     366 K  |     366 K  |
|       from small pool |      15    |      60    |     441 K  |     441 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 941.07 MiB already allocated; 139.94 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 74           |        cudaMalloc retries: 309       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     941 MB |    1009 MB |    2304 GB |    2303 GB |
|       from large pool |     843 MB |     911 MB |    2051 GB |    2050 GB |
|       from small pool |      97 MB |     167 MB |     253 GB |     253 GB |
|---------------------------------------------------------------------------|
| Active memory         |     941 MB |    1009 MB |    2304 GB |    2303 GB |
|       from large pool |     843 MB |     911 MB |    2051 GB |    2050 GB |
|       from small pool |      97 MB |     167 MB |     253 GB |     253 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1056 MB |    1058 MB |  108258 MB |  107202 MB |
|       from large pool |     942 MB |     948 MB |  101226 MB |  100284 MB |
|       from small pool |     114 MB |     172 MB |    7032 MB |    6918 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  117689 KB |  345437 KB |    2407 GB |    2407 GB |
|       from large pool |  100435 KB |  327682 KB |    2136 GB |    2136 GB |
|       from small pool |   17254 KB |   33723 KB |     271 GB |     271 GB |
|---------------------------------------------------------------------------|
| Allocations           |     550    |     569    |    1696 K  |    1695 K  |
|       from large pool |     125    |     133    |     570 K  |     570 K  |
|       from small pool |     425    |     534    |    1125 K  |    1125 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     550    |     569    |    1696 K  |    1695 K  |
|       from large pool |     125    |     133    |     570 K  |     570 K  |
|       from small pool |     425    |     534    |    1125 K  |    1125 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     134    |    8202    |    8096    |
|       from large pool |      49    |      53    |    4686    |    4637    |
|       from small pool |      57    |      86    |    3516    |    3459    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      74    |      75    |     812 K  |     812 K  |
|       from large pool |      42    |      47    |     368 K  |     368 K  |
|       from small pool |      32    |      60    |     443 K  |     443 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 11.74 GiB total capacity; 951.66 MiB already allocated; 165.94 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 75           |        cudaMalloc retries: 312       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     951 MB |    1009 MB |    2323 GB |    2322 GB |
|       from large pool |     854 MB |     911 MB |    2068 GB |    2068 GB |
|       from small pool |      97 MB |     167 MB |     254 GB |     254 GB |
|---------------------------------------------------------------------------|
| Active memory         |     951 MB |    1009 MB |    2323 GB |    2322 GB |
|       from large pool |     854 MB |     911 MB |    2068 GB |    2068 GB |
|       from small pool |      97 MB |     167 MB |     254 GB |     254 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1030 MB |    1058 MB |  109388 MB |  108358 MB |
|       from large pool |     918 MB |     948 MB |  102292 MB |  101374 MB |
|       from small pool |     112 MB |     172 MB |    7096 MB |    6984 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   80223 KB |  345437 KB |    2427 GB |    2427 GB |
|       from large pool |   64976 KB |  327682 KB |    2154 GB |    2154 GB |
|       from small pool |   15247 KB |   33723 KB |     273 GB |     272 GB |
|---------------------------------------------------------------------------|
| Allocations           |     507    |     569    |    1707 K  |    1706 K  |
|       from large pool |     112    |     133    |     574 K  |     574 K  |
|       from small pool |     395    |     534    |    1132 K  |    1132 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     507    |     569    |    1707 K  |    1706 K  |
|       from large pool |     112    |     133    |     574 K  |     574 K  |
|       from small pool |     395    |     534    |    1132 K  |    1132 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |    8279    |    8183    |
|       from large pool |      40    |      53    |    4731    |    4691    |
|       from small pool |      56    |      86    |    3548    |    3492    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      53    |      78    |     817 K  |     817 K  |
|       from large pool |      24    |      47    |     371 K  |     371 K  |
|       from small pool |      29    |      60    |     446 K  |     446 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 954.07 MiB already allocated; 149.94 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 76           |        cudaMalloc retries: 315       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     954 MB |    1009 MB |    2332 GB |    2331 GB |
|       from large pool |     856 MB |     911 MB |    2076 GB |    2075 GB |
|       from small pool |      97 MB |     167 MB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| Active memory         |     954 MB |    1009 MB |    2332 GB |    2331 GB |
|       from large pool |     856 MB |     911 MB |    2076 GB |    2075 GB |
|       from small pool |      97 MB |     167 MB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1058 MB |  110182 MB |  109136 MB |
|       from large pool |     942 MB |     948 MB |  103022 MB |  102080 MB |
|       from small pool |     104 MB |     172 MB |    7160 MB |    7056 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   94137 KB |  345437 KB |    2437 GB |    2437 GB |
|       from large pool |   87101 KB |  327682 KB |    2162 GB |    2162 GB |
|       from small pool |    7035 KB |   33723 KB |     274 GB |     274 GB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     569    |    1715 K  |    1714 K  |
|       from large pool |     120    |     133    |     577 K  |     577 K  |
|       from small pool |     425    |     534    |    1138 K  |    1137 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     569    |    1715 K  |    1714 K  |
|       from large pool |     120    |     133    |     577 K  |     577 K  |
|       from small pool |     425    |     534    |    1138 K  |    1137 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     134    |    8343    |    8248    |
|       from large pool |      43    |      53    |    4763    |    4720    |
|       from small pool |      52    |      86    |    3580    |    3528    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      78    |     820 K  |     820 K  |
|       from large pool |      21    |      47    |     372 K  |     372 K  |
|       from small pool |      14    |      60    |     448 K  |     448 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 997.72 MiB already allocated; 141.94 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:14 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 77           |        cudaMalloc retries: 318       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     997 MB |    1009 MB |    2354 GB |    2353 GB |
|       from large pool |     900 MB |     911 MB |    2095 GB |    2094 GB |
|       from small pool |      97 MB |     167 MB |     258 GB |     258 GB |
|---------------------------------------------------------------------------|
| Active memory         |     997 MB |    1009 MB |    2354 GB |    2353 GB |
|       from large pool |     900 MB |     911 MB |    2095 GB |    2094 GB |
|       from small pool |      97 MB |     167 MB |     258 GB |     258 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1054 MB |    1058 MB |  111738 MB |  110684 MB |
|       from large pool |     950 MB |     950 MB |  104510 MB |  103560 MB |
|       from small pool |     104 MB |     172 MB |    7228 MB |    7124 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   57632 KB |  345437 KB |    2460 GB |    2460 GB |
|       from large pool |   50590 KB |  327682 KB |    2182 GB |    2182 GB |
|       from small pool |    7042 KB |   33723 KB |     277 GB |     277 GB |
|---------------------------------------------------------------------------|
| Allocations           |     548    |     569    |    1732 K  |    1732 K  |
|       from large pool |     123    |     133    |     582 K  |     582 K  |
|       from small pool |     425    |     534    |    1150 K  |    1150 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     548    |     569    |    1732 K  |    1732 K  |
|       from large pool |     123    |     133    |     582 K  |     582 K  |
|       from small pool |     425    |     534    |    1150 K  |    1150 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |    8450    |    8352    |
|       from large pool |      46    |      53    |    4836    |    4790    |
|       from small pool |      52    |      86    |    3614    |    3562    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      78    |     829 K  |     829 K  |
|       from large pool |      15    |      47    |     375 K  |     375 K  |
|       from small pool |      14    |      60    |     453 K  |     453 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 914.43 MiB already allocated; 145.94 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:14 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 78           |        cudaMalloc retries: 321       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     899 MB |    1009 MB |    2382 GB |    2381 GB |
|       from large pool |     802 MB |     911 MB |    2120 GB |    2119 GB |
|       from small pool |      97 MB |     167 MB |     262 GB |     262 GB |
|---------------------------------------------------------------------------|
| Active memory         |     899 MB |    1009 MB |    2382 GB |    2381 GB |
|       from large pool |     802 MB |     911 MB |    2120 GB |    2119 GB |
|       from small pool |      97 MB |     167 MB |     262 GB |     262 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1050 MB |    1058 MB |  112922 MB |  111872 MB |
|       from large pool |     940 MB |     950 MB |  105616 MB |  104676 MB |
|       from small pool |     110 MB |     172 MB |    7306 MB |    7196 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  113310 KB |  345437 KB |    2491 GB |    2491 GB |
|       from large pool |  100120 KB |  327682 KB |    2210 GB |    2210 GB |
|       from small pool |   13190 KB |   33723 KB |     281 GB |     281 GB |
|---------------------------------------------------------------------------|
| Allocations           |     531    |     569    |    1755 K  |    1754 K  |
|       from large pool |     124    |     133    |     589 K  |     589 K  |
|       from small pool |     407    |     534    |    1165 K  |    1165 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     531    |     569    |    1755 K  |    1754 K  |
|       from large pool |     124    |     133    |     589 K  |     589 K  |
|       from small pool |     407    |     534    |    1165 K  |    1165 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     134    |    8545    |    8442    |
|       from large pool |      48    |      53    |    4892    |    4844    |
|       from small pool |      55    |      86    |    3653    |    3598    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      52    |      78    |     840 K  |     839 K  |
|       from large pool |      31    |      47    |     380 K  |     380 K  |
|       from small pool |      21    |      60    |     459 K  |     459 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.74 GiB total capacity; 994.39 MiB already allocated; 149.94 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:14 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 79           |        cudaMalloc retries: 324       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     994 MB |    1009 MB |    2402 GB |    2401 GB |
|       from large pool |     891 MB |     911 MB |    2138 GB |    2137 GB |
|       from small pool |     103 MB |     167 MB |     264 GB |     264 GB |
|---------------------------------------------------------------------------|
| Active memory         |     994 MB |    1009 MB |    2402 GB |    2401 GB |
|       from large pool |     891 MB |     911 MB |    2138 GB |    2137 GB |
|       from small pool |     103 MB |     167 MB |     264 GB |     264 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1058 MB |  113800 MB |  112754 MB |
|       from large pool |     940 MB |     950 MB |  106438 MB |  105498 MB |
|       from small pool |     106 MB |     172 MB |    7362 MB |    7256 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   52848 KB |  345437 KB |    2512 GB |    2512 GB |
|       from large pool |   49848 KB |  327682 KB |    2228 GB |    2228 GB |
|       from small pool |    3000 KB |   33723 KB |     283 GB |     283 GB |
|---------------------------------------------------------------------------|
| Allocations           |     533    |     569    |    1770 K  |    1770 K  |
|       from large pool |     111    |     133    |     594 K  |     594 K  |
|       from small pool |     422    |     534    |    1175 K  |    1175 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     533    |     569    |    1770 K  |    1770 K  |
|       from large pool |     111    |     133    |     594 K  |     594 K  |
|       from small pool |     422    |     534    |    1175 K  |    1175 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     134    |    8601    |    8513    |
|       from large pool |      35    |      53    |    4920    |    4885    |
|       from small pool |      53    |      86    |    3681    |    3628    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      78    |     847 K  |     847 K  |
|       from large pool |      23    |      47    |     383 K  |     383 K  |
|       from small pool |      18    |      60    |     463 K  |     463 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 996.11 MiB already allocated; 145.94 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:15 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 80           |        cudaMalloc retries: 332       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     996 MB |    1009 MB |    2430 GB |    2429 GB |
|       from large pool |     898 MB |     911 MB |    2162 GB |    2161 GB |
|       from small pool |      97 MB |     167 MB |     268 GB |     267 GB |
|---------------------------------------------------------------------------|
| Active memory         |     996 MB |    1009 MB |    2430 GB |    2429 GB |
|       from large pool |     898 MB |     911 MB |    2162 GB |    2161 GB |
|       from small pool |      97 MB |     167 MB |     268 GB |     267 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1050 MB |    1058 MB |  116346 MB |  115296 MB |
|       from large pool |     938 MB |     952 MB |  108824 MB |  107886 MB |
|       from small pool |     112 MB |     172 MB |    7522 MB |    7410 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   55186 KB |  345437 KB |    2539 GB |    2539 GB |
|       from large pool |   39949 KB |  327682 KB |    2251 GB |    2251 GB |
|       from small pool |   15237 KB |   33723 KB |     287 GB |     287 GB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     569    |    1792 K  |    1792 K  |
|       from large pool |     123    |     133    |     600 K  |     600 K  |
|       from small pool |     405    |     534    |    1191 K  |    1191 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     569    |    1792 K  |    1792 K  |
|       from large pool |     123    |     133    |     600 K  |     600 K  |
|       from small pool |     405    |     534    |    1191 K  |    1191 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |    8798    |    8700    |
|       from large pool |      42    |      53    |    5037    |    4995    |
|       from small pool |      56    |      86    |    3761    |    3705    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      78    |     858 K  |     857 K  |
|       from large pool |      16    |      47    |     388 K  |     388 K  |
|       from small pool |      25    |      60    |     469 K  |     469 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.74 GiB total capacity; 987.50 MiB already allocated; 141.94 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:15 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 81           |        cudaMalloc retries: 337       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     987 MB |    1009 MB |    2462 GB |    2461 GB |
|       from large pool |     886 MB |     911 MB |    2191 GB |    2190 GB |
|       from small pool |     100 MB |     167 MB |     271 GB |     271 GB |
|---------------------------------------------------------------------------|
| Active memory         |     987 MB |    1009 MB |    2462 GB |    2461 GB |
|       from large pool |     886 MB |     911 MB |    2191 GB |    2190 GB |
|       from small pool |     100 MB |     167 MB |     271 GB |     271 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1054 MB |    1058 MB |  118140 MB |  117086 MB |
|       from large pool |     948 MB |     952 MB |  110494 MB |  109546 MB |
|       from small pool |     106 MB |     172 MB |    7646 MB |    7540 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   68101 KB |  345437 KB |    2571 GB |    2571 GB |
|       from large pool |   62606 KB |  327682 KB |    2281 GB |    2281 GB |
|       from small pool |    5495 KB |   33723 KB |     290 GB |     290 GB |
|---------------------------------------------------------------------------|
| Allocations           |     525    |     569    |    1815 K  |    1814 K  |
|       from large pool |     116    |     133    |     608 K  |     608 K  |
|       from small pool |     409    |     534    |    1206 K  |    1206 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     525    |     569    |    1815 K  |    1814 K  |
|       from large pool |     116    |     133    |     608 K  |     608 K  |
|       from small pool |     409    |     534    |    1206 K  |    1206 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     134    |    8939    |    8847    |
|       from large pool |      39    |      53    |    5116    |    5077    |
|       from small pool |      53    |      86    |    3823    |    3770    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      78    |     868 K  |     868 K  |
|       from large pool |      28    |      47    |     392 K  |     392 K  |
|       from small pool |      14    |      60    |     475 K  |     475 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 11.74 GiB total capacity; 967.35 MiB already allocated; 156.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:17 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 82           |        cudaMalloc retries: 345       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     967 MB |    1009 MB |    2574 GB |    2573 GB |
|       from large pool |     870 MB |     911 MB |    2291 GB |    2290 GB |
|       from small pool |      97 MB |     167 MB |     283 GB |     283 GB |
|---------------------------------------------------------------------------|
| Active memory         |     967 MB |    1009 MB |    2574 GB |    2573 GB |
|       from large pool |     870 MB |     911 MB |    2291 GB |    2290 GB |
|       from small pool |      97 MB |     167 MB |     283 GB |     283 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1058 MB |  121444 MB |  120398 MB |
|       from large pool |     942 MB |     952 MB |  113616 MB |  112674 MB |
|       from small pool |     104 MB |     172 MB |    7828 MB |    7724 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   80538 KB |  345437 KB |    2689 GB |    2689 GB |
|       from large pool |   73508 KB |  327682 KB |    2386 GB |    2386 GB |
|       from small pool |    7030 KB |   33723 KB |     303 GB |     303 GB |
|---------------------------------------------------------------------------|
| Allocations           |     508    |     569    |    1900 K  |    1900 K  |
|       from large pool |     113    |     133    |     637 K  |     637 K  |
|       from small pool |     395    |     534    |    1263 K  |    1262 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     508    |     569    |    1900 K  |    1900 K  |
|       from large pool |     113    |     133    |     637 K  |     637 K  |
|       from small pool |     395    |     534    |    1263 K  |    1262 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     134    |    9187    |    9093    |
|       from large pool |      42    |      53    |    5273    |    5231    |
|       from small pool |      52    |      86    |    3914    |    3862    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      51    |      78    |     910 K  |     910 K  |
|       from large pool |      28    |      47    |     411 K  |     411 K  |
|       from small pool |      23    |      60    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 11.74 GiB total capacity; 1000.26 MiB already allocated; 144.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:17 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 83           |        cudaMalloc retries: 350       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1000 MB |    1009 MB |    2604 GB |    2603 GB |
|       from large pool |     903 MB |     911 MB |    2318 GB |    2317 GB |
|       from small pool |      97 MB |     167 MB |     286 GB |     286 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1000 MB |    1009 MB |    2604 GB |    2603 GB |
|       from large pool |     903 MB |     911 MB |    2318 GB |    2317 GB |
|       from small pool |      97 MB |     167 MB |     286 GB |     286 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1058 MB |    1058 MB |  123078 MB |  122020 MB |
|       from large pool |     946 MB |     952 MB |  115122 MB |  114176 MB |
|       from small pool |     112 MB |     172 MB |    7956 MB |    7844 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   59127 KB |  345437 KB |    2721 GB |    2721 GB |
|       from large pool |   43853 KB |  327682 KB |    2414 GB |    2414 GB |
|       from small pool |   15274 KB |   33723 KB |     307 GB |     307 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     569    |    1923 K  |    1922 K  |
|       from large pool |     111    |     133    |     645 K  |     644 K  |
|       from small pool |     405    |     534    |    1278 K  |    1277 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     569    |    1923 K  |    1922 K  |
|       from large pool |     111    |     133    |     645 K  |     644 K  |
|       from small pool |     405    |     534    |    1278 K  |    1277 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     134    |    9314    |    9222    |
|       from large pool |      36    |      53    |    5336    |    5300    |
|       from small pool |      56    |      86    |    3978    |    3922    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      78    |     921 K  |     921 K  |
|       from large pool |      15    |      47    |     416 K  |     416 K  |
|       from small pool |      28    |      60    |     504 K  |     504 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 952.18 MiB already allocated; 152.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:18 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 84           |        cudaMalloc retries: 353       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     952 MB |    1009 MB |    2622 GB |    2621 GB |
|       from large pool |     855 MB |     911 MB |    2333 GB |    2332 GB |
|       from small pool |      97 MB |     167 MB |     288 GB |     288 GB |
|---------------------------------------------------------------------------|
| Active memory         |     952 MB |    1009 MB |    2622 GB |    2621 GB |
|       from large pool |     855 MB |     911 MB |    2333 GB |    2332 GB |
|       from small pool |      97 MB |     167 MB |     288 GB |     288 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1050 MB |    1058 MB |  123826 MB |  122776 MB |
|       from large pool |     934 MB |     952 MB |  115796 MB |  114862 MB |
|       from small pool |     116 MB |     172 MB |    8030 MB |    7914 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  100171 KB |  345437 KB |    2740 GB |    2740 GB |
|       from large pool |   80849 KB |  327682 KB |    2430 GB |    2430 GB |
|       from small pool |   19321 KB |   33723 KB |     309 GB |     309 GB |
|---------------------------------------------------------------------------|
| Allocations           |     530    |     569    |    1936 K  |    1935 K  |
|       from large pool |     125    |     133    |     649 K  |     649 K  |
|       from small pool |     405    |     534    |    1286 K  |    1286 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     530    |     569    |    1936 K  |    1935 K  |
|       from large pool |     125    |     133    |     649 K  |     649 K  |
|       from small pool |     405    |     534    |    1286 K  |    1286 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     134    |    9386    |    9278    |
|       from large pool |      50    |      53    |    5371    |    5321    |
|       from small pool |      58    |      86    |    4015    |    3957    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      63    |      78    |     927 K  |     927 K  |
|       from large pool |      30    |      47    |     419 K  |     419 K  |
|       from small pool |      33    |      60    |     508 K  |     508 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 11.74 GiB total capacity; 905.71 MiB already allocated; 246.81 MiB free; 956.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:18 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 85           |        cudaMalloc retries: 356       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     905 MB |    1009 MB |    2632 GB |    2631 GB |
|       from large pool |     808 MB |     911 MB |    2342 GB |    2341 GB |
|       from small pool |      97 MB |     167 MB |     289 GB |     289 GB |
|---------------------------------------------------------------------------|
| Active memory         |     905 MB |    1009 MB |    2632 GB |    2631 GB |
|       from large pool |     808 MB |     911 MB |    2342 GB |    2341 GB |
|       from small pool |      97 MB |     167 MB |     289 GB |     289 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |     956 MB |    1058 MB |  125190 MB |  124234 MB |
|       from large pool |     842 MB |     952 MB |  117088 MB |  116246 MB |
|       from small pool |     114 MB |     172 MB |    8102 MB |    7988 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   51497 KB |  345437 KB |    2750 GB |    2750 GB |
|       from large pool |   34120 KB |  327682 KB |    2439 GB |    2439 GB |
|       from small pool |   17377 KB |   33723 KB |     310 GB |     310 GB |
|---------------------------------------------------------------------------|
| Allocations           |     474    |     569    |    1944 K  |    1943 K  |
|       from large pool |      62    |     133    |     651 K  |     651 K  |
|       from small pool |     412    |     534    |    1292 K  |    1291 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     474    |     569    |    1944 K  |    1943 K  |
|       from large pool |      62    |     133    |     651 K  |     651 K  |
|       from small pool |     412    |     534    |    1292 K  |    1291 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     134    |    9469    |    9393    |
|       from large pool |      19    |      53    |    5418    |    5399    |
|       from small pool |      57    |      86    |    4051    |    3994    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |      78    |     931 K  |     931 K  |
|       from large pool |      14    |      47    |     421 K  |     421 K  |
|       from small pool |      30    |      65    |     510 K  |     510 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 11.74 GiB total capacity; 835.65 MiB already allocated; 180.81 MiB free; 1022.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:18 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 86           |        cudaMalloc retries: 357       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     835 MB |    1009 MB |    2633 GB |    2632 GB |
|       from large pool |     737 MB |     911 MB |    2343 GB |    2342 GB |
|       from small pool |      97 MB |     167 MB |     289 GB |     289 GB |
|---------------------------------------------------------------------------|
| Active memory         |     835 MB |    1009 MB |    2633 GB |    2632 GB |
|       from large pool |     737 MB |     911 MB |    2343 GB |    2342 GB |
|       from small pool |      97 MB |     167 MB |     289 GB |     289 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1022 MB |    1058 MB |  125300 MB |  124278 MB |
|       from large pool |     916 MB |     952 MB |  117162 MB |  116246 MB |
|       from small pool |     106 MB |     172 MB |    8138 MB |    8032 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  190826 KB |  345437 KB |    2750 GB |    2750 GB |
|       from large pool |  182596 KB |  327682 KB |    2440 GB |    2439 GB |
|       from small pool |    8230 KB |   33723 KB |     310 GB |     310 GB |
|---------------------------------------------------------------------------|
| Allocations           |     469    |     569    |    1945 K  |    1944 K  |
|       from large pool |      81    |     133    |     651 K  |     651 K  |
|       from small pool |     388    |     534    |    1293 K  |    1292 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     469    |     569    |    1945 K  |    1944 K  |
|       from large pool |      81    |     133    |     651 K  |     651 K  |
|       from small pool |     388    |     534    |    1293 K  |    1292 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     134    |    9488    |    9415    |
|       from large pool |      20    |      53    |    5419    |    5399    |
|       from small pool |      53    |      86    |    4069    |    4016    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      78    |     932 K  |     932 K  |
|       from large pool |      14    |      47    |     421 K  |     421 K  |
|       from small pool |      26    |      65    |     511 K  |     511 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 959.01 MiB already allocated; 158.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:18 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 87           |        cudaMalloc retries: 360       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     959 MB |    1009 MB |    2655 GB |    2654 GB |
|       from large pool |     861 MB |     911 MB |    2362 GB |    2361 GB |
|       from small pool |      97 MB |     167 MB |     293 GB |     293 GB |
|---------------------------------------------------------------------------|
| Active memory         |     959 MB |    1009 MB |    2655 GB |    2654 GB |
|       from large pool |     861 MB |     911 MB |    2362 GB |    2361 GB |
|       from small pool |      97 MB |     167 MB |     293 GB |     293 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1044 MB |    1058 MB |  126070 MB |  125026 MB |
|       from large pool |     938 MB |     952 MB |  117860 MB |  116922 MB |
|       from small pool |     106 MB |     172 MB |    8210 MB |    8104 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   87029 KB |  345437 KB |    2777 GB |    2777 GB |
|       from large pool |   77968 KB |  327682 KB |    2462 GB |    2462 GB |
|       from small pool |    9061 KB |   33723 KB |     314 GB |     314 GB |
|---------------------------------------------------------------------------|
| Allocations           |     529    |     569    |    1966 K  |    1965 K  |
|       from large pool |     130    |     133    |     658 K  |     657 K  |
|       from small pool |     399    |     534    |    1308 K  |    1307 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     529    |     569    |    1966 K  |    1965 K  |
|       from large pool |     130    |     133    |     658 K  |     657 K  |
|       from small pool |     399    |     534    |    1308 K  |    1307 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     134    |    9558    |    9463    |
|       from large pool |      42    |      53    |    5453    |    5411    |
|       from small pool |      53    |      86    |    4105    |    4052    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      78    |     942 K  |     942 K  |
|       from large pool |      25    |      47    |     425 K  |     425 K  |
|       from small pool |      24    |      65    |     517 K  |     517 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 969.43 MiB already allocated; 146.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:19 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 88           |        cudaMalloc retries: 365       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     969 MB |    1009 MB |    2702 GB |    2701 GB |
|       from large pool |     872 MB |     911 MB |    2404 GB |    2403 GB |
|       from small pool |      97 MB |     167 MB |     298 GB |     298 GB |
|---------------------------------------------------------------------------|
| Active memory         |     969 MB |    1009 MB |    2702 GB |    2701 GB |
|       from large pool |     872 MB |     911 MB |    2404 GB |    2403 GB |
|       from small pool |      97 MB |     167 MB |     298 GB |     298 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1056 MB |    1058 MB |  127648 MB |  126592 MB |
|       from large pool |     946 MB |     952 MB |  119296 MB |  118350 MB |
|       from small pool |     110 MB |     174 MB |    8352 MB |    8242 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   88651 KB |  345437 KB |    2824 GB |    2824 GB |
|       from large pool |   75511 KB |  327682 KB |    2505 GB |    2505 GB |
|       from small pool |   13140 KB |   33723 KB |     319 GB |     319 GB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     569    |    2000 K  |    1999 K  |
|       from large pool |     127    |     133    |     669 K  |     668 K  |
|       from small pool |     417    |     534    |    1331 K  |    1330 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     569    |    2000 K  |    1999 K  |
|       from large pool |     127    |     133    |     669 K  |     668 K  |
|       from small pool |     417    |     534    |    1331 K  |    1330 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     134    |    9698    |    9596    |
|       from large pool |      47    |      53    |    5522    |    5475    |
|       from small pool |      55    |      87    |    4176    |    4121    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      78    |     957 K  |     957 K  |
|       from large pool |      19    |      47    |     432 K  |     432 K  |
|       from small pool |      18    |      65    |     525 K  |     525 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 956.19 MiB already allocated; 146.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:19 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 89           |        cudaMalloc retries: 369       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     950 MB |    1009 MB |    2737 GB |    2736 GB |
|       from large pool |     849 MB |     911 MB |    2434 GB |    2433 GB |
|       from small pool |     100 MB |     167 MB |     302 GB |     302 GB |
|---------------------------------------------------------------------------|
| Active memory         |     950 MB |    1009 MB |    2737 GB |    2736 GB |
|       from large pool |     849 MB |     911 MB |    2434 GB |    2433 GB |
|       from small pool |     100 MB |     167 MB |     302 GB |     302 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1056 MB |    1058 MB |  129234 MB |  128178 MB |
|       from large pool |     948 MB |     952 MB |  120778 MB |  119830 MB |
|       from small pool |     108 MB |     174 MB |    8456 MB |    8348 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   87282 KB |  345437 KB |    2861 GB |    2860 GB |
|       from large pool |   80110 KB |  327682 KB |    2537 GB |    2536 GB |
|       from small pool |    7171 KB |   33723 KB |     324 GB |     324 GB |
|---------------------------------------------------------------------------|
| Allocations           |     554    |     569    |    2027 K  |    2026 K  |
|       from large pool |     123    |     133    |     677 K  |     677 K  |
|       from small pool |     431    |     534    |    1349 K  |    1349 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     554    |     569    |    2027 K  |    2026 K  |
|       from large pool |     123    |     133    |     677 K  |     677 K  |
|       from small pool |     431    |     534    |    1349 K  |    1349 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     134    |    9823    |    9720    |
|       from large pool |      49    |      53    |    5595    |    5546    |
|       from small pool |      54    |      87    |    4228    |    4174    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      48    |      78    |     970 K  |     970 K  |
|       from large pool |      29    |      47    |     437 K  |     437 K  |
|       from small pool |      19    |      65    |     532 K  |     532 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 11.74 GiB total capacity; 967.73 MiB already allocated; 158.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:20 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 90           |        cudaMalloc retries: 372       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     967 MB |    1009 MB |    2741 GB |    2741 GB |
|       from large pool |     870 MB |     911 MB |    2439 GB |    2438 GB |
|       from small pool |      97 MB |     167 MB |     302 GB |     302 GB |
|---------------------------------------------------------------------------|
| Active memory         |     967 MB |    1009 MB |    2741 GB |    2741 GB |
|       from large pool |     870 MB |     911 MB |    2439 GB |    2438 GB |
|       from small pool |      97 MB |     167 MB |     302 GB |     302 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1044 MB |    1058 MB |  130030 MB |  128986 MB |
|       from large pool |     938 MB |     952 MB |  121512 MB |  120574 MB |
|       from small pool |     106 MB |     174 MB |    8518 MB |    8412 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   78102 KB |  345437 KB |    2865 GB |    2865 GB |
|       from large pool |   68965 KB |  327682 KB |    2540 GB |    2540 GB |
|       from small pool |    9137 KB |   33723 KB |     324 GB |     324 GB |
|---------------------------------------------------------------------------|
| Allocations           |     484    |     569    |    2030 K  |    2030 K  |
|       from large pool |      85    |     133    |     678 K  |     678 K  |
|       from small pool |     399    |     534    |    1351 K  |    1351 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     484    |     569    |    2030 K  |    2030 K  |
|       from large pool |      85    |     133    |     678 K  |     678 K  |
|       from small pool |     399    |     534    |    1351 K  |    1351 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |     134    |    9874    |    9789    |
|       from large pool |      32    |      53    |    5615    |    5583    |
|       from small pool |      53    |      87    |    4259    |    4206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      78    |     972 K  |     972 K  |
|       from large pool |      21    |      47    |     438 K  |     438 K  |
|       from small pool |      16    |      65    |     533 K  |     533 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 946.25 MiB already allocated; 154.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:20 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 91           |        cudaMalloc retries: 375       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     946 MB |    1009 MB |    2798 GB |    2797 GB |
|       from large pool |     849 MB |     911 MB |    2489 GB |    2488 GB |
|       from small pool |      97 MB |     167 MB |     308 GB |     308 GB |
|---------------------------------------------------------------------------|
| Active memory         |     946 MB |    1009 MB |    2798 GB |    2797 GB |
|       from large pool |     849 MB |     911 MB |    2489 GB |    2488 GB |
|       from small pool |      97 MB |     167 MB |     308 GB |     308 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1048 MB |    1058 MB |  131190 MB |  130142 MB |
|       from large pool |     942 MB |     952 MB |  122596 MB |  121654 MB |
|       from small pool |     106 MB |     174 MB |    8594 MB |    8488 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  104188 KB |  345437 KB |    2926 GB |    2926 GB |
|       from large pool |   95152 KB |  327682 KB |    2595 GB |    2595 GB |
|       from small pool |    9035 KB |   33723 KB |     331 GB |     331 GB |
|---------------------------------------------------------------------------|
| Allocations           |     540    |     569    |    2076 K  |    2075 K  |
|       from large pool |     115    |     133    |     695 K  |     694 K  |
|       from small pool |     425    |     534    |    1380 K  |    1380 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     540    |     569    |    2076 K  |    2075 K  |
|       from large pool |     115    |     133    |     695 K  |     694 K  |
|       from small pool |     425    |     534    |    1380 K  |    1380 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |    9962    |    9864    |
|       from large pool |      45    |      53    |    5665    |    5620    |
|       from small pool |      53    |      87    |    4297    |    4244    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      51    |      78    |     994 K  |     994 K  |
|       from large pool |      33    |      47    |     448 K  |     448 K  |
|       from small pool |      18    |      65    |     545 K  |     545 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 977.07 MiB already allocated; 160.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:22 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 92           |        cudaMalloc retries: 380       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     977 MB |    1009 MB |    2928 GB |    2927 GB |
|       from large pool |     879 MB |     911 MB |    2606 GB |    2606 GB |
|       from small pool |      97 MB |     167 MB |     321 GB |     321 GB |
|---------------------------------------------------------------------------|
| Active memory         |     977 MB |    1009 MB |    2928 GB |    2927 GB |
|       from large pool |     879 MB |     911 MB |    2606 GB |    2606 GB |
|       from small pool |      97 MB |     167 MB |     321 GB |     321 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1042 MB |    1058 MB |  133722 MB |  132680 MB |
|       from large pool |     936 MB |     952 MB |  125002 MB |  124066 MB |
|       from small pool |     106 MB |     174 MB |    8720 MB |    8614 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   66491 KB |  345437 KB |    3065 GB |    3065 GB |
|       from large pool |   57413 KB |  327682 KB |    2720 GB |    2720 GB |
|       from small pool |    9077 KB |   33723 KB |     344 GB |     344 GB |
|---------------------------------------------------------------------------|
| Allocations           |     526    |     569    |    2164 K  |    2164 K  |
|       from large pool |     121    |     133    |     727 K  |     727 K  |
|       from small pool |     405    |     534    |    1437 K  |    1437 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     526    |     569    |    2164 K  |    2164 K  |
|       from large pool |     121    |     133    |     727 K  |     727 K  |
|       from small pool |     405    |     534    |    1437 K  |    1437 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |   10147    |   10049    |
|       from large pool |      45    |      53    |    5787    |    5742    |
|       from small pool |      53    |      87    |    4360    |    4307    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      38    |      78    |    1037 K  |    1037 K  |
|       from large pool |      21    |      47    |     469 K  |     469 K  |
|       from small pool |      17    |      65    |     567 K  |     567 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 972.24 MiB already allocated; 162.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:22 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 93           |        cudaMalloc retries: 383       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     948 MB |    1009 MB |    2938 GB |    2937 GB |
|       from large pool |     851 MB |     911 MB |    2615 GB |    2614 GB |
|       from small pool |      97 MB |     167 MB |     322 GB |     322 GB |
|---------------------------------------------------------------------------|
| Active memory         |     948 MB |    1009 MB |    2938 GB |    2937 GB |
|       from large pool |     851 MB |     911 MB |    2615 GB |    2614 GB |
|       from small pool |      97 MB |     167 MB |     322 GB |     322 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1058 MB |  134954 MB |  133914 MB |
|       from large pool |     934 MB |     952 MB |  126154 MB |  125220 MB |
|       from small pool |     106 MB |     174 MB |    8800 MB |    8694 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   69382 KB |  345437 KB |    3075 GB |    3075 GB |
|       from large pool |   60298 KB |  327682 KB |    2729 GB |    2729 GB |
|       from small pool |    9084 KB |   33723 KB |     345 GB |     345 GB |
|---------------------------------------------------------------------------|
| Allocations           |     538    |     569    |    2171 K  |    2171 K  |
|       from large pool |     121    |     133    |     729 K  |     729 K  |
|       from small pool |     417    |     534    |    1442 K  |    1442 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     538    |     569    |    2171 K  |    2171 K  |
|       from large pool |     121    |     133    |     729 K  |     729 K  |
|       from small pool |     417    |     534    |    1442 K  |    1442 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     134    |   10238    |   10140    |
|       from large pool |      45    |      53    |    5838    |    5793    |
|       from small pool |      53    |      87    |    4400    |    4347    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      78    |    1041 K  |    1041 K  |
|       from large pool |      22    |      47    |     471 K  |     471 K  |
|       from small pool |      19    |      65    |     569 K  |     569 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 11.74 GiB total capacity; 972.18 MiB already allocated; 162.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:23 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 94           |        cudaMalloc retries: 388       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     972 MB |    1009 MB |    2976 GB |    2975 GB |
|       from large pool |     875 MB |     911 MB |    2649 GB |    2648 GB |
|       from small pool |      97 MB |     167 MB |     327 GB |     326 GB |
|---------------------------------------------------------------------------|
| Active memory         |     972 MB |    1009 MB |    2976 GB |    2975 GB |
|       from large pool |     875 MB |     911 MB |    2649 GB |    2648 GB |
|       from small pool |      97 MB |     167 MB |     327 GB |     326 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1040 MB |    1058 MB |  137170 MB |  136130 MB |
|       from large pool |     934 MB |     952 MB |  128246 MB |  127312 MB |
|       from small pool |     106 MB |     174 MB |    8924 MB |    8818 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   69451 KB |  345437 KB |    3113 GB |    3113 GB |
|       from large pool |   60280 KB |  327682 KB |    2763 GB |    2763 GB |
|       from small pool |    9171 KB |   33723 KB |     350 GB |     350 GB |
|---------------------------------------------------------------------------|
| Allocations           |     483    |     569    |    2201 K  |    2200 K  |
|       from large pool |      94    |     133    |     738 K  |     738 K  |
|       from small pool |     389    |     534    |    1462 K  |    1461 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     483    |     569    |    2201 K  |    2200 K  |
|       from large pool |      94    |     133    |     738 K  |     738 K  |
|       from small pool |     389    |     534    |    1462 K  |    1461 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     134    |   10392    |   10306    |
|       from large pool |      33    |      53    |    5930    |    5897    |
|       from small pool |      53    |      87    |    4462    |    4409    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      78    |    1055 K  |    1055 K  |
|       from large pool |      12    |      47    |     477 K  |     477 K  |
|       from small pool |      18    |      65    |     577 K  |     577 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 1018.00 MiB already allocated; 146.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:23 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 95           |        cudaMalloc retries: 391       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1013 MB |    1018 MB |    2984 GB |    2983 GB |
|       from large pool |     912 MB |     916 MB |    2656 GB |    2655 GB |
|       from small pool |     101 MB |     167 MB |     328 GB |     328 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1013 MB |    1018 MB |    2984 GB |    2983 GB |
|       from large pool |     912 MB |     916 MB |    2656 GB |    2655 GB |
|       from small pool |     101 MB |     167 MB |     328 GB |     328 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1056 MB |    1058 MB |  138184 MB |  137128 MB |
|       from large pool |     948 MB |     952 MB |  129188 MB |  128240 MB |
|       from small pool |     108 MB |     174 MB |    8996 MB |    8888 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   43718 KB |  345437 KB |    3122 GB |    3122 GB |
|       from large pool |   36622 KB |  327682 KB |    2770 GB |    2770 GB |
|       from small pool |    7096 KB |   33723 KB |     352 GB |     352 GB |
|---------------------------------------------------------------------------|
| Allocations           |     558    |     569    |    2209 K  |    2208 K  |
|       from large pool |     127    |     133    |     740 K  |     740 K  |
|       from small pool |     431    |     534    |    1468 K  |    1467 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     558    |     569    |    2209 K  |    2208 K  |
|       from large pool |     127    |     133    |     740 K  |     740 K  |
|       from small pool |     431    |     534    |    1468 K  |    1467 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |   10469    |   10373    |
|       from large pool |      42    |      53    |    5971    |    5929    |
|       from small pool |      54    |      87    |    4498    |    4444    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      78    |    1059 K  |    1058 K  |
|       from large pool |      20    |      47    |     478 K  |     478 K  |
|       from small pool |      17    |      65    |     580 K  |     580 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 943.04 MiB already allocated; 148.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 96           |        cudaMalloc retries: 394       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     943 MB |    1018 MB |    3036 GB |    3035 GB |
|       from large pool |     845 MB |     916 MB |    2702 GB |    2701 GB |
|       from small pool |      97 MB |     167 MB |     334 GB |     334 GB |
|---------------------------------------------------------------------------|
| Active memory         |     943 MB |    1018 MB |    3036 GB |    3035 GB |
|       from large pool |     845 MB |     916 MB |    2702 GB |    2701 GB |
|       from small pool |      97 MB |     167 MB |     334 GB |     334 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1054 MB |    1058 MB |  139060 MB |  138006 MB |
|       from large pool |     948 MB |     952 MB |  129988 MB |  129040 MB |
|       from small pool |     106 MB |     174 MB |    9072 MB |    8966 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  113618 KB |  345437 KB |    3178 GB |    3178 GB |
|       from large pool |  104602 KB |  327682 KB |    2819 GB |    2819 GB |
|       from small pool |    9016 KB |   33723 KB |     358 GB |     358 GB |
|---------------------------------------------------------------------------|
| Allocations           |     533    |     569    |    2250 K  |    2249 K  |
|       from large pool |     126    |     133    |     755 K  |     755 K  |
|       from small pool |     407    |     534    |    1495 K  |    1494 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     533    |     569    |    2250 K  |    2249 K  |
|       from large pool |     126    |     133    |     755 K  |     755 K  |
|       from small pool |     407    |     534    |    1495 K  |    1494 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     134    |   10551    |   10446    |
|       from large pool |      52    |      53    |    6015    |    5963    |
|       from small pool |      53    |      87    |    4536    |    4483    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      53    |      78    |    1078 K  |    1078 K  |
|       from large pool |      35    |      47    |     487 K  |     487 K  |
|       from small pool |      18    |      65    |     590 K  |     590 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 938.09 MiB already allocated; 154.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 97           |        cudaMalloc retries: 397       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     932 MB |    1018 MB |    3090 GB |    3089 GB |
|       from large pool |     835 MB |     916 MB |    2750 GB |    2749 GB |
|       from small pool |      97 MB |     167 MB |     339 GB |     339 GB |
|---------------------------------------------------------------------------|
| Active memory         |     932 MB |    1018 MB |    3090 GB |    3089 GB |
|       from large pool |     835 MB |     916 MB |    2750 GB |    2749 GB |
|       from small pool |      97 MB |     167 MB |     339 GB |     339 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1048 MB |    1058 MB |  140264 MB |  139216 MB |
|       from large pool |     934 MB |     952 MB |  131114 MB |  130180 MB |
|       from small pool |     114 MB |     174 MB |    9150 MB |    9036 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   97873 KB |  345437 KB |    3232 GB |    3232 GB |
|       from large pool |   80656 KB |  327682 KB |    2868 GB |    2868 GB |
|       from small pool |   17217 KB |   33723 KB |     364 GB |     364 GB |
|---------------------------------------------------------------------------|
| Allocations           |     524    |     569    |    2289 K  |    2288 K  |
|       from large pool |     127    |     133    |     769 K  |     769 K  |
|       from small pool |     397    |     534    |    1519 K  |    1519 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     524    |     569    |    2289 K  |    2288 K  |
|       from large pool |     127    |     133    |     769 K  |     769 K  |
|       from small pool |     397    |     534    |    1519 K  |    1519 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     111    |     134    |   10653    |   10542    |
|       from large pool |      54    |      54    |    6078    |    6024    |
|       from small pool |      57    |      87    |    4575    |    4518    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      78    |    1097 K  |    1097 K  |
|       from large pool |      23    |      47    |     496 K  |     496 K  |
|       from small pool |      24    |      65    |     600 K  |     600 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 958.69 MiB already allocated; 152.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 98           |        cudaMalloc retries: 400       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     958 MB |    1018 MB |    3128 GB |    3127 GB |
|       from large pool |     861 MB |     916 MB |    2784 GB |    2783 GB |
|       from small pool |      97 MB |     167 MB |     343 GB |     343 GB |
|---------------------------------------------------------------------------|
| Active memory         |     958 MB |    1018 MB |    3128 GB |    3127 GB |
|       from large pool |     861 MB |     916 MB |    2784 GB |    2783 GB |
|       from small pool |      97 MB |     167 MB |     343 GB |     343 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1050 MB |    1058 MB |  141668 MB |  140618 MB |
|       from large pool |     936 MB |     952 MB |  132448 MB |  131512 MB |
|       from small pool |     114 MB |     174 MB |    9220 MB |    9106 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   93498 KB |  345437 KB |    3271 GB |    3271 GB |
|       from large pool |   76294 KB |  327682 KB |    2903 GB |    2902 GB |
|       from small pool |   17203 KB |   33723 KB |     368 GB |     368 GB |
|---------------------------------------------------------------------------|
| Allocations           |     557    |     569    |    2316 K  |    2315 K  |
|       from large pool |     130    |     133    |     778 K  |     778 K  |
|       from small pool |     427    |     534    |    1537 K  |    1537 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     557    |     569    |    2316 K  |    2315 K  |
|       from large pool |     130    |     133    |     778 K  |     778 K  |
|       from small pool |     427    |     534    |    1537 K  |    1537 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     134    |   10763    |   10653    |
|       from large pool |      53    |      54    |    6153    |    6100    |
|       from small pool |      57    |      87    |    4610    |    4553    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      56    |      78    |    1110 K  |    1110 K  |
|       from large pool |      29    |      47    |     503 K  |     503 K  |
|       from small pool |      27    |      65    |     607 K  |     607 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 940.47 MiB already allocated; 144.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 99           |        cudaMalloc retries: 408       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     940 MB |    1018 MB |    3177 GB |    3176 GB |
|       from large pool |     843 MB |     916 MB |    2827 GB |    2826 GB |
|       from small pool |      97 MB |     167 MB |     349 GB |     349 GB |
|---------------------------------------------------------------------------|
| Active memory         |     940 MB |    1018 MB |    3177 GB |    3176 GB |
|       from large pool |     843 MB |     916 MB |    2827 GB |    2826 GB |
|       from small pool |      97 MB |     167 MB |     349 GB |     349 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1058 MB |    1058 MB |  145104 MB |  144046 MB |
|       from large pool |     950 MB |     952 MB |  135710 MB |  134760 MB |
|       from small pool |     108 MB |     174 MB |    9394 MB |    9286 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  120349 KB |  345437 KB |    3318 GB |    3317 GB |
|       from large pool |  109266 KB |  327682 KB |    2943 GB |    2943 GB |
|       from small pool |   11083 KB |   33723 KB |     374 GB |     374 GB |
|---------------------------------------------------------------------------|
| Allocations           |     550    |     569    |    2352 K  |    2352 K  |
|       from large pool |     127    |     133    |     789 K  |     789 K  |
|       from small pool |     423    |     534    |    1562 K  |    1562 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     550    |     569    |    2352 K  |    2352 K  |
|       from large pool |     127    |     133    |     789 K  |     789 K  |
|       from small pool |     423    |     534    |    1562 K  |    1562 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     134    |   11025    |   10919    |
|       from large pool |      52    |      54    |    6328    |    6276    |
|       from small pool |      54    |      87    |    4697    |    4643    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      53    |      78    |    1127 K  |    1127 K  |
|       from large pool |      36    |      47    |     510 K  |     510 K  |
|       from small pool |      17    |      65    |     616 K  |     616 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 11.74 GiB total capacity; 985.42 MiB already allocated; 166.81 MiB free; 1.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 100          |        cudaMalloc retries: 411       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     985 MB |    1018 MB |    3183 GB |    3182 GB |
|       from large pool |     888 MB |     916 MB |    2833 GB |    2832 GB |
|       from small pool |      97 MB |     167 MB |     350 GB |     349 GB |
|---------------------------------------------------------------------------|
| Active memory         |     985 MB |    1018 MB |    3183 GB |    3182 GB |
|       from large pool |     888 MB |     916 MB |    2833 GB |    2832 GB |
|       from small pool |      97 MB |     167 MB |     350 GB |     349 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1036 MB |    1058 MB |  145772 MB |  144736 MB |
|       from large pool |     930 MB |     952 MB |  136332 MB |  135402 MB |
|       from small pool |     106 MB |     174 MB |    9440 MB |    9334 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   51797 KB |  345437 KB |    3324 GB |    3324 GB |
|       from large pool |   42737 KB |  327682 KB |    2949 GB |    2949 GB |
|       from small pool |    9060 KB |   33723 KB |     375 GB |     375 GB |
|---------------------------------------------------------------------------|
| Allocations           |     512    |     569    |    2356 K  |    2355 K  |
|       from large pool |     111    |     133    |     791 K  |     791 K  |
|       from small pool |     401    |     534    |    1564 K  |    1564 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     512    |     569    |    2356 K  |    2355 K  |
|       from large pool |     111    |     133    |     791 K  |     791 K  |
|       from small pool |     401    |     534    |    1564 K  |    1564 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |   11071    |   10974    |
|       from large pool |      44    |      54    |    6351    |    6307    |
|       from small pool |      53    |      87    |    4720    |    4667    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      78    |    1129 K  |    1129 K  |
|       from large pool |      14    |      47    |     511 K  |     511 K  |
|       from small pool |      17    |      65    |     617 K  |     617 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 955.91 MiB already allocated; 156.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 101          |        cudaMalloc retries: 413       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     950 MB |    1018 MB |    3193 GB |    3192 GB |
|       from large pool |     853 MB |     916 MB |    2842 GB |    2842 GB |
|       from small pool |      97 MB |     167 MB |     350 GB |     350 GB |
|---------------------------------------------------------------------------|
| Active memory         |     950 MB |    1018 MB |    3193 GB |    3192 GB |
|       from large pool |     853 MB |     916 MB |    2842 GB |    2842 GB |
|       from small pool |      97 MB |     167 MB |     350 GB |     350 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1046 MB |    1058 MB |  145816 MB |  144770 MB |
|       from large pool |     942 MB |     952 MB |  136344 MB |  135402 MB |
|       from small pool |     104 MB |     174 MB |    9472 MB |    9368 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   98116 KB |  345437 KB |    3334 GB |    3334 GB |
|       from large pool |   91123 KB |  327682 KB |    2958 GB |    2958 GB |
|       from small pool |    6992 KB |   33723 KB |     375 GB |     375 GB |
|---------------------------------------------------------------------------|
| Allocations           |     536    |     569    |    2361 K  |    2360 K  |
|       from large pool |     131    |     133    |     793 K  |     793 K  |
|       from small pool |     405    |     534    |    1567 K  |    1567 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     536    |     569    |    2361 K  |    2360 K  |
|       from large pool |     131    |     133    |     793 K  |     793 K  |
|       from small pool |     405    |     534    |    1567 K  |    1567 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |   11088    |   10991    |
|       from large pool |      45    |      54    |    6352    |    6307    |
|       from small pool |      52    |      87    |    4736    |    4684    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |      78    |    1131 K  |    1131 K  |
|       from large pool |      29    |      47    |     512 K  |     512 K  |
|       from small pool |      15    |      65    |     618 K  |     618 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 11.74 GiB total capacity; 1009.19 MiB already allocated; 150.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 102          |        cudaMalloc retries: 418       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1009 MB |    1018 MB |    3253 GB |    3252 GB |
|       from large pool |     911 MB |     916 MB |    2896 GB |    2895 GB |
|       from small pool |      98 MB |     167 MB |     357 GB |     356 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1009 MB |    1018 MB |    3253 GB |    3252 GB |
|       from large pool |     911 MB |     916 MB |    2896 GB |    2895 GB |
|       from small pool |      98 MB |     167 MB |     357 GB |     356 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1052 MB |    1058 MB |  148266 MB |  147214 MB |
|       from large pool |     942 MB |     952 MB |  138672 MB |  137730 MB |
|       from small pool |     110 MB |     174 MB |    9594 MB |    9484 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   43837 KB |  345437 KB |    3397 GB |    3397 GB |
|       from large pool |   31576 KB |  327682 KB |    3015 GB |    3015 GB |
|       from small pool |   12261 KB |   33723 KB |     382 GB |     382 GB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     569    |    2404 K  |    2403 K  |
|       from large pool |      81    |     133    |     808 K  |     808 K  |
|       from small pool |     408    |     534    |    1595 K  |    1595 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     569    |    2404 K  |    2403 K  |
|       from large pool |      81    |     133    |     808 K  |     808 K  |
|       from small pool |     408    |     534    |    1595 K  |    1595 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      80    |     134    |   11240    |   11160    |
|       from large pool |      25    |      54    |    6443    |    6418    |
|       from small pool |      55    |      87    |    4797    |    4742    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      78    |    1151 K  |    1151 K  |
|       from large pool |       8    |      47    |     522 K  |     522 K  |
|       from small pool |      16    |      65    |     629 K  |     629 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 11.74 GiB total capacity; 962.77 MiB already allocated; 186.81 MiB free; 1016.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 103          |        cudaMalloc retries: 421       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     962 MB |    1018 MB |    3272 GB |    3271 GB |
|       from large pool |     864 MB |     916 MB |    2913 GB |    2912 GB |
|       from small pool |      97 MB |     167 MB |     359 GB |     359 GB |
|---------------------------------------------------------------------------|
| Active memory         |     962 MB |    1018 MB |    3272 GB |    3271 GB |
|       from large pool |     864 MB |     916 MB |    2913 GB |    2912 GB |
|       from small pool |      97 MB |     167 MB |     359 GB |     359 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1016 MB |    1058 MB |  149698 MB |  148682 MB |
|       from large pool |     908 MB |     952 MB |  140052 MB |  139144 MB |
|       from small pool |     108 MB |     174 MB |    9646 MB |    9538 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   54509 KB |  345437 KB |    3417 GB |    3417 GB |
|       from large pool |   44239 KB |  327682 KB |    3032 GB |    3032 GB |
|       from small pool |   10270 KB |   33723 KB |     384 GB |     384 GB |
|---------------------------------------------------------------------------|
| Allocations           |     491    |     569    |    2419 K  |    2419 K  |
|       from large pool |      83    |     133    |     813 K  |     813 K  |
|       from small pool |     408    |     534    |    1606 K  |    1605 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     491    |     569    |    2419 K  |    2419 K  |
|       from large pool |      83    |     133    |     813 K  |     813 K  |
|       from small pool |     408    |     534    |    1606 K  |    1605 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     134    |   11318    |   11237    |
|       from large pool |      27    |      54    |    6495    |    6468    |
|       from small pool |      54    |      87    |    4823    |    4769    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      78    |    1159 K  |    1159 K  |
|       from large pool |      14    |      47    |     525 K  |     525 K  |
|       from small pool |      22    |      65    |     633 K  |     633 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 11.74 GiB total capacity; 962.26 MiB already allocated; 176.81 MiB free; 1.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 104          |        cudaMalloc retries: 424       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     962 MB |    1018 MB |    3306 GB |    3305 GB |
|       from large pool |     865 MB |     916 MB |    2941 GB |    2941 GB |
|       from small pool |      97 MB |     167 MB |     364 GB |     364 GB |
|---------------------------------------------------------------------------|
| Active memory         |     962 MB |    1018 MB |    3306 GB |    3305 GB |
|       from large pool |     865 MB |     916 MB |    2941 GB |    2941 GB |
|       from small pool |      97 MB |     167 MB |     364 GB |     364 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1026 MB |    1058 MB |  150858 MB |  149832 MB |
|       from large pool |     914 MB |     952 MB |  141138 MB |  140224 MB |
|       from small pool |     112 MB |     174 MB |    9720 MB |    9608 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   65274 KB |  345437 KB |    3454 GB |    3454 GB |
|       from large pool |   49985 KB |  327682 KB |    3063 GB |    3063 GB |
|       from small pool |   15289 KB |   33723 KB |     390 GB |     390 GB |
|---------------------------------------------------------------------------|
| Allocations           |     474    |     569    |    2448 K  |    2448 K  |
|       from large pool |      85    |     133    |     821 K  |     821 K  |
|       from small pool |     389    |     534    |    1627 K  |    1626 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     474    |     569    |    2448 K  |    2448 K  |
|       from large pool |      85    |     133    |     821 K  |     821 K  |
|       from small pool |     389    |     534    |    1627 K  |    1626 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     134    |   11398    |   11311    |
|       from large pool |      31    |      54    |    6538    |    6507    |
|       from small pool |      56    |      87    |    4860    |    4804    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |      78    |    1172 K  |    1172 K  |
|       from large pool |      17    |      47    |     531 K  |     530 K  |
|       from small pool |      27    |      65    |     641 K  |     641 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 11.74 GiB total capacity; 962.09 MiB already allocated; 178.81 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:29 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 105          |        cudaMalloc retries: 430       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     962 MB |    1018 MB |    3391 GB |    3390 GB |
|       from large pool |     864 MB |     916 MB |    3017 GB |    3016 GB |
|       from small pool |      97 MB |     167 MB |     373 GB |     373 GB |
|---------------------------------------------------------------------------|
| Active memory         |     962 MB |    1018 MB |    3391 GB |    3390 GB |
|       from large pool |     864 MB |     916 MB |    3017 GB |    3016 GB |
|       from small pool |      97 MB |     167 MB |     373 GB |     373 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1024 MB |    1058 MB |  153004 MB |  151980 MB |
|       from large pool |     918 MB |     952 MB |  143182 MB |  142264 MB |
|       from small pool |     106 MB |     174 MB |    9822 MB |    9716 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   63395 KB |  345437 KB |    3546 GB |    3546 GB |
|       from large pool |   55172 KB |  327682 KB |    3146 GB |    3146 GB |
|       from small pool |    8223 KB |   33723 KB |     400 GB |     400 GB |
|---------------------------------------------------------------------------|
| Allocations           |     491    |     569    |    2511 K  |    2510 K  |
|       from large pool |      83    |     133    |     842 K  |     842 K  |
|       from small pool |     408    |     534    |    1668 K  |    1667 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     491    |     569    |    2511 K  |    2510 K  |
|       from large pool |      83    |     133    |     842 K  |     842 K  |
|       from small pool |     408    |     534    |    1668 K  |    1667 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     134    |   11534    |   11452    |
|       from large pool |      29    |      54    |    6623    |    6594    |
|       from small pool |      53    |      87    |    4911    |    4858    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      78    |    1202 K  |    1202 K  |
|       from large pool |      16    |      47    |     544 K  |     544 K  |
|       from small pool |      18    |      65    |     657 K  |     657 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 11.74 GiB total capacity; 975.95 MiB already allocated; 150.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:29 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 106          |        cudaMalloc retries: 433       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     975 MB |    1018 MB |    3392 GB |    3391 GB |
|       from large pool |     878 MB |     916 MB |    3019 GB |    3018 GB |
|       from small pool |      97 MB |     167 MB |     373 GB |     373 GB |
|---------------------------------------------------------------------------|
| Active memory         |     975 MB |    1018 MB |    3392 GB |    3391 GB |
|       from large pool |     878 MB |     916 MB |    3019 GB |    3018 GB |
|       from small pool |      97 MB |     167 MB |     373 GB |     373 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1052 MB |    1058 MB |  153868 MB |  152816 MB |
|       from large pool |     938 MB |     952 MB |  143982 MB |  143044 MB |
|       from small pool |     114 MB |     174 MB |    9886 MB |    9772 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   77871 KB |  345437 KB |    3548 GB |    3548 GB |
|       from large pool |   60571 KB |  327682 KB |    3147 GB |    3147 GB |
|       from small pool |   17300 KB |   33723 KB |     400 GB |     400 GB |
|---------------------------------------------------------------------------|
| Allocations           |     514    |     569    |    2512 K  |    2511 K  |
|       from large pool |     119    |     133    |     843 K  |     843 K  |
|       from small pool |     395    |     534    |    1669 K  |    1668 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     514    |     569    |    2512 K  |    2511 K  |
|       from large pool |     119    |     133    |     843 K  |     843 K  |
|       from small pool |     395    |     534    |    1669 K  |    1668 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     134    |   11598    |   11501    |
|       from large pool |      40    |      54    |    6655    |    6615    |
|       from small pool |      57    |      87    |    4943    |    4886    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      57    |      78    |    1202 K  |    1202 K  |
|       from large pool |      23    |      47    |     544 K  |     544 K  |
|       from small pool |      34    |      65    |     658 K  |     657 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 954.91 MiB already allocated; 152.81 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:29 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 107          |        cudaMalloc retries: 436       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     954 MB |    1018 MB |    3406 GB |    3405 GB |
|       from large pool |     857 MB |     916 MB |    3031 GB |    3030 GB |
|       from small pool |      97 MB |     167 MB |     374 GB |     374 GB |
|---------------------------------------------------------------------------|
| Active memory         |     954 MB |    1018 MB |    3406 GB |    3405 GB |
|       from large pool |     857 MB |     916 MB |    3031 GB |    3030 GB |
|       from small pool |      97 MB |     167 MB |     374 GB |     374 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1050 MB |    1058 MB |  154620 MB |  153570 MB |
|       from large pool |     940 MB |     952 MB |  144684 MB |  143744 MB |
|       from small pool |     110 MB |     174 MB |    9936 MB |    9826 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   97375 KB |  345437 KB |    3562 GB |    3562 GB |
|       from large pool |   84243 KB |  327682 KB |    3160 GB |    3160 GB |
|       from small pool |   13132 KB |   33723 KB |     401 GB |     401 GB |
|---------------------------------------------------------------------------|
| Allocations           |     553    |     569    |    2520 K  |    2520 K  |
|       from large pool |     126    |     133    |     846 K  |     846 K  |
|       from small pool |     427    |     534    |    1674 K  |    1673 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     553    |     569    |    2520 K  |    2520 K  |
|       from large pool |     126    |     133    |     846 K  |     846 K  |
|       from small pool |     427    |     534    |    1674 K  |    1673 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     104    |     134    |   11659    |   11555    |
|       from large pool |      49    |      54    |    6691    |    6642    |
|       from small pool |      55    |      87    |    4968    |    4913    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      78    |    1206 K  |    1206 K  |
|       from large pool |      29    |      47    |     546 K  |     546 K  |
|       from small pool |      20    |      65    |     660 K  |     660 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.74 GiB total capacity; 1000.33 MiB already allocated; 168.44 MiB free; 1.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:30 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 108          |        cudaMalloc retries: 440       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     939 MB |    1038 MB |    3465 GB |    3464 GB |
|       from large pool |     842 MB |     941 MB |    3084 GB |    3083 GB |
|       from small pool |      97 MB |     167 MB |     380 GB |     380 GB |
|---------------------------------------------------------------------------|
| Active memory         |     939 MB |    1038 MB |    3465 GB |    3464 GB |
|       from large pool |     842 MB |     941 MB |    3084 GB |    3083 GB |
|       from small pool |      97 MB |     167 MB |     380 GB |     380 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1060 MB |    1076 MB |  156250 MB |  155190 MB |
|       from large pool |     956 MB |     970 MB |  146244 MB |  145288 MB |
|       from small pool |     104 MB |     174 MB |   10006 MB |    9902 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   49601 KB |  345437 KB |    3623 GB |    3623 GB |
|       from large pool |   42516 KB |  327682 KB |    3215 GB |    3215 GB |
|       from small pool |    7084 KB |   33723 KB |     407 GB |     407 GB |
|---------------------------------------------------------------------------|
| Allocations           |     523    |     569    |    2562 K  |    2561 K  |
|       from large pool |     118    |     133    |     861 K  |     861 K  |
|       from small pool |     405    |     534    |    1700 K  |    1700 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     523    |     569    |    2562 K  |    2561 K  |
|       from large pool |     118    |     133    |     861 K  |     861 K  |
|       from small pool |     405    |     534    |    1700 K  |    1700 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |   11768    |   11672    |
|       from large pool |      44    |      54    |    6765    |    6721    |
|       from small pool |      52    |      87    |    5003    |    4951    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      78    |    1226 K  |    1226 K  |
|       from large pool |      11    |      47    |     556 K  |     556 K  |
|       from small pool |      19    |      65    |     670 K  |     670 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 1008.72 MiB already allocated; 150.44 MiB free; 1.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:30 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 109          |        cudaMalloc retries: 443       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1002 MB |    1038 MB |    3483 GB |    3482 GB |
|       from large pool |     905 MB |     941 MB |    3100 GB |    3099 GB |
|       from small pool |      97 MB |     167 MB |     383 GB |     383 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1002 MB |    1038 MB |    3483 GB |    3482 GB |
|       from large pool |     905 MB |     941 MB |    3100 GB |    3099 GB |
|       from small pool |      97 MB |     167 MB |     383 GB |     383 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1078 MB |    1084 MB |  157194 MB |  156116 MB |
|       from large pool |     974 MB |     974 MB |  147104 MB |  146130 MB |
|       from small pool |     104 MB |     174 MB |   10090 MB |    9986 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   76845 KB |  345437 KB |    3643 GB |    3643 GB |
|       from large pool |   69819 KB |  327682 KB |    3232 GB |    3232 GB |
|       from small pool |    7025 KB |   33723 KB |     410 GB |     410 GB |
|---------------------------------------------------------------------------|
| Allocations           |     540    |     569    |    2577 K  |    2576 K  |
|       from large pool |     125    |     133    |     865 K  |     865 K  |
|       from small pool |     415    |     534    |    1711 K  |    1711 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     540    |     569    |    2577 K  |    2576 K  |
|       from large pool |     125    |     133    |     865 K  |     865 K  |
|       from small pool |     415    |     534    |    1711 K  |    1711 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     134    |   11847    |   11751    |
|       from large pool |      44    |      54    |    6802    |    6758    |
|       from small pool |      52    |      87    |    5045    |    4993    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      78    |    1233 K  |    1233 K  |
|       from large pool |      20    |      47    |     559 K  |     559 K  |
|       from small pool |      15    |      65    |     674 K  |     674 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.74 GiB total capacity; 1010.93 MiB already allocated; 154.44 MiB free; 1.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:30 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 110          |        cudaMalloc retries: 446       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1010 MB |    1038 MB |    3485 GB |    3484 GB |
|       from large pool |     913 MB |     941 MB |    3101 GB |    3100 GB |
|       from small pool |      97 MB |     167 MB |     383 GB |     383 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1010 MB |    1038 MB |    3485 GB |    3484 GB |
|       from large pool |     913 MB |     941 MB |    3101 GB |    3100 GB |
|       from small pool |      97 MB |     167 MB |     383 GB |     383 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1074 MB |    1084 MB |  158106 MB |  157032 MB |
|       from large pool |     968 MB |     974 MB |  147958 MB |  146990 MB |
|       from small pool |     106 MB |     174 MB |   10148 MB |   10042 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   64586 KB |  345437 KB |    3644 GB |    3644 GB |
|       from large pool |   55468 KB |  327682 KB |    3232 GB |    3232 GB |
|       from small pool |    9117 KB |   33723 KB |     411 GB |     411 GB |
|---------------------------------------------------------------------------|
| Allocations           |     492    |     569    |    2578 K  |    2577 K  |
|       from large pool |     101    |     133    |     865 K  |     865 K  |
|       from small pool |     391    |     534    |    1712 K  |    1712 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     492    |     569    |    2578 K  |    2577 K  |
|       from large pool |     101    |     133    |     865 K  |     865 K  |
|       from small pool |     391    |     534    |    1712 K  |    1712 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      90    |     134    |   11907    |   11817    |
|       from large pool |      37    |      54    |    6833    |    6796    |
|       from small pool |      53    |      87    |    5074    |    5021    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      78    |    1234 K  |    1234 K  |
|       from large pool |      20    |      47    |     559 K  |     559 K  |
|       from small pool |      22    |      65    |     675 K  |     675 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 974.77 MiB already allocated; 160.44 MiB free; 1.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:30 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 111          |        cudaMalloc retries: 449       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     974 MB |    1038 MB |    3504 GB |    3503 GB |
|       from large pool |     877 MB |     941 MB |    3118 GB |    3117 GB |
|       from small pool |      97 MB |     167 MB |     385 GB |     385 GB |
|---------------------------------------------------------------------------|
| Active memory         |     974 MB |    1038 MB |    3504 GB |    3503 GB |
|       from large pool |     877 MB |     941 MB |    3118 GB |    3117 GB |
|       from small pool |      97 MB |     167 MB |     385 GB |     385 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1068 MB |    1084 MB |  159010 MB |  157942 MB |
|       from large pool |     958 MB |     974 MB |  148790 MB |  147832 MB |
|       from small pool |     110 MB |     174 MB |   10220 MB |   10110 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   95463 KB |  345437 KB |    3665 GB |    3665 GB |
|       from large pool |   82344 KB |  327682 KB |    3252 GB |    3252 GB |
|       from small pool |   13119 KB |   33723 KB |     413 GB |     413 GB |
|---------------------------------------------------------------------------|
| Allocations           |     538    |     569    |    2591 K  |    2590 K  |
|       from large pool |     131    |     133    |     870 K  |     870 K  |
|       from small pool |     407    |     534    |    1720 K  |    1720 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     538    |     569    |    2591 K  |    2590 K  |
|       from large pool |     131    |     133    |     870 K  |     870 K  |
|       from small pool |     407    |     534    |    1720 K  |    1720 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     134    |   11987    |   11881    |
|       from large pool |      51    |      54    |    6877    |    6826    |
|       from small pool |      55    |      87    |    5110    |    5055    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      60    |      78    |    1240 K  |    1240 K  |
|       from large pool |      31    |      47    |     562 K  |     562 K  |
|       from small pool |      29    |      65    |     678 K  |     678 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 994.76 MiB already allocated; 154.44 MiB free; 1.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:31 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 112          |        cudaMalloc retries: 461       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     994 MB |    1038 MB |    3584 GB |    3583 GB |
|       from large pool |     897 MB |     941 MB |    3189 GB |    3188 GB |
|       from small pool |      97 MB |     167 MB |     395 GB |     395 GB |
|---------------------------------------------------------------------------|
| Active memory         |     994 MB |    1038 MB |    3584 GB |    3583 GB |
|       from large pool |     897 MB |     941 MB |    3189 GB |    3188 GB |
|       from small pool |      97 MB |     167 MB |     395 GB |     395 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1074 MB |    1084 MB |  163794 MB |  162720 MB |
|       from large pool |     966 MB |     974 MB |  153250 MB |  152284 MB |
|       from small pool |     108 MB |     174 MB |   10544 MB |   10436 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81138 KB |  345437 KB |    3743 GB |    3743 GB |
|       from large pool |   70010 KB |  327682 KB |    3319 GB |    3319 GB |
|       from small pool |   11128 KB |   33723 KB |     424 GB |     424 GB |
|---------------------------------------------------------------------------|
| Allocations           |     530    |     569    |    2653 K  |    2652 K  |
|       from large pool |     115    |     133    |     889 K  |     889 K  |
|       from small pool |     415    |     534    |    1763 K  |    1763 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     530    |     569    |    2653 K  |    2652 K  |
|       from large pool |     115    |     133    |     889 K  |     889 K  |
|       from small pool |     415    |     534    |    1763 K  |    1763 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     136    |   12373    |   12273    |
|       from large pool |      46    |      54    |    7101    |    7055    |
|       from small pool |      54    |      87    |    5272    |    5218    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      78    |    1269 K  |    1269 K  |
|       from large pool |      29    |      47    |     574 K  |     574 K  |
|       from small pool |      20    |      65    |     694 K  |     694 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 11.74 GiB total capacity; 973.05 MiB already allocated; 172.44 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:32 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 113          |        cudaMalloc retries: 464       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     973 MB |    1038 MB |    3592 GB |    3591 GB |
|       from large pool |     875 MB |     941 MB |    3196 GB |    3195 GB |
|       from small pool |      97 MB |     167 MB |     396 GB |     396 GB |
|---------------------------------------------------------------------------|
| Active memory         |     973 MB |    1038 MB |    3592 GB |    3591 GB |
|       from large pool |     875 MB |     941 MB |    3196 GB |    3195 GB |
|       from small pool |      97 MB |     167 MB |     396 GB |     396 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1056 MB |    1084 MB |  164600 MB |  163544 MB |
|       from large pool |     950 MB |     974 MB |  154006 MB |  153056 MB |
|       from small pool |     106 MB |     174 MB |   10594 MB |   10488 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   84942 KB |  345437 KB |    3751 GB |    3751 GB |
|       from large pool |   75845 KB |  327682 KB |    3326 GB |    3326 GB |
|       from small pool |    9096 KB |   33723 KB |     425 GB |     425 GB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     569    |    2659 K  |    2658 K  |
|       from large pool |     113    |     133    |     891 K  |     891 K  |
|       from small pool |     415    |     534    |    1767 K  |    1766 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     569    |    2659 K  |    2658 K  |
|       from large pool |     113    |     133    |     891 K  |     891 K  |
|       from small pool |     415    |     534    |    1767 K  |    1766 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     136    |   12427    |   12333    |
|       from large pool |      41    |      54    |    7130    |    7089    |
|       from small pool |      53    |      87    |    5297    |    5244    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      78    |    1272 K  |    1272 K  |
|       from large pool |      27    |      47    |     575 K  |     575 K  |
|       from small pool |      16    |      65    |     696 K  |     696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 11.74 GiB total capacity; 999.20 MiB already allocated; 174.44 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:34 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 114          |        cudaMalloc retries: 483       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     999 MB |    1038 MB |    3773 GB |    3772 GB |
|       from large pool |     902 MB |     941 MB |    3359 GB |    3358 GB |
|       from small pool |      97 MB |     167 MB |     413 GB |     413 GB |
|---------------------------------------------------------------------------|
| Active memory         |     999 MB |    1038 MB |    3773 GB |    3772 GB |
|       from large pool |     902 MB |     941 MB |    3359 GB |    3358 GB |
|       from small pool |      97 MB |     167 MB |     413 GB |     413 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1054 MB |    1084 MB |  170892 MB |  169838 MB |
|       from large pool |     948 MB |     978 MB |  159932 MB |  158984 MB |
|       from small pool |     106 MB |     174 MB |   10960 MB |   10854 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   56118 KB |  345437 KB |    3935 GB |    3935 GB |
|       from large pool |   47028 KB |  327682 KB |    3491 GB |    3491 GB |
|       from small pool |    9090 KB |   33723 KB |     443 GB |     443 GB |
|---------------------------------------------------------------------------|
| Allocations           |     506    |     569    |    2783 K  |    2782 K  |
|       from large pool |     111    |     133    |     935 K  |     935 K  |
|       from small pool |     395    |     534    |    1847 K  |    1846 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     506    |     569    |    2783 K  |    2782 K  |
|       from large pool |     111    |     133    |     935 K  |     935 K  |
|       from small pool |     395    |     534    |    1847 K  |    1846 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     136    |   12906    |   12814    |
|       from large pool |      39    |      58    |    7426    |    7387    |
|       from small pool |      53    |      87    |    5480    |    5427    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      78    |    1333 K  |    1333 K  |
|       from large pool |      18    |      47    |     605 K  |     605 K  |
|       from small pool |      25    |      65    |     727 K  |     727 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 11.74 GiB total capacity; 957.71 MiB already allocated; 162.69 MiB free; 1.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:34 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 115          |        cudaMalloc retries: 486       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     942 MB |    1038 MB |    3781 GB |    3780 GB |
|       from large pool |     841 MB |     941 MB |    3366 GB |    3365 GB |
|       from small pool |     100 MB |     167 MB |     414 GB |     414 GB |
|---------------------------------------------------------------------------|
| Active memory         |     942 MB |    1038 MB |    3781 GB |    3780 GB |
|       from large pool |     841 MB |     941 MB |    3366 GB |    3365 GB |
|       from small pool |     100 MB |     167 MB |     414 GB |     414 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1066 MB |    1084 MB |  171782 MB |  170716 MB |
|       from large pool |     962 MB |     978 MB |  160760 MB |  159798 MB |
|       from small pool |     104 MB |     174 MB |   11022 MB |   10918 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   93801 KB |  345437 KB |    3944 GB |    3944 GB |
|       from large pool |   90204 KB |  327682 KB |    3499 GB |    3499 GB |
|       from small pool |    3597 KB |   33723 KB |     444 GB |     444 GB |
|---------------------------------------------------------------------------|
| Allocations           |     541    |     569    |    2789 K  |    2788 K  |
|       from large pool |     120    |     133    |     938 K  |     938 K  |
|       from small pool |     421    |     534    |    1850 K  |    1850 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     541    |     569    |    2789 K  |    2788 K  |
|       from large pool |     120    |     133    |     938 K  |     938 K  |
|       from small pool |     421    |     534    |    1850 K  |    1850 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     136    |   12972    |   12878    |
|       from large pool |      42    |      58    |    7461    |    7419    |
|       from small pool |      52    |      87    |    5511    |    5459    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      78    |    1336 K  |    1336 K  |
|       from large pool |      34    |      47    |     606 K  |     606 K  |
|       from small pool |      15    |      65    |     729 K  |     729 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 1.02 GiB already allocated; 144.69 MiB free; 1.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:35 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 116          |        cudaMalloc retries: 489       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1045 MB |    1045 MB |    3831 GB |    3830 GB |
|       from large pool |     946 MB |     946 MB |    3411 GB |    3410 GB |
|       from small pool |      98 MB |     167 MB |     419 GB |     419 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1045 MB |    1045 MB |    3831 GB |    3830 GB |
|       from large pool |     946 MB |     946 MB |    3411 GB |    3410 GB |
|       from small pool |      98 MB |     167 MB |     419 GB |     419 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1084 MB |    1084 MB |  173304 MB |  172220 MB |
|       from large pool |     978 MB |     978 MB |  162210 MB |  161232 MB |
|       from small pool |     106 MB |     174 MB |   11094 MB |   10988 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   39122 KB |  345437 KB |    3998 GB |    3998 GB |
|       from large pool |   31900 KB |  327682 KB |    3548 GB |    3548 GB |
|       from small pool |    7222 KB |   33723 KB |     450 GB |     450 GB |
|---------------------------------------------------------------------------|
| Allocations           |     507    |     569    |    2825 K  |    2824 K  |
|       from large pool |      87    |     133    |     950 K  |     950 K  |
|       from small pool |     420    |     534    |    1874 K  |    1874 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     507    |     569    |    2825 K  |    2824 K  |
|       from large pool |      87    |     133    |     950 K  |     950 K  |
|       from small pool |     420    |     534    |    1874 K  |    1874 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     136    |   13062    |   12981    |
|       from large pool |      28    |      58    |    7515    |    7487    |
|       from small pool |      53    |      87    |    5547    |    5494    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      78    |    1353 K  |    1353 K  |
|       from large pool |      14    |      47    |     614 K  |     614 K  |
|       from small pool |      15    |      65    |     738 K  |     738 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 995.26 MiB already allocated; 162.69 MiB free; 1.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:36 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 117          |        cudaMalloc retries: 494       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     984 MB |    1045 MB |    3878 GB |    3877 GB |
|       from large pool |     887 MB |     946 MB |    3452 GB |    3451 GB |
|       from small pool |      97 MB |     167 MB |     426 GB |     426 GB |
|---------------------------------------------------------------------------|
| Active memory         |     984 MB |    1045 MB |    3878 GB |    3877 GB |
|       from large pool |     887 MB |     946 MB |    3452 GB |    3451 GB |
|       from small pool |      97 MB |     167 MB |     426 GB |     426 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1066 MB |    1084 MB |  175588 MB |  174522 MB |
|       from large pool |     956 MB |     978 MB |  164346 MB |  163390 MB |
|       from small pool |     110 MB |     174 MB |   11242 MB |   11132 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   83281 KB |  345437 KB |    4047 GB |    4047 GB |
|       from large pool |   70089 KB |  327682 KB |    3590 GB |    3590 GB |
|       from small pool |   13192 KB |   33723 KB |     456 GB |     456 GB |
|---------------------------------------------------------------------------|
| Allocations           |     500    |     569    |    2864 K  |    2863 K  |
|       from large pool |     106    |     133    |     962 K  |     962 K  |
|       from small pool |     394    |     534    |    1901 K  |    1901 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     500    |     569    |    2864 K  |    2863 K  |
|       from large pool |     106    |     133    |     962 K  |     962 K  |
|       from small pool |     394    |     534    |    1901 K  |    1901 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     136    |   13230    |   13136    |
|       from large pool |      39    |      58    |    7609    |    7570    |
|       from small pool |      55    |      87    |    5621    |    5566    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      57    |      78    |    1371 K  |    1371 K  |
|       from large pool |      24    |      47    |     622 K  |     622 K  |
|       from small pool |      33    |      65    |     749 K  |     749 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 11.74 GiB total capacity; 970.96 MiB already allocated; 176.69 MiB free; 1.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:37 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 118          |        cudaMalloc retries: 506       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |     970 MB |    1045 MB |    4020 GB |    4019 GB |
|       from large pool |     873 MB |     946 MB |    3578 GB |    3577 GB |
|       from small pool |      97 MB |     167 MB |     442 GB |     442 GB |
|---------------------------------------------------------------------------|
| Active memory         |     970 MB |    1045 MB |    4020 GB |    4019 GB |
|       from large pool |     873 MB |     946 MB |    3578 GB |    3577 GB |
|       from small pool |      97 MB |     167 MB |     442 GB |     442 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1078 MB |    1110 MB |  180838 MB |  179760 MB |
|       from large pool |     972 MB |     998 MB |  169366 MB |  168394 MB |
|       from small pool |     106 MB |     174 MB |   11472 MB |   11366 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  109608 KB |  345437 KB |    4193 GB |    4193 GB |
|       from large pool |  100446 KB |  327682 KB |    3719 GB |    3719 GB |
|       from small pool |    9162 KB |   33723 KB |     474 GB |     474 GB |
|---------------------------------------------------------------------------|
| Allocations           |     493    |     569    |    2970 K  |    2970 K  |
|       from large pool |      94    |     133    |     998 K  |     998 K  |
|       from small pool |     399    |     534    |    1972 K  |    1972 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     493    |     569    |    2970 K  |    2970 K  |
|       from large pool |      94    |     133    |     998 K  |     998 K  |
|       from small pool |     399    |     534    |    1972 K  |    1972 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     136    |   13574    |   13486    |
|       from large pool |      35    |      58    |    7838    |    7803    |
|       from small pool |      53    |      87    |    5736    |    5683    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      78    |    1422 K  |    1422 K  |
|       from large pool |      22    |      47    |     645 K  |     645 K  |
|       from small pool |      20    |      65    |     777 K  |     777 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 11.74 GiB total capacity; 1.01 GiB already allocated; 148.69 MiB free; 1.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:38 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 119          |        cudaMalloc retries: 509       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1035 MB |    1045 MB |    4044 GB |    4043 GB |
|       from large pool |     938 MB |     946 MB |    3600 GB |    3599 GB |
|       from small pool |      97 MB |     167 MB |     444 GB |     444 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1035 MB |    1045 MB |    4044 GB |    4043 GB |
|       from large pool |     938 MB |     946 MB |    3600 GB |    3599 GB |
|       from small pool |      97 MB |     167 MB |     444 GB |     444 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1106 MB |    1110 MB |  182468 MB |  181362 MB |
|       from large pool |    1000 MB |    1000 MB |  170914 MB |  169914 MB |
|       from small pool |     106 MB |     174 MB |   11554 MB |   11448 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   72536 KB |  345437 KB |    4218 GB |    4217 GB |
|       from large pool |   63435 KB |  327682 KB |    3741 GB |    3741 GB |
|       from small pool |    9101 KB |   33723 KB |     476 GB |     476 GB |
|---------------------------------------------------------------------------|
| Allocations           |     536    |     569    |    2986 K  |    2985 K  |
|       from large pool |     121    |     133    |    1003 K  |    1003 K  |
|       from small pool |     415    |     534    |    1982 K  |    1981 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     536    |     569    |    2986 K  |    2985 K  |
|       from large pool |     121    |     133    |    1003 K  |    1003 K  |
|       from small pool |     415    |     534    |    1982 K  |    1981 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     137    |   13688    |   13592    |
|       from large pool |      43    |      58    |    7911    |    7868    |
|       from small pool |      53    |      87    |    5777    |    5724    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      78    |    1430 K  |    1430 K  |
|       from large pool |      17    |      47    |     649 K  |     649 K  |
|       from small pool |      15    |      65    |     780 K  |     780 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.74 GiB total capacity; 1011.19 MiB already allocated; 164.69 MiB free; 1.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:38 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 120          |        cudaMalloc retries: 512       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1011 MB |    1045 MB |    4058 GB |    4057 GB |
|       from large pool |     914 MB |     946 MB |    3613 GB |    3612 GB |
|       from small pool |      97 MB |     167 MB |     445 GB |     445 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1011 MB |    1045 MB |    4058 GB |    4057 GB |
|       from large pool |     914 MB |     946 MB |    3613 GB |    3612 GB |
|       from small pool |      97 MB |     167 MB |     445 GB |     445 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1090 MB |    1110 MB |  183718 MB |  182628 MB |
|       from large pool |     982 MB |    1000 MB |  172116 MB |  171134 MB |
|       from small pool |     108 MB |     174 MB |   11602 MB |   11494 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   80697 KB |  345437 KB |    4232 GB |    4232 GB |
|       from large pool |   69560 KB |  327682 KB |    3755 GB |    3754 GB |
|       from small pool |   11137 KB |   33723 KB |     477 GB |     477 GB |
|---------------------------------------------------------------------------|
| Allocations           |     537    |     569    |    2994 K  |    2993 K  |
|       from large pool |     112    |     133    |    1007 K  |    1007 K  |
|       from small pool |     425    |     534    |    1987 K  |    1986 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     537    |     569    |    2994 K  |    2993 K  |
|       from large pool |     112    |     133    |    1007 K  |    1007 K  |
|       from small pool |     425    |     534    |    1987 K  |    1986 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     137    |   13764    |   13667    |
|       from large pool |      43    |      58    |    7963    |    7920    |
|       from small pool |      54    |      87    |    5801    |    5747    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      78    |    1434 K  |    1434 K  |
|       from large pool |      24    |      47    |     651 K  |     651 K  |
|       from small pool |      17    |      65    |     782 K  |     782 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.74 GiB total capacity; 1.01 GiB already allocated; 148.69 MiB free; 1.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:38 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 121          |        cudaMalloc retries: 515       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1031 MB |    1045 MB |    4069 GB |    4068 GB |
|       from large pool |     934 MB |     946 MB |    3622 GB |    3621 GB |
|       from small pool |      97 MB |     167 MB |     446 GB |     446 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1031 MB |    1045 MB |    4069 GB |    4068 GB |
|       from large pool |     934 MB |     946 MB |    3622 GB |    3621 GB |
|       from small pool |      97 MB |     167 MB |     446 GB |     446 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1106 MB |    1110 MB |  184978 MB |  183872 MB |
|       from large pool |    1002 MB |    1002 MB |  173324 MB |  172322 MB |
|       from small pool |     104 MB |     174 MB |   11654 MB |   11550 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   76380 KB |  345437 KB |    4242 GB |    4242 GB |
|       from large pool |   69342 KB |  327682 KB |    3764 GB |    3764 GB |
|       from small pool |    7038 KB |   33723 KB |     478 GB |     478 GB |
|---------------------------------------------------------------------------|
| Allocations           |     540    |     569    |    3002 K  |    3001 K  |
|       from large pool |     115    |     133    |    1009 K  |    1009 K  |
|       from small pool |     425    |     534    |    1992 K  |    1992 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     540    |     569    |    3002 K  |    3001 K  |
|       from large pool |     115    |     133    |    1009 K  |    1009 K  |
|       from small pool |     425    |     534    |    1992 K  |    1992 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     137    |   13841    |   13747    |
|       from large pool |      42    |      58    |    8014    |    7972    |
|       from small pool |      52    |      87    |    5827    |    5775    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      78    |    1438 K  |    1438 K  |
|       from large pool |      26    |      47    |     653 K  |     653 K  |
|       from small pool |      14    |      65    |     785 K  |     785 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-09-07 14:28:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.74 GiB total capacity; 1.02 GiB already allocated; 146.69 MiB free; 1.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-07 14:28:39 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 122          |        cudaMalloc retries: 518       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1044 MB |    1045 MB |    4105 GB |    4104 GB |
|       from large pool |     946 MB |     946 MB |    3654 GB |    3653 GB |
|       from small pool |      97 MB |     167 MB |     451 GB |     451 GB |
|---------------------------------------------------------------------------|
| Active memory         |    1044 MB |    1045 MB |    4105 GB |    4104 GB |
|       from large pool |     946 MB |     946 MB |    3654 GB |    3653 GB |
|       from small pool |      97 MB |     167 MB |     451 GB |     451 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1108 MB |    1110 MB |  186694 MB |  185586 MB |
|       from large pool |     998 MB |    1002 MB |  174960 MB |  173962 MB |
|       from small pool |     110 MB |     174 MB |   11734 MB |   11624 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   65510 KB |  345437 KB |    4281 GB |    4281 GB |
|       from large pool |   52344 KB |  327682 KB |    3797 GB |    3797 GB |
|       from small pool |   13166 KB |   33723 KB |     483 GB |     483 GB |
|---------------------------------------------------------------------------|
| Allocations           |     508    |     569    |    3031 K  |    3031 K  |
|       from large pool |     113    |     133    |    1019 K  |    1018 K  |
|       from small pool |     395    |     534    |    2012 K  |    2012 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     508    |     569    |    3031 K  |    3031 K  |
|       from large pool |     113    |     133    |    1019 K  |    1018 K  |
|       from small pool |     395    |     534    |    2012 K  |    2012 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     137    |   13957    |   13858    |
|       from large pool |      44    |      58    |    8090    |    8046    |
|       from small pool |      55    |      87    |    5867    |    5812    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      48    |      78    |    1452 K  |    1452 K  |
|       from large pool |      21    |      47    |     659 K  |     659 K  |
|       from small pool |      27    |      65    |     793 K  |     793 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-09-07 14:28:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
